[["data-science.html", "# 8 Data Science 8.1 DataCamp Python Skills for Data Science 8.2 DataCamp R Skills for Data Science 8.3 Azure Data Science Certification 8.4 AWS Data Science Certification 8.5 GCP - Professional Machine Learning Engineer 8.6 Kyndryl Data Science Roudmap", " # 8 Data Science 8.1 DataCamp Python Skills for Data Science 8.1.1 Introduction to Python 8.1.1.1 Python Basic Version 3.x - https://www.python.org/downloads We can save the script with .py and use python as calculator or usa ipython or python shell # Addition and subtraction print(5 + 5) print(5 - 5) # Multiplication and division print(3 * 5) print(10 / 2) # Exponentiation print(4 ** 2) # invest print(100 * 7.1) # Modulo print(18 % 7) # How much is your $100 worth after 7 years? print(100*1.1**7) Variable Specific, case-sensitive type(&lt;variable&gt;) to check the type of variable Types int - integer numbers float - real numbers bool - True , False str - string, text Lists [a,b,c, 1, True, 1.2 [a,b]] Collection of values, contain any type Slicing First element index 0 -1 last element Range [3:5] , last element not included [start : end(excluded)] Subsetting list of list [][] 8.2 DataCamp R Skills for Data Science 8.3 Azure Data Science Certification 8.4 AWS Data Science Certification 8.4.1 Demystifying AI / ML / DL What is AI ? Ability to scan and interpret the physical devices, for that we need to provide info of real world Knowledge (data) + Software programs = decisions Transfer human expertise to solve a specific problem (model) Machine learning and Deep learning are subset of AI ML : Data -&gt; processing -&gt; Predictions Machine learning can do : Make predictions Optimize utility functions Extract hidden data structures Classify data DL Enable the machine define the features itself, for instance, you show the machine several samples of rectangle and machine will be able to extract the features and recognize a probably rectangle. How to Establish an Effective AI Strategy Fast computing environments Data gathering from several sources, ubiquitous data Advanced learning algorithms The Flywheel of Data AI on AWS What is Machine Learning Subset of AI Process that takes data and use that to make predictinos and support decisions Types of Machine Learning Suggestion Intro to ML video 1: Complete sequence of videos here What is Deep Learning Deep Learning is a subset of Machine Learning Use many layers of non-linear processing units, for feature extraction and transformation Algorithms can be supervised and unsupervised Types of Neural Networks Feedforward Recurrent AWS Deep Learning Based Managed Services Amazon Lex : conversational engine Amazon Polly : lifelike speech Amazon Rekognition : Image analysis AWS Deep Learning AMI (custom models) AMI is pre-configured with : MXNet, TensorFlow, Microsoft Cognitive Engine, Caffe, Theano, Torch and Keras Support auto-scaling cluster of GPU for large training Suggestion Intro to DL video 1: Complete sequence of videos here 8.4.2 Machine Learning Essentials for Business and Technical Decision Makers What is Machine Learning(ML) ? : Process of training computers, using math and statistical processes, to find and recognize patterns in data. Iterative process How Amazon uses ML in products ? Browsing and purchasing data to provide recommendations Use voice interactions with Alexa using NLP Use ML to ship 1.6M packages per day How is machine learning helping AWS customers? Amazon Forecast Amazon Fraud Detector Amazon Personalize (product recommendation,direct marketing) Amazon Polly (TTS - text-to-speech) uses advanced deep learning technologies to synthesize natural-sounding human speech Amazon Transcribe (STT - speech-to-text) Amazon SageMaker Machine Learning on AWS How does machine learning work? What is AI ? : any system that is able to ingest human-level knowledge to automate and accelerate tasks performable by humans through natural intelligence. Narrow AI : where an AI imitates human intelligence in a single context (Today’s AI) General AI : where an AI learns and behaves with intelligence across multiple contexts (Future AI) What kind of solutions can ML provide? Regression : Prediction a numerical value , Zillow case Classification : Predicting label, duolingo case Ranking : Ordering items to find most relevant , Domino’s case Recommendation : Finding relevant items based on past behavior Hyatt Hotels Clustering : Finding patterns in examples NASA Anomaly detection : Finding outliers from examples, Fraud.net case’s What are some potential problems with machine learning ? Ingestion of poor quality data Explain complex models 8.4.3 Machine Learning for Business Leaders When is ML an option ? If the problem is persistent If the problem challenges progress or growth If the solution needs to scale If the problem requires personalization ir order to be solved What Does a successfull ML solution require ? People (Data Scientist, Data Engineer, ML Scientist, Software Engineers, etc) Time Cost Ask the right questions to team What are the made assumptions ? What is your learning target (hipotesis)? What type of ML problem is it ? Why did you choose this algorithm ? How will you evaluate the model performance ? How confident are you that you can generalize the results ? How to define and scope a ML Problem What is the specific business problem ? What is the current state solution ? What are the current pain points ? What is causing the pain points ? What is the problems impact ? How would the solution be used ? What is out of scope ? How do you define success (success criteria)? Input Gathering Do we have sufficient data ? Is there labeled examples ? If not , how difficult would it be to create/obtain ? What are our features ? What are going to be the most useful inputs ? Where is the data ? What is the data quality ? Output Definitions What business metric is defining success ? What are the trade-offs ? Are there existing baselines ? If not, what is the simplest solutions ? Is there any data validation need to green light the project ? How important is runtime and performance ? With those inputs and outputs we can formulate the problem as a Learning Task, is this a classification or regression problem ? What are the risks ? etc … When should you consider using machine learning to solve a problem ? Use ML when software logic is too difficult to code Use ML when the manual process is not cost effective Use ML when there is ample training data Use ML when the problems is formalizable as an ML Problem (reduce to well known ML problem regression, classification, cluster) When is Machine Learning NOT a Good Solution? No data No Labels Need to launch quickly No tolerance for mistakes When is Machine Learning is a Good Solution ? Difficult to directly code a solution Difficult to scale a code-based solution Personalized output Functions change over time 8.4.4 Process Model : CRISP-DM on the AWS Stack 8.5 GCP - Professional Machine Learning Engineer 8.5.1 Big Data and ML Fundamentals Compute power We can easy create a server, execute the job, pause or delete the server Storage Big Data and Machine Learning are on top of Compute power, storage and Networking that are on top of security To create a storage bucket from UI is very simple by command line we can gsutil mb -p [PROJECT_NAME] -c [STORAGE_CLASS] -l [BUCKET_LOCATTION] gs://[BUCKET_NAME]/ Types of Storage Networking GCP datacenter are interconnected Every machine can talk with each other with 10GBps Security Communication to GCP are encrypted in transit Stored data are encrypted BigQuery data are encrypted GCP Offers 8.5.2 Recommending Products using Cloud SQL and Spark Cloud SQL : Google managed RDBMS Mysql 8.6 Kyndryl Data Science Roudmap 8.6.1 Data Science- Project Management Methodology - CRISP-DM 8.6.1.1 KDD Select Interpret the data Select data relevant to analysis Preprocessing Outliers Missing Values Transform Useful features Smoothing (- binning - cluster) Aggregation (- Weekly - month) Normalization Data Mining Explore Graph Predict Models Evaluating Check Evaluate the results Analysis 8.6.1.2 SEMMA Sample : Subset of data (train, test validation) Explore: Understand the data M:odify: Clean, feature engineering Model: data mining, modeling Assess: Model performance 8.6.1.3 CRISP-DM Business Understand Data Understand Data Preparation Modeling Evaluation Deploy 8.6.1.3.1 1. Business Understand initial plan Steps: Define Business Problem : Define the objective, the analitical problem, the expectations, success criteria, pain points Assess and Analyze Scenarios Define Data Mining Problem Project plan : Deliverable (timeline, costs, success criteria, assumptions, constraints, etc) 8.6.1.3.2 2. Data Understand Data Collection : Primary data source (survery, experiments) or secondary data source (ERP, CRM, database) Data Preparation / Description Quantitative (count, continuous ) vs Qualitative (categorical) Balance vs Imbalance (one class less than 30% = Imbalance) Structure (tabular) vs Unstructured(video, img, audio, text) vs Semi-structure Exploration - Data Analysis Inferencial stats Sampling - Balacing vs Imbalancing Balancing : random sampling, sampling Imbalancing: stratified sampling, K-fold, smote, msmote, leve-one-out Descriptive stats Meam , media, mode variance, std, range skewness kurtoses Graphical Univariant Boxplot - Outliers, shape of distribution Histogram - Shape, outliers QQ Plot check train and test dataset if they are in the same distribution Bivariant Scatter : correlation, coeficient (+1, -1) , strong (r &gt; 0.85 ) weak (r &lt; 0.4), cluster, linear Data Quality Analysis Idenfity outliers, missing values Levels of granularity Inconsistence Wrong data errors Meta info 8.6.1.3.3 3. Data Preparation In this step we clean, curate, wrangle and prepare the data Outliers : 3R Techniques (Rectify, Remove, Retain) Missing Data: Imputation (mean, median, mode, regression, knn, etc) Data Transform : Log, exp, boxcox, etc, done when data are non-normal Data Normalization / Standartization Normalization (mean = 0 , std =1 ) Standardization (min = 0 , max = 1) - MinMaxScaller Discretization, Binning, Grouping Dummy variable - OneHotEncoding Apply domain knowledge to generate more features 8.6.1.3.4 4. Modeling Select model techniques Model building Model evaluation and tuning Model Assessment Supervised Learning Predict Y based on X Categorical (2 class or multiclass) numerical - Prediction User preference - Recommendation Relevance - Retrival Regression Analysis y = continuous : Linear Regression y = discrete (2 categories) : Logistic Regression y = discrete (&gt; 2 categories) : Multinominal / Ordinal Regression y = Count : Poisson / Negative Binominal REgression (var &gt; mean) Excessive Zero : ZIP (Zero Inflated Position) ZINB (Zero Inflated Negative Binomial) Hurdle KNN Naive Bayes Black Box Neural Network Support Vector Machine Ensemble Stacking : Multi Techniques (Linear + DT + KNN) mean or majority Bagging : Randon Forest - good for discrete Boosting: Decistion tree, Gradient boosting, XGB, AdaBoost Unsupervised Learning Cluster / Segmentation - reduce Row Kmeans - non hierarchical - elbow curve Hierarchical - agglomerative - deprogram DBSCAN - application with noise OPTICS - ordering points to identify cluster structure CLARA - cluster large application - for large datasets K-medians / K-medoids (for lot of outlines) / K-modes (lot of categorical variables) Dimension Reduction - reduce columns PCA SVD Association Rules / Market Basket Analysis / Affinity Analysis Support Confidence EFT Ration &gt; 1 Recommended system Network Analysis Degree Page rank others Test Mining / NLP Bow TDW / DTW TF / TDIDF Forecasting / Time Series Model Based Approaches Trend: Linear, Exponential , Quadratic Seasonality : additive or multiplicative Data Base Approaches AR - Auto regressive MA - Movie average ES - Exponential smoothing SES HOHS / Double Exponential Smoothing Winters, others Overtiffing (variance) vs Underfitting (Bias) Reinforcement Learning (learning from rewards) Semi-supervised learning Active learning, transfer learning, structure prediction 8.6.1.3.5 5. Evaluation There are no better type of evaluate need to analyze the problem and data / results to select the best metric Mean Error Mean Absolute deviation Mean Squared Error Root Mean Squared Error Mean Percentage Error Mean Absolute percentage error For Categorical we also have the Confustion Matrix TP : Correct Predictive Positive TN : Correct Predictive Negative FP : Incorrect Predictive Positive FN : Incorrect Predict Negative Precision : Prob of correctly identify a random patient with disease have a disease. (Positive Correct predicted) Sensitive (Recall or Hit Rate): Proportion of people with disease who are correctly identified as having disease Specificity (True Negative Rate) : Proportion of people with NO disease being characterized as not have disease FP Rate (Type 1 error) : 1 - Specificity FN Rate (Type 2 error) : 1 - Sensitivity F1 : 1 to 0 Measure that balance precision and recall ROC AUC : Are under the curve 0.9 - 1.0 : outstanding 0.8 - 0.9 : good 0.7 - 0.8 : acceptable 0.6 - 0.7 : poor 0.5 - 0.5 : no discrimination Model Assessment Model performance and success criteria agreed upon early are in sync Model should be repeatable and reproducible Model is in line with Non-functional requirements, such as scale, robust, maintainable, easy to deploy Model evaluation gives satisfactory results Model is meeting business requirements Rank final models based on the quality of results and relevance Any assumptions or constants that were invalidated by the model ? Cost of deploy the entire pipeline Any pain points Data Sufficiency report Final suggestions, feedback Monitoring : PEST or SWOT 8.6.1.3.6 6. Deploy DEV to PROD Proper resources - Hardware, server, software , human model saved and then deployed Maintenance and monitoring (PEST) 8.6.2 Statistics for Data Analysis Using Python 8.6.2.1 Descriptive Statistics Central Tendency Mean : Average Mode : Most occuring number Median : Moddle value when arranged in asc or desc order Dispersion Range : highest - lowest value Standard Deviation : squared root of variance Variance Inter Quartile Range IQR : If divide the data into four parts (Q1, Q2 and Q3) Quantiles, if we divide the data into n parts, we get (n-1) points of split called quantiles 8.6.2.2 Distributions BINOMIAL The experiment consist of n repeated trials Each trial can result in just two possible outcomes(success and failure) The probability of success, denoted by p, is the same on every trial The trials are independent, that is, the outcome on one trial does not affect the outcome of other trials In Python from scypy.stats import binom binom.cdf(k , n , p) # cumulative distibution function - for less than or equal to 2 binom.pmf(k , n , p) # Probability mass function - for specific number of, defects binom.sf(k , n , p) # for more than 2 (similar 1 - cdf) binom.mean(n, p) # for mean of the dist binom.std(n, p) # for standard deviation of the dist binom.var(n, p) # for the variance of the dist POISSON The possibilities of success are infinite (Number of people in a queue, Number of accident in a city) are sample of this distribution Measure the number of success similar to binomial As binomial are for discrete distribution Properties : The experiment results in a success or failure The mean of success occurs in a specific region is known Outcomes are random The outcomes of interest are rare relative to the possible outcomes The variance is equal to mean In Python from scypy.stats import binom poisson.cdf(k , mu) # cumulative distribution function - for less than or equal to poisson.pmf(k , mu) # probability mass function - for exact value poisson.sf(k , mu) # for more than (similar 1 - cdf) poisson.mean(mu) # for mean of the distr poisson.var(mu) # for variance of the distr poisson.std(mu) # for standard deviation of the distr NORMAL Most common distribution for continuous data Properties : Normal distribution is symmetrically Long Tails / Bell shaped Mean, mode and median are the same 68% of area under the curve falls with 1 std of the mean 95% of area under the curve falls with 2 std of the mean 99.7% of area under the curve fall with 3 std of the mean The total area under the normal curve is equal to 1 The probability of any particular value is 0 The probability that X is greater than or less than a value = area norm.cdf(x,mu,sigma) # Cumulative distribution function - for less than or equal to norm.pdf(x,mu,sigma) # Probability density function (not Probability mass function) - for exact value norm.sf(x,mu,sigma) # For more than (similar to 1-cdf) norm.mean(mu) # For mean of the distribution norm.var(mu) # For variance of the distribution norm.std(mu) # For standard deviation of the distribution 8.6.2.3 Inferencial and Hypothesis Testing Inferencial Stats We infer about the population based on sample data Central Limit Theorem For almost all porpulations, the sampling distribution of the mean can be approximated closely by a normal distribution, provided the sample size sufficiently large If a variable has a mens of µ and the variance \\(σ^{2}\\), as the sample size increase, the sample mean approaches a normal distribution with mean µ\\(\\overline{x}\\) and variance σ\\(\\frac{2}{x}\\) Hypothesis Testing Hypothesis testing is a method of statistical inference Commonly used tests include Comapre sample statistics with the population parameter Compare two datasets Steps for Hypothesis Testing Taking a sample and based on that sample we are predictin about the population State the alternative hypothesis State the null hypothesis Select a probability of error level (alpha). generally 0.05 Calculate the test statistics(e.g t or z score) z = (x-μ)/σ (Basic one sample) z = (x – μ) / (σ / √n) (multiple samples) Critical test statistic Use the \\(\\alpha\\) and check on Test Table Interpret the results Null Hypothesis : Basic assumption, for example : The person is innocent Alternate Hypothesis : You need to provide proof of this, for example : The person is guilty In Statistical terms you: Reject the Null Hypothesis, or Fail to reject the Null Hypothesis (not accept the Null Hypothesis) Type I Error : False Alarm Type II Error : Something change and we fail to detect the change Confidence level : C = 0.90, 0.95, 0.99 (90%, 95%, 99%) Level of Significance or Type I Error : \\(\\alpha\\) = 1 - C(0.10, 0.05, 0.01) Power Power : 1 - \\(\\beta\\) (or 1 - type II error) Type II Error : Fail to reject null hypothesis when null hypothesis is false Likelihood of rejecting null hypothesis when null hypothesis is false Or : Power is the ability of a test to correctly reject the null hypothesis P-value p-value is the lowest value of alpha for which the null hypothesis can be rejected. (Probability that the null hypothesis is correct) For example, if p = 0.045 you can reject the null hypothesis at \\(\\alpha\\) = 0.05 p is low the null must go (null get rejected), if p is high the null fly (null stay) Proportions &amp; Variances Conditions for z Test Random samples Each observation should be independent of other Sample with replacement, or If sample without replacement, the sample size should not be more than 10% of population Sampling distribution approximates Normal Distribution Population is Normally distributed and the population standard deviation is known , or Sample size &gt;= 30 One Sample One Sample z Test : Used when we have one sample from one machine Conditions for z test: Random Samples Each observation should be independent of each other (sample with replacement) or (if sample without replacement sample size should not be more than 10% or population) Sample distribution approximates Normal Distribution (Population is Normally distributed and the population std dev is known or size &gt;= 30) One Sample t Test : When we have less than 30 numbers of sample and we do not know the population standard deviation Conditions for t test: Random samples Each observation should be independent of each other (sample with replacement) or (if sample without replacement sample size should not be more than 10% or population) Sample distribution approximates Normal Distribution (Population is Normally distributed and the population std dev is unknown or size &lt; 30) One Proportion Test : Compare proportions Conditions for One Proportion test Random samples Each observation should be independent of each other (sample with replacement) or (if sample without replacement sample size should not be more than 10% or population) The data contains only two categories, such as pass / fail or yes / no For Normal Approximation (both np &gt;= 10 and n(n-p) &gt;= 10 - data should have at least 10 “successes” and at least 10 “failures”) One Variance Test : Check if variance has changed Conditions for One Variance test Random samples Each observation should be independent of each other (sample with replacement) or (if sample without replacement sample size should not be more than 10% or population) The data follows a Normal Distribution Variance Tests Chi-square Test For testing the population variance against a specified value Testing goodness of fit of some probability distribution Testing for independence of two attributes (Contingency Tables) F-test for testing equality of two variances from different population for testing equality of several means with technique of ANOVA Two Samples Two Sample z Test : Compare the sample (mean) from two machines Conditions for z test: Random Samples Each observation should be independent of each other (sample with replacement) or (if sample without replacement sample size should not be more than 10% or population) Sample distribution approximates Normal Distribution (Population is Normally distributed and the population std dev is known or size &gt;= 30) Sample of Z test hypothesis for two sample: Null Hypothesis : μ1 = μ2 Alternative hypothesis : μ1 != μ2 R sample Python sample Two Sample t test Conditions for t test: Random Samples Each observation should be independent of each other (sample with replacement) or (if sample without replacement sample size should not be more than 10% or population) Sample distribution approximates Normal Distribution (Population is Normally distributed and the population std dev is unknown or size &lt; 30) How to calculate ? Variance equal Since we have a small size of sample we going to use t test independent stats.ttest_ind() function import scipy.stats as stats machine1 = [150,152,154,152,151] machine2 = [156,155,158,155,154] stats.ttest_ind(machine1, machine2, equal_var=True) #Output # Statistics = -4.0055 # pvalue = 0.0039 Result Since the value of pvalue is less than 0.05 we will reject the Null Hypotheses H0 since there is no significant difference in the variance of two machines Variance unequal Since we have a small size of sample we going to use t test independent stats.ttest_ind() function import scipy.stats as stats machine1 = [150,152,154,152,151] machine3 = [144,162,177,150,140] stats.ttest_ind(machine1, machine3, equal_var=False) #Output # Statistics = 0.4146 # pvalue = 0.6992 Result Since pvalue is high than 0.05 we will fail reject the Null Hypotheses H0 since there is significant difference in the variance of two machines Paired t test : Compare when you have before and after results If the value in one sample affect the value in the other sample, then the samples are dependent : (Ex: Blood pressure before and after specific medicine) How to calculate ? Find the difference between two set of readings as d1, d2..dn Find the mean and std dev of these differences Using Python we can use the package scipy.stats and ttest_rel function import scipy.stats as stats before = [120,122,143,100,109] after = [122,120,141,109,109] stats.ttest_rel(before, after) # output # statistics = -0.068 # pvalue = 0.530 Results: Since pvalue is high to 0.05 we fail to reject the H0 (null hypothesis), which means there are no significant difference between the values before and after Two Proportions Test : Compare the proportions from two samples Conditions for Proportions test Random Samples Each observation should be independent of each other (sample with replacement) or (if sample without replacement sample size should not be more than 10% or population) The data contains only two categories, such as pass/fail or yes/no For Normal approximation : both np &gt;= 10 and np(1-p) &gt;= 10 : Data should have at least 10 successes and at least 10 failures for each sample (some books it is 5) Methods to calculate Pooled : H0 : p1 = p2 and Ha p1 != p2 Un-pooled : H0 p1 - p2 = d(difference) and Ha p1 - p2 != d(difference) How to calculate ? # H0 = p = p0 # Ha = p != p0 # From vendor A we test 200 pieces and find 30 defects # From vendor B we test 100 pieces and find 10 defects # Is there a significant difference in quality of those 2 vendors? (95% confidence level) from statsmodels.stats.proportion import proportion proportion.test_proportions_2indep(30,200, 10, 100, method=&#39;score&#39;) #output # Statistics = 1.198 # pvalue = 0.230 Results: Since the pvalue is higher than 0.05 we fail to reject the null hypotheses , we cannot say there is any significant difference in the proportion of this two samples Two Variances : Compare the variances from two samples Conditions and test used for two variance test: F-test for testing equality of two variances from different population for testing equality of several means with technique of ANOVA How to calculate ? * 8 samples from machine A : STDEV 1.1 * 5 samples from machine B : STDEV 11 * Is there a difference in variance at (90% confidence level) ? from scipy.stats import f # find f calculated F_cal = 11/ (1.1**2) # output 9.09 # find critical values on right dfn = n - 1 f.isf(0.05, dfn = 4, dfd = 7) # output : 4.12 # find critical value on left f.isf(0.95,4,7) # output 0.16 Results: Since the F_calc(9.09) is in the reject zone higher than right value (4.12), we reject the null hypotheses, there is a significant difference between the machines We also can use stats.bartlett(machine1, machine2) or stats.levene(machine1 , machine2) Levene test is a robust test compared with Bartlett More Than 2 Samples ANOVA is Analysis of Variance ANOVA : If we have 3 or more machines to compare To analyze the variance we have chi-square test for 1 variance test and F-test for two variance test For testing equality of several means with technique of ANOVA H0 : μ1 = μ2 = μ3 = μ4 … = μn (means are equal) Ha : At least one of the means is different from others (means are NOT equal) How to calculate ? from scipy.stats as stats m1 = [150,151,152,152,151,150] m2 = [153,152,148,151,149,152] m3 = [156,154,155,156,157,155] stats.f_oneway(m1,m2,m3) #output: #statistics : 22.264 #pvalue : 3.23e-05 Results: As the pvalue is very small we conclude that at least one machine is different from others We can also use the package statsmodels.stats with method oneway.anova_oneway() ANOVA Concept Variation within : Variation of the values in the same machine (inside or ERROR) Variation between: Variation of the values between machines (treatment) To check we take the ration of these variations using F test to conclude if there are variation of not Post Hoc Tests Post Hoc Tests attempt to control the experimentwise error rate (usually alpha = 0.05) just like one-way ANOVA is used instead of multiple t-test Tukey’s Test from statsmodels.stats.multicomp method pairwise_tukeyhsd import statsmodels.stats.oenway as oneway from statsmodels.stats.multicomp import pairwise_tukeyhsd df = mpg[mpg[&#39;cylinders&#39;] == 4][[&#39;mpg&#39;, &#39;origin&#39;]] result = pairwise_tukeyhsd(endog = df[&#39;mpg&#39;] , groups = df[&#39;origin&#39;] , alpha = 0.05 ) print(result) #output # p-adj (pvalue) = 0.7995 # Based on result we going to see the there are no significant different between europe and usa Goodmess of Fit Test Use Chi Square as test statistics To test if the sample is coming from a population with specific distribution Other goodness-of-fit tests are: Anderson-Darling Kolmogorov-Smirnov H0 : The data follow a specified distribution Ha : The data do not follow the specified distribution Sample A coin is flipped 100 times. Number of heads (40) and tails(60) . Is this coin biased ? (95% confidence level) H0 : Coin is not biased Ha : Coin is biased alpha = 0.05 # Using python import scipy.stats as stats exp = [50,50] obs = [40,60] stats.chisquare(f_obs = obs, f_exp = exp) #output pvalue = 0.0455 Result : We reject the null hypotheses which means the coin are biased Contingency Tables Help to find relationship between two discrete variables H0 : Is that there is no relationship between the row and column variables Ha : is that there is a relationship (Ha does not tell what type of relationship exists) Using python we can use scipy.stats import scipy.stats as stats sh_op = np.array([[22,26,23], [28,62,26], [72,22,66]]) stats.chip2_contingency(sh_op) # output : pvalue = 3.45e-10 Results : Reject the null hypothesis which means there is a relationship between rows and columns "]]
