[["index.html", "Learning notes : Linux, Devops, entre outros # 1 Intro", " Learning notes : Linux, Devops, entre outros Bruno Machado 2022-06-28 # 1 Intro During some courses I usually take notes for future reference, so I use markdown to organize my notes for study and future projects. "],["server-administrator-i-rh124.html", "# 2 Server Administrator I - RH124 2.1 What is linux ? 2.2 Command Line 2.3 Managing File system 2.4 Help 2.5 Text Files 2.6 Managing Local Users and Groups 2.7 Controlling access to files 2.8 Monitoring and Managing Linux process 2.9 Controlling Services and Daemons 2.10 Configuring and Securing SSH 2.11 Analysing and Storing Logs 2.12 Managing Networking 2.13 Archiving and transferring files 2.14 Intalling and updating software packages 2.15 Accessing Linux File Systems 2.16 Analysing Server and Getting Support 2.17 Extra", " # 2 Server Administrator I - RH124 2.1 What is linux ? Modular operations system where you can add components : Open Source Copyleft lic GNU : General Public License LGNU: Lesser General Public License Permissive lic: MIT Simplified BSD Apache Software License Red Hat contribute and facilitate open source projects, validating the code and support code that Red Hat validate and test. Sample of Red Hat Products: * Red Hat Enterprise Linux 8 * Red Hat Open Shift 4 * Red Hat Ansible Automation * Red Hat Ceph Storage * Red Hat OpenStack Platform * Red Hat Virtualization * Red Hat Gluster Storage Benefits of open source software: Code can survive the loss of orginal developer or distributor We can learn from real-world code and develop more effective applications Red Hat sponsor and integrate open source projects into the Fedora project and participate in upstream projects 2.2 Command Line 2.2.1 $ and # Attention on shell when you are on command line: Root user : # Normal user : $ Sample of using - or -- -all : Shell going to be interpreting all arguments individually # Shell going to be interpreting all arguments individually a, l and l ls -all . --all : Shell will interpreting the entire word # Shell will interpreting the entire word ls --all . 2.2.2 To login into another computer using ssh ssh &lt;machine or server&gt; 2.2.3 Executing commands # List the user that you login whoami # formatting the dates date +%F # Executing one command after the other date +%A; uname -r; whoami ## bruno ## 2022-06-28 ## Tuesday ## 5.4.0-113-generic ## bruno Conditional commands execution &amp;&amp; This is the basic IF statement The date command must complete successfully than the uname -r command will be running and if execute successfully the whoami command will be executed # If with and date +%F &amp;&amp; uname -r &amp;&amp; whoami echo echo # If with or ate +%F || uname -r &amp;&amp; whoami ## 2022-06-28 ## 5.4.0-113-generic ## bruno ## ## ## bash: line 7: ate: command not found ## 5.4.0-113-generic ## bruno command history !! : execute the last command !12: execute the command 12 on history ctr+r check type of file The file command is used to determining the type fo file : file /etc/issue echo echo file /bin/bash ## /etc/issue: ASCII text ## ## ## /bin/bash: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=2a9f157890930ced4c3ad0e74fc1b1b84aad71e6, for GNU/Linux 3.2.0, stripped check content of file cat or tac head or tail wc -l : count lines of file 2.3 Managing File system 2.3.1 The file system hierarchy / Try command tree -d / : Top of system file system hierarchy /usr : unix system resources, installed software and libraries /usr/bin : regular commands and utilities /bin : binaries executable that are usable by normal users /sbin : system binaries executable typically used by root user /boot : component that are necessary to boot file system, like bootloader called grub 2 and linux kernel /dev : device files, represent hardware components /etc : extended text configuration configuration files /home : home dir for normal users /run : runtime data going to be recreate on a reboot /var: variable data that should survive a reboot, log files, database and website files /root : root user home dir /tmp : accommodate temporary files, deleted by 10 days /var/tmp : another temp dir , purge every 30 days 2.3.2 Absolute and Relative paths Absolute path, is the complete path, no ambiguity, start with / : #sample cat /etc/issue Relative path is the path to a file relative the current position, do not start with / cd / cat etc/issue 2.3.3 Managing Files mkdir -p : create the parents that do not exists pwd mkdir -p dirA/dirB/dirc ls -R dirA ## /home/bruno/Documents/projects/learning_linux/book-notes-linux-learn ## dirA: ## dirB ## ## dirA/dirB: ## dirc ## ## dirA/dirB/dirc: Commands cp -r : copy dir rm -i : show to you msg to confirm rmdir : remove dir rm -r : delete dir recursive mv : move touch : create files Links Index Node ((inode_): How files are identified, keep track of: * permissions * ownership * date &amp; time stamps * paths to data on file system ls -li LICENSE ## 40652348 -rw-rw-r-- 2 bruno bruno 6556 Aug 12 2021 LICENSE 40652348 : file ID (inode) 1 : means one name using this iNode right now Hard Link ln LICENSE LIC2 ls -li LIC* ## ln: failed to create hard link &#39;LIC2&#39;: File exists ## 40652348 -rw-rw-r-- 2 bruno bruno 6556 Aug 12 2021 LIC2 ## 40652348 -rw-rw-r-- 2 bruno bruno 6556 Aug 12 2021 LICENSE Both LIC2 and LICENSE have the same iNode but the number right now is 2, not able to identify which one was create first, this is a hard link Soft Link ln -s README.md README_2.md ls -li READ* ## ln: failed to create symbolic link &#39;README_2.md&#39;: File exists ## 40643321 lrwxrwxrwx 1 bruno bruno 9 Aug 16 2021 README_2.md -&gt; README.md ## 40652371 -rw-rw-r-- 1 bruno bruno 741 Aug 12 2021 README.md Both have different iNode number, so they are two different files, the permission there are a l meaning link, this is a soft link, if we delete file 3 we have a broken link # create tmp files mkdir -p tmp ;cd tmp ;touch 1file 2file 3file 4file able alfa baker bravo cast easy echo _src # List files that match this case echo echo &quot;List files that match this case Xfile&quot; ls ?file echo &quot;----&quot; # list files that begin with a or c or e echo echo &quot;List files that begin with a or c or e &quot; ls [ace]* echo &quot;----&quot; # List files that do not start with a c or e echo echo &quot;List files that do not start with a c or e&quot; ls [^ace]* # or ls [!ace]* echo &quot;----&quot; # List all file that begin with an alphabetical character echo echo &quot;List all file that begin with an alphabetical character&quot; ls [[:alpha:]]* # List all files that begin with digit echo echo &quot;List all files that begin with digi&quot; ls [[:digit:]]* echo &quot;----&quot; # List all files that match digit or alphabetical char echo echo &quot;List all files that match digit or alphabetical char&quot; ls [[:alnum:]]* echo &quot;----&quot; # List all files that begin with punctuation echo echo &quot; List all files that begin with punctuation&quot; ls [[:punct:]]* echo &quot;----&quot; # clean cd .. rm -rf tmp ## ## List files that match this case Xfile ## 1file ## 2file ## 3file ## 4file ## ---- ## ## List files that begin with a or c or e ## able ## alfa ## cast ## easy ## echo ## ---- ## ## List files that do not start with a c or e ## 1file ## 2file ## 3file ## 4file ## baker ## bravo ## _src ## ---- ## ## List all file that begin with an alphabetical character ## able ## alfa ## baker ## bravo ## cast ## easy ## echo ## ## List all files that begin with digi ## 1file ## 2file ## 3file ## 4file ## ---- ## ## List all files that match digit or alphabetical char ## 1file ## 2file ## 3file ## 4file ## able ## alfa ## baker ## bravo ## cast ## easy ## echo ## ---- ## ## List all files that begin with punctuation ## _src ## ---- Brace expansion mkdir -p tmp ;cd tmp echo {Sun,Mon,Tues,Wednes}day.log # create dirs mkdir -p RHEL{6,7,8}; ls RHEL* # create sequence of files using .. touch song{1..5}.mp3 ; ls *.mp3 #clean cd .. rm -rf tmp ## Sunday.log Monday.log Tuesday.log Wednesday.log ## RHEL6: ## ## RHEL7: ## ## RHEL8: ## song1.mp3 ## song2.mp3 ## song3.mp3 ## song4.mp3 ## song5.mp3 Variable SOMETHING=value echo $SOMETHING # command substitution echo &quot;Today is $(date +%A)&quot; ## value ## Today is Tuesday skell The contenct of path /etc/skel is automatically copied to all users 2.4 Help All man pages are on /usr/share/man Commands man pinfo man -k cron ## anacrontab (5) - configuration file for anacron ## anacron (8) - runs commands periodically ## cron (8) - daemon to execute scheduled commands (Vixie Cron) ## crontab (1) - maintain crontab files for individual users (Vixie Cron) ## crontab (5) - tables for driving cron ## dh_installcron (1) - install cron scripts into etc/cron.* To go direct to a section , belo example of command to go direct to session 5 man 5 crontab 2.5 Text Files Channels 0 : stdin , read only used by keyboard 1 : stdout, write only, terminal 2 : stderr, write only, terminal 3+ : file name, read and write We can direct errors to a specific file like below I do not have the file or dir abc ls abc file1 2&gt; errors.log # output file1 # on errors.log ls: cannot access &#39;abc&#39;: No such file or directory We also can send both to the same file ls /show /boot &amp;&gt; combine.log # another option # ls /show /boot &gt; combine.log 2&gt;&amp;1 cat combine.log rm combine.log ## ls: cannot access &#39;/show&#39;: No such file or directory ## /boot: ## config-5.4.0-113-generic ## config-5.4.0-120-generic ## efi ## grub ## initrd.img ## initrd.img-5.4.0-113-generic ## initrd.img-5.4.0-120-generic ## initrd.img.old ## System.map-5.4.0-113-generic ## System.map-5.4.0-120-generic ## vmlinuz ## vmlinuz-5.4.0-113-generic ## vmlinuz-5.4.0-120-generic ## vmlinuz.old Another usage when we have an output with lot of errors and can only the the results, all the errors message was sent to /dev/null find / -iname passwd 2&gt; /dev/null #output /usr/share/bash-completion/completions/passwd /usr/share/doc/passwd /usr/share/lintian/overrides/passwd /usr/bin/passwd /etc/passwd /etc/pam.d/passwd A good usage is send successfully output to an output.log file and the errors to errors.log file ls /shoe /boot &gt;&gt; output.log 2&gt;&gt; errors.log Send emails with content of file, the &lt; will direct the content of file to email to &lt;user&gt; mail -s &quot;Subject text&quot; &lt;user&gt; &lt; file Pipe The resolv.conf have 6 lines in total but we can see the lines without comment # and direct the output using pipe | wc -l /etc/resolv.conf echo # check the lines without comment grep ^[^#] /etc/resolv.conf echo # combine the grep with pipe grep ^[^#] /etc/resolv.conf | wc -l ## 6 /etc/resolv.conf ## ## nameserver 127.0.0.1 ## ## 1 Another way is get the output and save on file using pipe | and tee find / -iname passwd 2&gt; /dev/null | tee find.out 2.5.1 vim Modes : Insert : i Command : default yy : Copy p : paste 5p : paste 5 times dd : delete ZZ : save and quite cw : change word x : delete character r : replace character a : append Extend command : : q! : exit without save wq : save and exit Visual : v crt + v : block column to manipulate column shift + v: line mode to select line x : delete u : undo 2.5.2 Changing shell Setting a editor # To set env EDITOR=nano crontab -e # To unset expor -n EDITOR The crotab will open on nano editor instead of vi or vim Also can set the variable EDITOR to nano and the same will hapens if we call crontab é # using export export EDITOR=nano Some .bash* files and adjust All the history are saved on .bash_history .bashrc user specific env for example: we can add more lines on history add the variable export HISTFILESIZE=2000 add export HISTTIMEFORMAT=\"%F %T \" 2.6 Managing Local Users and Groups whoami : Show your user id : show your user, groups Type of users* Super User root Account users, not used by people Regular users 2.6.1 Users /etc/passwd : file with user info grep bruno /etc/passwd #output bruno:x:1000:1000:bruno,,,:/home/bruno:/bin/bash User : Bruno x : long time ago where password where stored 1000 : User ID 1000 : group ID Comment User home dir : /home/bruno:/bin/bash The password is stored on /etc/shadow, only root can access this file Sample : 6$CSsXcYG1L/4ZfHr/$2W6evvJahUfzfHpc9X.45Jc6H30E...output omitted.. 6 : Hash algorithm used , SHA-512 CSsXcYG1L/4ZfHr/ Used by cript info W6evvJahUfzfHpc9X.45Jc6H30E has password Another way to list and return user info is using getent command getent passwd bruno #output bruno:x:1000:1000:bruno,,,:/home/bruno:/bin/bash useradd : create new user, sample : useradd kano userdel : remove user sample 1: userdel kano, but do not remove the data and home dir sample 2: userdel -r kano, delete user and data/home dir usermod -c : add comments to user usermod -a -G &lt;user&gt; -a : append to the secondary group membership, if do not use a overwritten the current info -G : group name usermod -g &lt;group&gt; &lt;user&gt; -g : change the primary group info usermod -L &lt;user&gt; : Lock a user account usermod -U &lt;user&gt; : Unlock a user account 2.6.2 Groups /etc/group : file with group info grep bruno /etc/group #output Show in New Window /etc/issue: ASCII text adm:x:4:syslog,bruno cdrom:x:24:bruno sudo:x:27:bruno dip:x:30:bruno plugdev:x:46:bruno lpadmin:x:113:bruno bruno:x:1000: sambashare:x:130:bruno bruno : group name x : pwd info that is not used 1000 : group id Members : of group Show groups from my users using command groups. groupadd : add new group (GID &gt; 1000) -g : specify the GID, groupadd -g 1000 group1 -r : create the system group (GID 0-999) groupdel : remove group groupdel &lt;group_name&gt; groupmod : modify the group -n : change the group name, groupmod -n &lt;new_name&gt; &lt;old_name&gt; 2.6.3 Gaining superuser access su : change to root super user, without set the profile, the path still your regular user su - : change to root user and set the profile and env of root sudo : allow run commands as another user sudo visudo : sudoers file Line : if you are member the group wheel you can be logged in from any computer and you can run all cmds as all users # Allows people in group wheel to run all commands %wheel ALL=(ALL) ALL % indicate a group sudo -i or sudo su - : assume as root user /etc/sudoers.d/&lt;user&gt; : drop config into that on dir /etc/sudores.d as if you are editing the /etc/sudoers Tip To create /etc/sudoers.d/admin file and grant all members of admin group total privileges echo &quot;%admin ALL=(ALL) ALL&quot; &gt;&gt; /etc/sudoers.d/admin To create just for one user echo &quot;user ALL=(ALL) ALL&quot; &gt;&gt; /etc/sudoers.d/user 2.6.4 Managing user passwords We no longer store password on /etc/passwd, the password are stored on /etc/shadow chage : used to change the aging info for a user password Sample : chage -m 1 -M 26 -W 4 -I 3 -E (2019-05-31) &lt;user&gt; -m : min wait time before user can change the pwd again, in this case 1 day -M : when they have to changed the pwd, in this case in 26 days need to change -W : warning of 4 days -I : once the pwd expire they have 3 more days to login and change the pwd -E : expiration date chage -l : show all the info /etc/login.defs : Define info of login such PASS_MAX_DAYS, PASSMIN_DAYS, etc Add user with nologin shell useradd -s &lt;user&gt; -s /sbin/nologin Tip Change the user and update expiration to more 180 days chage -E $(date -d + 180days +%Y-%m-%d) &lt;user&gt; 2.7 Controlling access to files drwxrwxrwx d : dir, can be l link, etc r | 4 : read w | 2 : write x | 1 : execute Owning User : first 3 rwx Owning Group : next 3 Other : last 3 Commands chmod : change the permission mod chmod 740 &lt;file or dir&gt; or chmod o+rw &lt;file or dir&gt; chown : change ownership chmod user:group &lt;file or dir&gt; chmod :group &lt;file or dir&gt; or chmod user &lt;file or dir&gt; Tip Give you read and execute permission do dir but no execute on files inside the dir, i.e, we can list the content but not execute files inside chmod -R a=rX 2.7.1 Special permissions Sticky bit : In a collaborative dir you can create files on the dir and only delete files that you have created u+s (suid) : Files executes as the user that owns the file, not the user that ran the file g+s (guid) : file execute as the group that owner the file, files newly create in directory have their group owner set to match the group owner of the dir. o+t (sticky): Users with write access to dir can only remove files that they own Sticky bit — directories o+t To set o+t or 1 chmod o+t &lt;dir&gt; or chmod 1770 &lt;dir&gt; To remove o-t or 0 chmod o-t &lt;dir&gt; or chmod 0770 &lt;dir&gt; # To set sticky bit chmod o+t dirA ls -ld dirA # To remove sticky bit #chmod o-t dirA s : Files created on the dir will have the same owning group of dir Set grid to dirs or files g+s To set g+s chmod g+s &lt;dir&gt; To unset g-s chmod g-s &lt;dir&gt; # To set chmod g+s dirA ls -ld dirA # To remove # chmod g-s dirA Set uid to files u+s To set u+s chmod u+s &lt;file&gt; To unset u-s chmod u-s &lt;file&gt; 2.7.2 Default permissions - umask umask : show umask info umask 0000 : set the umask to 0000 or 777 to new dirs To change the user umask we can update the .bashrc file 0022: * The permissions of files going to be 755 2.8 Monitoring and Managing Linux process Process state description 2.8.1 Commands to monitor top %CPU load average tasks ps aux , ps -ef PID : process ID PPID: Parent process ID Time CMD TTY : from where the process is running htop 2.8.2 Controlling jobs sample : gnome-calculator &amp; Command : * jobs : list all jobs * fg %&lt;job_number&gt; : Bring the job to foreground : * Ctrl+z : suspend/stop the job * ps j : show the info relate jobs * bg %&lt;job_number&gt; : restart job * Ctrl+c : terminate job 2.8.3 Killing process Signals kill -l ## 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP ## 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1 ## 11) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM ## 16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP ## 21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ ## 26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR ## 31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+3 ## 38) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+8 ## 43) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13 ## 48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12 ## 53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-7 ## 58) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-2 ## 63) SIGRTMAX-1 64) SIGRTMAX default kill -15 &lt;process&gt; die right now kill -9 &lt;process&gt; stop kill -19 &lt;process&gt; continue kill -18 &lt;process&gt; kill with -15 several with same name killall &lt;name&gt; pkill -t pts/2 : terminate the user logged on pts/2* pkill -SIGTERM tail : will kill the tail process running pstree : display tree view of process pgrep -l -u &lt;user&gt; : identify the process that going to be killed by pkill 2.8.4 Monitor process activity Load Average uptime command review the load average uptime ## 10:59:17 up 29 days, 49 min, 1 user, load average: 0,97, 1,40, 1,26 1,24 : Last min 1,07 : Last 5 min 0,93 : Last 15min To analyze the load average need to know how many CPU’s do we have lscpu | grep -i &#39;CPU(s)&#39; #or echo echo &quot;========Number of CPUs=====&quot; : cat /proc/cpuinfo | grep &quot;model name&quot; | wc -l #output CPU(s): 8 On-line CPU(s) list: 0-7 NUMA node0 CPU(s): 0-7 ========Number of CPUs===== : 8 So 1,24 / 8 is 0,155 , what means that my CPU is busy 15% of the time on last min On last 15min my cpu is 11% (0,93/8) last 15min and 13%(1,07/8) last 5min. To know if the CPU is overload the results show be 1.13 what means that my CPU is overload by 13% on that particular time Another example #From lscpu, the system has four logical CPUs, so divide # load average: 2.92, 4.48, 5.20 #divide by number of logical CPUs: 4 4 4 ---- ---- ---- per-CPU load average: 0.73 1.12 1.30 # This system&#39;s load average appears to be decreasing. # With a load average of 2.92 on four CPUs, all CPUs were in use ~73% of the time. # During the last 5 minutes, the system was overloaded by ~12%. # During the last 15 minutes, the system was overloaded by ~30% top is another command that can be used to monitor the system k : ask by the PID to be terminated M : sort by memory h : help shift + w : write the top config on /home/&lt;user&gt;/.config/procps/toprc On top : PID : process ID PR : Priority VIRT : Virtual memory that process is using RES : Physical RAM used SHR : Shared memory %CPU %MEN Time : how long it is running Command 2.9 Controlling Services and Daemons The systemd is responsable for initializing the system and uses units that represent daemons: * service : database, web service, tc * target : collection of units * device * socket systemctl list-units | head -n 10 # or # systemctl # systemctl list-units --type=service # systemctl list-units --type=target status Status of sshd : systemctl status sshd #systemctl list-units --type=service systemctl status bluetooth.service #output ● bluetooth.service - Bluetooth service Loaded: loaded (/lib/systemd/system/bluetooth.service; enabled; vendor preset: enabled) Active: active (running) since Sun 2021-08-22 08:50:54 -03; 5 days ago Docs: man:bluetoothd(8) Main PID: 906 (bluetoothd) Status: &quot;Running&quot; Tasks: 1 (limit: 18970) Memory: 2.7M CGroup: /system.slice/bluetooth.service └─906 /usr/lib/bluetooth/bluetooth Stop : systemctl stop &lt;service&gt; Start : systemctl start &lt;service&gt; Restart: systemctl restart &lt;service&gt; Reload : systemctl reload &lt;service&gt; ask : systemctl is-active &lt;service&gt; or ...is-enable..., is-failed disable: systemctl disable &lt;service&gt;, the service will not start when the system is started list dependency : systemcl list-dependecies &lt;service&gt; mask : systemctl mask &lt;service&gt; prevent service to be started unmask: systemctl unmask &lt;service&gt; 2.10 Configuring and Securing SSH On RHEL8 we have OpenSSH which implement not only the SSH daemon but also SSH command line tool ~/.ssh/known_hosts stores the fingerprint sent by server to future communication To define the StrictHostKeyChecking we can edit the file ~/.ssh/config or /etc/ssh/ssh_config 2.10.1 Configure SSH Key-based Authentication private - decrypt When you connect to ssh server using private key the remote machine will generate a challenge (encrypted with your public key), if you are able to decrypt that encrypted challenge then yo are allowed to make a connection public - encrypt when you ssh to a server the public key is saved on the SSH server 2.10.2 Create a public and private keypair Issue to start and create the Key : ssh-keygen Enter the file name Create a paraphrase Result: We going to have a public and private key created Install the key on server ssh-copy-id -i .ssh/&lt;public key name&gt; &lt;hostname&gt; Test ssh -i .ssh/&lt;public key name&gt; &lt;hostname&gt; Add the private key to agent # start the agent eval $(ssh-agent) # add the key ssh-add .ssh/&lt;public key name&gt; 2.10.3 Customizing OpenSSH Service Config Disable the root ability to login on /etc/ssh/sshd_config to modify the daemon config and change the PermitRootLogin to no Tecniques to avoid password Create a sshusers group and configure /etc/ssh/sshd_config 4 allowGroups, or Create a private key and public key 2.11 Analysing and Storing Logs Systemd is the heart of RHEL8 system and need to analyse how it is working The journal collects messages from several sources (booting, daemons, etc) and we can query using journalctl but this is not persistent by default. There are another process rsyslog that read syslog and receive systemd-journal and save it on /var/log Facility is one of the following keywords : auth, authpriv, cron, daemon, kern, lpr, mail, mark, news, security, syslog, user, uucp, local0 to local7. Priority is one of the following keywords : debug, info, notice, warning, warn, err, error, crit, alert, emerg, panic /var/log/messages : Store most of syslog messages /var/log/secure : store syslog messages related to security and authentication operations rsyslog : service that organize syslog messages into /var/log /var/log : directory of syslog files /var/log/maillog : store syslog messages related mail server /var/log/cron : store syslog messages related to the schedule jobs /var/log/boot.log : store console message related to system startup Syslog codes 0 - emerg 1 - alert 2 - crit 3 - err 4 - warning 5 - notice 6 - info 7 - debug The message is organized as facility.priority and we can configure to direct it to particular file like authpriv.notice and send to /var/log/foo Config file : /etc/rsyslog.conf, where we can add all rules Avoid edit the main config file /etc/rsyslog.conf and use drop-in directory /etc/rsyslog.d Log rotate prune framework /etc/logrotate.conf : Config file /etc/logrotate.d : drop-in dir Manual messages to syslog sample : logger -p local7.notice &quot;log entry created on host&quot; 2.11.1 Reviewing System Hournal Entries Journal is not persistent and can find the logs on /run/log/journal systemd-journald - command journalctl journalctl -r : reverse the log message, last page of msg show first journalctl -u &lt;unit&gt; : show logs from unit such as sshd.service journalctl -u &lt;unit&gt; --since today : show today message of particular unit journalctl -u &lt;unit&gt; --since \"2019-04-15 09:00:00\" --until \"2019-04-15 11:00:00\" -p warning : Show the warning message of unit in particular time journalctl --since \"2019-04-15 09:00:00\" --until \"2019-04-15 11:00:00\" -p warning : Show ALL warning message in particular time since request all warning we also going to see the err, crit, alert and emerg messages journalctl -p err : show the errors journalctl -f : to monitor the system journalctl -n 50 : to show the last 50 messages journalctl _SYSTEM_UNIT=sshd.service _PID=xx : will show to you message about this unit and PID journalctl -b -1 : show the data related one boot if the journal is persistent 2.11.2 Preserving the system journal By default the journal is not persistent on /run/log/journal and after reboot we lose all data. To preserve the journal we need to create the dir /var/log/journal, also by default journal are not allow to get more than 10% of file system or leave less than 15% of the file system free. The configuration are on /etc/journald.conf 2.11.3 Maitaining Accurate time Have system in sync is important Checking time timedatectl #output Local time: Fri 2021-08-27 13:24:50 -03 Universal time: Fri 2021-08-27 16:24:50 UTC RTC time: Fri 2021-08-27 16:24:50 Time zone: America/Sao_Paulo (-03, -0300) System clock synchronized: yes NTP service: n/a RTC in local TZ: no To list the timezones we can use the command timedatectl list-timezones and set with command: timedatectl set-timezone &lt;timezone&gt; The system will use NTP (Network Time Protocol) to synchronize the time with a machine to perform that we can set it to True timedatectl set-ntp true RedHat 7 and 8 use the chronyd.service to synchronize the time and inside of /etc/chrony.conf we going to find the list of servers that are in sync and can check the sources with chronyc sources 2.12 Managing Networking TCP/IP layers : Application : How clients communicate across plataforms, sample Web browser talking withWeb server SSH : remote login HTPS : web NFS or CIFS : file share SMTP : emal Transport : How packets are sent and received like TCP : connection-oriented bi-direction form of guaranteed messaging UDP : connectionless unidirectional and non-guaranteed messaging Internet : Specify how packets are routed acrosss network Link layers : physical askpects of networking MAC Devices : The name dependes on where and how the device is connected EN : Ethernet devices ENO1 : Onboard Ethernet Interface with index number 1 ENS3 : Ethernet device in hotplug slot 3 WLAN : Wireless LAN devices WWAN : Wireless AN devices WLP4S0 : PCI buss number 4 connect on slot 0 2.12.1 TCPIP IPV4 IP IP Address : 32bits divided up into 4 octets, this is the interface unique identity on the network Subnet mask : divide the IP address into the host portion as well as the network portion and that is used to facilitate routing Notation : 255.255.255.0 or /24 IPV6 “New implementation of IP” : Allow every single person to have its own unique IP Address IPV6 is 128-bit number with 8 colon separated groups of 4 hexadecimal nibbles, also use subnet mask and normally make use of /64, i.e, 64 bits Common ipv6 address ::1/28 : localhost ::/0 : default route fe80::/10 : link-local (start with fe80), those link are not routable, when we have IPV6 enable the link-local will be allocated automatically and allow to communicate with other machines using IPV6 on the same local segment ip a #output 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: enp3s0f1: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc fq_codel state DOWN group default qlen 1000 link/ether 80:fa:5b:4d:16:6d brd ff:ff:ff:ff:ff:ff 3: wlp4s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether f8:94:c2:74:dc:04 brd ff:ff:ff:ff:ff:ff inet 192.168.15.8/24 brd 192.168.15.255 scope global dynamic noprefixroute wlp4s0 valid_lft 35492sec preferred_lft 35492sec inet6 fe80::19da:6be7:d851:7ea9/64 scope link noprefixroute valid_lft forever preferred_lft forever The IPV6 : inet6 fe80::19da:6be7:d851:7ea9/64 scope link Interface : enp3s0f1 MAC : link/ether 80:fa:5b:4d:16:6d brd ff:ff:ff:ff:ff:ff IPV4 : inet 192.168.15.8/24 brd 192.168.15.255 scope global dynamic Commands PING tool for IPV6 : ping6 &lt;ipv6&gt;%&lt;interface&gt; Show IPv4 table : ip route Show IPv6 table : ip -6 route Show specific interface : ip a s &lt;interface Properties of interface : ip link show Link statistics : ip -s link show Files /etc/hosts : There are number of names that resolve ip address /etc/services : List of commonly used services /etc/resolv.conf : Wejre DNS service are defined 2.12.2 Validating Network Configuration Commands to review IP config ip a Tools nmcli : command line tool to manage network nmtui : graphical tool to manager network ip : to list ip config netstat : check ports and process tracepath : work similar traceourte tracepath access.redhat.com ss : information about process that are opening up listening sample : ss -plunt sample2 : ss -lt : What is listening, t for TCP 2.12.3 Configure Networking from the Command Line Tool: nmcli an interface to the Network Manager daemon that supoprt tab complition Config on : /etc/sysconfig/network-scripts High level of how network is configured : nmcli #output Show in New Window /etc/issue: ASCII text /bin/bash: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=a6cb40078351e05121d46daa768e271846d5cc54, for GNU/Linux 3.2.0, stripped Show in New Window 40652348 -rw-rw-r-- 2 bruno bruno 6556 Aug 12 21:50 LICENSE Show in New Window wlp4s0: connected to Auto ALMAX-BRUNO &quot;Intel Wireless-AC 3168NGW&quot; wifi (iwlwifi), F8:94:C2:74:DC:04, hw, mtu 1500 ip4 default, ip6 default inet4 192.168.15.8/24 route4 0.0.0.0/0 route4 169.254.0.0/16 route4 192.168.15.0/24 inet6 fe80::19da:6be7:d851:7ea9/64 route6 fe80::/64 route6 ::/0 p2p-dev-wlp4s0: disconnected &quot;p2p-dev-wlp4s0&quot; wifi-p2p, hw enp3s0f1: unavailable &quot;Realtek RTL8111/8168/8411&quot; ethernet (r8169), 80:FA:5B:4D:16:6D, hw, mtu 1500 lo: unmanaged &quot;lo&quot; loopback (unknown), 00:00:00:00:00:00, sw, mtu 65536 DNS configuration: servers: 192.168.15.1 interface: wlp4s0 servers: fe80::aec6:62ff:fefc:7110 interface: wlp4s0 Use &quot;nmcli device show&quot; to get complete information about known devices and &quot;nmcli connection show&quot; to get an overview on active connection profiles. Consult nmcli(1) and nmcli-examples(7) manual pages for complete usage details. Connection show : nmcli connection show #output NAME UUID TYPE DEVICE Auto ALMAX-BRUNO 72e81729-a7a7-473b-8577-013339a4420b wifi wlp4s0 Wired connection 1 44945eb1-1b88-305b-84c0-c0c13d340a0a ethernet -- Details about connections : nmcli connection show \"Auto ALMAX-BRUNO\" Sample of modify profile ipv4.dns # modify the dns ncli con mod &lt;connection name&gt; ipv4.dns &lt;New DNS Name&gt; # or append, in this case we will have two values of dns ncli con mod &lt;connection name&gt; +ipv4.dns &lt;New DNS Name&gt; # grep nmcli con show &lt;name&gt; | grep dns # reinitialize the profile ncli con up &lt;connection name&gt; # check /etc/resolv.conf cat /etc/resolv.conf Status: nmcli dev status #output DEVICE TYPE STATE CONNECTION wlp4s0 wifi connected Auto ALMAX-BRUNO p2p-dev-wlp4s0 wifi-p2p disconnected -- enp3s0f1 ethernet unavailable -- lo loopback unmanaged -- Add conn sample nmcli con add con-name &lt;name of connection&gt; type ethernet ifname &lt;name of connection&gt; Up and Disconnect # Activate connection interface nmcli con up &lt;connection_name&gt; # disconect interface nmcli dev dis &lt;device&gt; Delete , reload and modify # Delete nmcli con del &lt;device&gt; # reload nmcli con reload # modify nmcli con mod &lt;name&gt; List # show NetworkManager of all Net Interface nmcli dev status # show connections nmcli con show # show config nmcli con show &lt;name&gt; 2.12.4 Editing Network Configuration Files Files : We can modify the config file and perform reload nmcli con reload and nmcli con up &lt;name&gt; or use the nmcli commands Every Device have a config file into /etc/sysconfig/network-scripts/ifcfg-*, where there are the hardware address, ipaddr, gateway, domain, etc Sample after update ifcf-* file nmcli con reload mcli con down &quot;static-ens3&quot; nmcli con up &quot;static-ens3&quot; 2.12.5 Configuring Hot Names and Name Resolution Command hostname hostnamectl hostnamectl status hostnamectl status #output Static hostname: turing Icon name: computer-laptop Chassis: laptop Machine ID: bbada5cfcc9a4a80ac8dc5ce6d9f53aa Boot ID: cf3c63ed65874b238e2f825055abbf1f Operating System: Linux Mint 20.2 Kernel: Linux 5.4.0-81-generic Architecture: x86-64 To update hostname hostnamectl set-hostname &lt;new_name&gt; # check cat /etc/hostname Name resolution are om /etc/hosts , the order of resolution are on /etc/nsswitch.conf Adding additional DNS search and IP Address for IPv4 similar we can perform for IPv6 sample # add nmcli con mod &lt;connection name&gt; +ipv4.dns &lt;IP&gt; +ipv4.dns-search &lt;name&gt; # up the connection nmcli con up &lt;connection name&gt; # check cat /etc/resolv.conf 2.13 Archiving and transferring files 2.13.1 TAR Create tar file tar -cf etc.tar /etc Test tar -tf etc.tar Extract tar -xf etc.tar # Extract one file tar -xf etc.tar etc/hosts compression and tar Create tar file and compress with gzip2 tar -czf etc-backup-$(date +%F).tar.gz /etc Create a tar file and compress with bzip2 tar -cjf etc-backup-$(date +%F).tar.bz /etc Create a tar file and compress with Xzip tar -cJf etc-backup-$(date +%F).tar.xz /etc 2.13.2 SCP Securely transfer files between systems Copy dir and files from another system copy xf dir from servera to current dir ‘.’ scp -r user@servera:/xf . Copy without password # create keygen ssh-keygen -N &#39;&#39; # copy id ssh-copy-id servera # run scp command scp -r user@servera:/xf . 2.13.3 SFTP # connect sftp user@servera # list sftp&gt; ls # create dir mkdir backup cd backup # local change dir sftp&gt; lcd /etc # upload hosts file sftp&gt; put hosts 2.13.4 Synchronizing Files Between System Transfer using rsync rsync -Par servera:/xf . If we update the data into servera on xf dir next time we perform rsync it will perform incremental receiving and download only the difference 2.14 Intalling and updating software packages To have the Red Hat software update and donwload the bins from RedHat vendor we need a subscription, there are 4 basics elements : You have to register the server with Red Hat or in fact a satellite server After that need to subscribe the server to entitle it to update Enable repositories Go to lifecycle manamgent to track and manage entitlements via portal or subscription asset manager tool Commands Status : subscription-manager status Register : subscription-manager register Attach to subscription : subscription-manager attach --auto Enable repos : subscription-manager repos --disable='*'--enable='repos_name' 2.14.1 Package manager rpm : Red Hat Package Manager is a popular format for installing software rpm database : keep track of what software is installed and versions Query the rpm database to list all packages installed rpm -qa Query a particular package rpm -q &lt;package&gt; #or rpm -qi &lt;package&gt; List files associate with packages rpm -ql &lt;package&gt; Show the config files rpm -qc &lt;package&gt; Show de documentation rpm -qd &lt;package&gt; Show the script that going to be executed rpm -q -p --scripts &lt;package&gt;.rpm Install Limitations : Cannot manage dependency rpm -i &lt;package&gt; Which package provide a particular file rpm -qf &lt;file&gt; Download using yumdownloader yumdownloader &lt;package&gt; Query on rpm file, listing a list of files provided by this package rpm -qpl &lt;file&gt;.rpm Query the config files into the rpm file rpm -qpc file.rpm Extract all the files and dirs that are inside a RPM file using rpm2cio rpm2cpio &lt;file&gt;.rpm | cpio -duim 2.14.2 Inslalling and Update Software Packages with Yum Search or get info on current repos yum search &lt;pacakge&gt; yum info &lt;pacakge&gt; Query RPM files without have to download using repoquery repoquery -l &lt;package&gt; Find out what package provide a particular file yum provides &lt;file like /etc/fstab&gt; yum profides *bin/authconfig Install yum resolve all dependencies sudo yum install &lt;package&gt; #or sudo yum localinstall &lt;package&gt;.rpm Update sudo yum update &lt;package&gt; Remove Tip : do not use -y to review what going to be removed sudo yum remove &lt;package&gt; Check groups yum group list Check packages associate with a group for example Development Tools group yum group info &quot;Development Tool&quot; Install in a particular group yum group install &quot;Development Tool Log can be checked on /var/log/dnf.rpm.log and we can see the history yum history We can undo a transaction on history sudo yum history undo &lt;number_of_line&gt; Tip List if the package are installed sudo yum list httpd mod_ssl Install ^list^-y install 2.14.3 Enabling Yum Software Repositories List the repositories that we are connected and repo definition path are on /etc/yum.repos.d/ yum repolist all Enable and disable repos using yum-config-manager yum-config-manager --disable &lt;repo_name&gt; yum-config-manager --enable &lt;repo_name&gt; List subscription manager repos list subscription-manager-repos --list 2.14.4 Managing Packages Module Streams Module is a set of RPM packages List modules and get info yum module list &lt;package/module like perl&gt; yum module info &lt;package/module like perl&gt; # List installed modules yum module list --installed Install module stream yum module install &lt;package/module like perl&gt; Remove stream yum module remove &lt;package/module like perl&gt; Disable stream yum module disable perl To add we can use the structure [BASE] name=Base baseurl=httpd://...... gpgcheck=0 Also if we have config-manager yum config-manager --add-repo &lt;URL&gt; 2.15 Accessing Linux File Systems df -h : overview of various file systems blkid /dev/vda1 : show the UUID (universally-unique identifier) of file system findmnt : tree overview of file system starting at roots lsblk : show overview of the various block devices like /dev/vda or /dev/sda lsblk -fp /dev/vdb check the UUID lsblk #Output NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 931,5G 0 disk ├─sda1 8:1 0 512M 0 part /boot/efi ├─sda2 8:2 0 915,1G 0 part /run/timeshift/backup └─sda3 8:3 0 15,9G 0 part [SWAP] Check the entire disk $ls -l /dev/sda brw-rw---- 1 root disk 8, 0 Aug 22 08:50 /dev/sda [SWAP] b : Indicate that this is a block based device du -sh : check space that has being occupied by a directory s : summary h : human read format $du -sh /var/log 1,3G /var/log 2.15.1 Mount and Unmounting File systems To Mount Check the UUID of block device using blkid /dev/vb1 for instance Create directory : mkdir -p &lt;path&gt; Mount : mount UUID=\"XXX\" /&lt;path&gt; Check the return code echo $? (0 means successfully) Check with df -h or mount command To Unmounting umount &lt;/dev/vdb1&gt; 2.15.2 Locating Files on the System find Basic find syntax : find &lt;where&gt; &lt;how&gt; &lt;what is lokking for&gt; Some examples : find / -name ssh_config : looking for particular name find / -iname ssh_config: using i going to use case insensitive search find /usr -iname \"*.pdf\" : looking all files that end with pdf find / -user &lt;user_name&gt; : find files for a particular user find / -user &lt;user_name&gt; -delete : find and delete find / -type f -user rick -size 10M : find files from user rick with 10MB (+10M more than 10MB , -10M less than 10MB) find /home -size +10M -exec ls -lh {} \\; : find on home files more than 10MB and list with ls -lh find /home -size +10M iname \"*.mkv\" -exec rm -f {} \\; : find files on home with more than 10MB ending with .mkv and execute rm -f on each file, similar find /home -size +10M iname \"*.mkv\" -delete` find /home -type f -perm /111 : find files on home that have execute permission (111) find /home -min -60 : find files modified in last 60min find / -user &lt;user_name&gt; -exec -cp {} /&lt;path_copy&gt;/ \\; locate pre-req : run the command updatedb to create an index of the files on file system locate &lt;file&gt; : basic locate a file 2.16 Analysing Server and Getting Support cockpit service used to analysing and managing remote servers, this is a web user interface that use TCP 9090 Enable : systemctl enable --now cokpit.socket Reload firewall daemon and set permanent : firewall-cm --add-service cockpit --permanent and firewall-cmd --reload link : https://:9090/system 2.16.1 Deteting and Resolving issues with Red Hat Insights It is a form of AI which is available as a software as a service Install Insights Register and attach the server on subscription-manager sudo subscription-manager register --auto-attach Install client Insights and register sudo yum install -y insights-client sudo insights-client --register Go to insights dashboard cloud.readhat.com/insight 2.17 Extra 2.17.1 Create 8G file disk Use dd command to create 8G disk file dd if=/dev/zero of=&lt;/path/diskfile.img&gt; bs=1M count=8192 create loop device loseup -fP diskfile.img list the device losetup -a 4.Can use new diskfile as a device fdisk /dev/loop0 2.17.2 Find users wit Set UID find / -perm /4000 -exec ls -l {} \\ 2&gt; /dev/null 2.17.3 Show dump device xxd -l 512 /dev/sda | less 2.17.4 Configure labels of FS tune2fs -L &lt;label&gt; /dev/sda&lt;number&gt; "],["server-administrator-ii-rh134.html", "# 3 Server Administrator II - RH134 3.1 Improving Command Line Productivity 3.2 Scheduling Future Tasks 3.3 Tuning System Performance 3.4 Controling Access to Files with ACLs 3.5 Managing SELinux Security 3.6 Managing Basic Storage 3.7 Managing Logical Volumes 3.8 Implementing Advanced Storage Features 3.9 Accessing Network-Attached Storage 3.10 Controlling The Boot Process 3.11 Managing Network Security 3.12 Installing Red Hat Enterprise Linux 3.13 Running Containers", " # 3 Server Administrator II - RH134 3.1 Improving Command Line Productivity Writing Simple Bash Scripts The Bash interpreter start in the first line of script with #! /bin/bash and to run the script we need : Config the execute permission on script Have the script on $HOME/bin and call the script name or any other bin path inside your $PATH LOOPS Syntax of bash for loop for &lt;VARIABLE&gt; in &lt;LIST&gt;; do &lt;COMMAND&gt; &lt;VARIABLE&gt; done Sample: #1 for HOST in host1 host2 host3; do echo $HOST; done #2 for HOST in host{1,2,3}; do echo $HOST; done #3 for HOST in host{1..3}; do echo $HOST; done #4 for FILE in file* ; do ls $FILE; done #5 for FILE in file{a..c}; do ls $FILE; done #6 for PACKAGE in $(rpm -qa | grep kernel); \\ do echo &quot;$PACKAGE was installed on \\ $(date -d @$(rpm -q -qqf &quot;%{INSTALLTIME} \\n&quot; $PACKAGE))&quot;; done Sequence Sample : #1 seq 2 2 10 output 2 4 6 8 10 #2 for EVEN in $(seq 2 2 10); do &quot;$EVEN&quot;;done Exit Codes 0 : successfully 0 : not successfully 0 to 255 : range To display the exit code echo $? IF / THEN #1 if &lt;condition&gt;; then &lt;STATEMENT&gt; ... &lt;STATEMENT&gt; else &lt;STATEMENT&gt; ... &lt;STATEMENT&gt; fi #2 if &lt;condition&gt;; then &lt;STATEMENT&gt; ... &lt;STATEMENT&gt; elif &lt;condition&gt;; then &lt;STATEMENT&gt; ... &lt;STATEMENT&gt; else &lt;STATEMENT&gt; fi Sample systemctl is-active psacct &gt; /dev/null 2&gt;&amp;1 if [ $? -ne 0 ] ; then sudo systemctl start psacct else sudo systemctl stop psacct fi Regex to match text in command outputs Regex can be used with command as vim, grep and less There are several options below some samples : Show me lines that starts with cat using ^ grep &#39;^cat&#39; &lt;FILE&gt; Show me lines that ends with cat using $ grep &#39;cat$&#39; &lt;FILE&gt; Show me lines that start and end with cat using ^ and $ grep &#39;^cat$&#39; &lt;FILE&gt; Whildcard for any character c.t grep &#39;^c.t$&#39; &lt;FILE&gt; Output cat cit cot cpt cst Show me lines with one character of list in [xxx] grep &#39;^c[aou]t$&#39; &lt;FILE&gt; output cat cot cut Show me lines with c + &lt;characters&gt; + t grep &#39;c.*t&#39; &lt;FILE&gt; output cat zuchetto zymochemistry Show me lines with c + &lt;2 characters&gt; + t Simple way ‘c..t’ but we can use modifiers : grep &#39;c.\\{2\\}t&#39; &lt;FILE&gt; #output Yacolt Zacynthus zoocultural Show me lines that starts with c + &lt;starts with 2 up to 3 characters&gt; + t grep &#39;c.\\{2,3\\}t&#39; &lt;FILE&gt; #output zirconate zoophysicist zuchetto zygocactus zoocyst grep Options -i case insensitivity -v invert the search -E search for ln and nx in particular file grep -E 'ln|nx file' Do not show lines that starts with # ^# or blank lines ^$ grep -vE &#39;^#|^$&#39; file Do not show lines that starts with # or ; grep -v &#39;^[#;]&#39; file 3.2 Scheduling Future Tasks One time Job : at command at TIMESPEC command to schedule a new job Samples at now + 5min at teatime tomorrow (teatime is 16:00) at noon + 4 days at 5pm august 3 2021 Cool options -g specify a queue g , queue goes a to z *atq to check the jobs schedules on my queue To inspect all info about job at -c &lt;job number&gt; To remove a job atrm &lt;job number&gt; To monitor a job or queue watch atq Recurring Jobs cron command USER Basic commands : crontab -l : list the jobs for the current user crontab -r : Remove all jobs from current user crontab -e : Edit jobs crontab &lt;filename&gt; : Remove all jobs and replace with the job read from filename, if no file is specified stdin is used Once the job is schedule will have a file on /var/spool/cron/ To check the structure of contrab schedule we can check the file /etc/crontab cat /etc/crontab #output SHELL=/bin/sh PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin # Example of job definition: # .---------------- minute (0 - 59) # | .------------- hour (0 - 23) # | | .---------- day of month (1 - 31) # | | | .------- month (1 - 12) OR jan,feb,mar,apr ... # | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat # | | | | | # * * * * * user-name command to be executed Other options # every 5min */5 * * * * &lt;xxx&gt; # every 5min, between 9am and 5pm Sun and Wed in July */5 9-16 * Jul sun, wed Logs : Can check the cron jobs logs on /var/log/cron SYSTEM cron jobs There are some ways to have a system cron jobs /etc/crontab Simple add the job on file /etc/cron.d/ Add a file with job schedule on this path /etc/cron/ run the command to check ls /etc/cron\\.* we can have folders with cron job files or scripts The backup mechanism of cron files is anacron and it is configured on /etc/anacrontab and the purpose is make sure that all important jobs always run There are different files on /var/spool/anacron for each daily, weekly and monthly jobs with timestamps based on schedule if there were a job that should be perform and machine is offline the job will be triggered Systemd Timer Unit This is a new scheduling function introduced on RHEL7, sample : The sysstat package provides a systemd timer unit called sysstat-collect.timer to collect system statistics every 10 minutes. The following output shows the configuration lines of /usr/lib/systemd/system/sysstat-collect.timer. ...output omitted... [Unit] Description=Run system activity accounting tool every 10 minutes [Timer] OnCalendar=*:00/10 [Install] WantedBy=sysstat.service To check the timers on system systemctl --type timer If need to modify we can do on /etc/systemd/system and after change the timer unit config files need to reload the daemon and activate the timer unit # Reload systemctl daemon-reload # Activate systemctl enable --now &lt;unitname&gt;.timr Managing Temporary Files We can configure timers that manages temporary files. Some applications use /tmp to hold temp data Others use use specific locations such as daemon and user-specific volatile dirs under /run, when system reboot those volatile store will be gone. The tool systemd-tmpfiles provide structured and configurable method to manage temp dirs and files When systemd starts a system, one of the first service units launched is systemd-tmpfiles-setup. This service runs the command systemd-tmpfiles --create --remove This command reads configuration files from : /usr/lib/tmpfiles.d/*.conf, /run/tmpfiles.d/*.conf, and /etc/tmpfiles.d/*.conf. Any files and directories marked for deletion in those configuration files is removed, and any files and directories marked for creation (or permission fixes) will be created with the correct permissions if necessary. Cleaning Temporary Files with a Systemd Timer The systemd timer unit called systemd-tmpfiles-clean.timer triggered systemd-tmpfiles-clean.service on regular interval, which executes the command to clean systemd-tmpfiles --clean To view the contents of the systemd-tmpfiles-clean.timer config files: systemctl cat systemd-tmpfiles-clean.timer If need to check the parameter frequence of clean up we need to make sure to reload and enable the timer systemctl daemon-reload systemctl enable --now systemd-tmpfiles.clean.timer Cleaning Temporary Files Manually Command : systemd-tmpfiles --clean this command wll purge all files which have not been accessed, changed, or modified more recently than max age defined on config file The format of config file systemd-tmpfiles is detailed in the tmpfiles.d manual page Sample: create the /run/systemd/seats directory if it does not yet exist, owned by the user root and the group root, with permissions set to rwxr-xr-x. This directory will not be automatically purged. #Type, Path, Mode, UID, GID, Age, and Argument d /run/systemd/seats 0755 root root - Create the /home/student directory if it does not yet exist. If it does exist, empty it of all contents. When systemd-tmpfiles –clean is run, remove all files which have not been accessed, changed, or modified in more than one day. #Type, Path, Mode, UID, GID, Age, and Argument D /home/student 0700 student student 1d Create the symbolic link /run/fstablink pointing to /etc/fstab. #Never automatically purge this line. #Type, Path, Mode, UID, GID, Age, and Argument L /run/fstablink - root root - /etc/fstab Configuration File Precedence The config file can exists in three places: /etc/tmpfiles.d/*.conf provided by the relevant RPM packages, should not edit /run/tmpfiles.d/*.conf volatile files, used by daemons /usr/lib/tmpfiles.d/*.conf 3.3 Tuning System Performance tuned daemon allow us optmize system performance by selection a tunning profile To install and enable tuned yum install tuned systemctl enable --now tuned 3.3.1 Profiles balanced : Ideal for systems that require a compromise between power saving and performance. desktop : Derived from the balanced profile. Provides faster response of interactive applications. throughput-performance : Tunes the system for maximum throughput. latency-performance : Ideal for server systems that require low latency at the expense of power consumption. network-latency : Derived from the latency-performance profile. It enables additional network tuning parameters to provide low network latency. network-throughput : Derived from the throughput-performance profile. Additional network tuning parameters are applied for maximum network throughput. powersave : Tunes the system for maximum power saving. oracle : Optimized for Oracle database loads based on the throughput-performance profile. virtual-guest : Tunes the system for maximum performance if it runs on a virtual machine. virtual-host : Tunes the system for maximum performance if it acts as a host for virtual machines 3.3.2 Managing profiles from command line To active tuned-adm active To List all available profiles tuned-adm list To switch the active profile to a different one tuned-adm profile &lt;profile_name&gt; tuned-adm active To have a recommendation of profile tuned-adm recommened To deactivate tuned-adm off tuned-adm active 3.3.3 Influencing Process Scheduling Prioritize or de-prioritize specific process with nice and renice Nice values 19 : Nicest (lowest priority) 0 : Neutral -20: Least nice (highest priority) Display Nice Levels from the command line ps axo pid, comm, nice, cls --sort=-nice Start process with different Nice levels # default nice is 10 nice sha1sum /dev/zero &amp; # setting to 15 nice -n 15 sha1sum &amp; Change the Nice level of existing process renice -n &lt;level&gt; &lt;process number&gt; 3.4 Controling Access to Files with ACLs ACLs Access Control List are extention of permissions To check if file have ACL we going to see a + on long list ouput -rwxrw----+ 1 user operators 130 Mar 19 23:56 reports.txt The group permission on ls -l is masked, will not be the real permission of this file need to check ACL settings Changing group permissions on a file with an ACL by using chmod does not change the group owner permissions, but does change the ACL mask. Use setfacl -m g::perms file if the intent is to update the file’s group owner permissions. View File or Directory ACLs using getfacl command [user@host content]$ getfacl reports.txt # file: reports.txt # owner: user # group: operators user::rwx user:consultant3:--- user:1005:rwx #effective:rw- group::rwx #effective:rw- group:consultant1:r-- group:2210:rwx #effective:rw- mask::rw- other::--- The ACL MASK Defines the maximum permissions that you can grant. It does not restrict permissions of the file owner or other user. all files and directories that implement ACL will have an ACL mask. By default , the mask is recalculated whenever any of the affected ACLs are added, modified or deleted Changing ACL file permissions setfacl $ setfacl -m u:name:rX file -m : modify , x delete u : user, g for group, o for others name: name of user rX : permission X uppercase can be used to indicate that execute permission should only be set on dir and not regular files, unless the file already has the relevant execute permission. ACL recursive modifications $ setfacl -R -m u:name:rX directory Deleting ACL $ setfacl -x u:name,g:name file Delete all ACL entries $ setfacl -b file 3.5 Managing SELinux Security Security Enhanced Linux (SELinux) is an additional layer of system security. The primary goal of SELinux is to protect user data from system services that have been compromised Targeted policy default SELinux consists of sets of policies, defined by the application developers, that declare exactly what actions and accesses are proper and allowed for each binary executable, configuration file, and data file used by an application. Modes Enforcing : SELinux is enforcing access control rules. Computers generally run in this mode. Permissive : SELinux is active but instead of enforcing access control rules, it records warnings of rules that have been violated. This mode is used primarily for testing and troubleshooting Disabled : SELinux is turned off entirely: no SELinux violations are denied, nor even recorded. Discouraged! To show SELinux information ls -lZ Checking the current state [user@host ~]# getenforce Enforcing Check the persistently state cat /etc/selinux/config Set enforce mode [user@host ~]# setenforce usage: setenforce [ Enforcing | Permissive | 1 | 0 ] [user@host ~]# setenforce 0 [user@host ~]# getenforce Permissive Check status sestatus 3.5.1 Controlling SELinux File Contexts When copy a file or create a new file the file inherits the SELinux context of the directory If I move a file it retain the SELinux context Change context of a file Commands semanage : create the role, a create page is man semanage-fcontext , sample of rule : # create the rule semanage fcontext -a -t httpd_sys_content_t &quot;/web(/.*)?&quot; # apply a new rule restorecon -R -v /web fcontext : used to list and see changes # l : list # C : change semage fcontext -lC restorecon : use when the file already in the correct location, for example, if someone moved the file from A to B and B is the correct location, using the command restorecon &lt;file&gt; will restore the context chcon : It is not persistent , does not survive restorecon or relable, avoid 3.5.2 Adjusting SELinux Policy with Booleans SELinux booleans are switches that change the behavior of the SELinux policy. SELinux booleans are rules that can be enabled or disabled. Check booleans [user@host ~]$ getsebool -a abrt_anon_write --&gt; off abrt_handle_event --&gt; off abrt_upload_watch_anon_write --&gt; on antivirus_can_scan_system --&gt; off antivirus_use_jit --&gt; off ...output omitted... [user@host ~]$ getsebool httpd_enable_homedirs httpd_enable_homedirs --&gt; off # list booleans [user@host ~]$ sudo semanage boolean -l | grep httpd_enable_homedirs httpd_enable_homedirs (on,on) Allow httpd to enable homedirs write pending values to policy, P(persistence) setsebool -P httpd_enable_homedirs on list booleans with current state and diff from default state [user@host ~]$ sudo semanage boolean -l -C SELinux boolean State Default Description cron_can_relabel (off,on) Allow cron to can relab 3.5.3 Investigating and REsolving SELinux issues sealert display info during SELinux troubleshooting Guide to troubleshooting SELinux issues: Before thinking of making any adjustments, consider that SELinux may be doing its job correctly by prohibiting the attempted access. The most common SELinux issue is an incorrect file context, “when we move files”. Another remedy for overly restrictive access could be the adjustment of a Boolean. It is possible that the SELinux policy has a bug that prevents a legitimate access. Monitor SELinux Violations If we have the package setroubleshoot-serve installed /var/log/audit/audit.log : received log messages related SELinux violations /var/log/messages : short summary of SELinux violations messages 3.6 Managing Basic Storage 3.6.1 Adding Partition, File Systems and Persistent Mounts MBR Partitioning Schema The Master Boot Record (MBR) partitioning schema dictated how disks are partitioned on system running BIOS firmware Can have manimum of four primary partition Maximum disk and partition size of 2TiB Using extended and logical partitions we can create a maximum of 15 partitions GPT Partitioning Schema The Unified Extensible Firmware Interface (UEFI) firmware is a standard laying out Does not have limit of 2TB Maximum of 128 partitions Maximum of 8ZiB zebibytes have a backup on the end of the disk Managing Partitions with Parted Partition editors are programs which allow admin to make changes to a disk’s parted command Display info of /dev/vda with subcommand print if do not provide subcommand will open an interactive session we can change the display to KB, MB, GB, TB or S for sector using unit s argument parted makes the change immediately [root@host ~]# parted /dev/vda print Model: Virtio Block Device (virtblk) Disk /dev/vda: 53.7GB Sector size (logical/physical): 512B/512B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 1049kB 10.7GB 10.7GB primary xfs boot 2 10.7GB 53.7GB 42.9GB primary xfs Writing the partition table on a NEW DISK MBR [root@host ~]# parted /dev/vdb mklabel msdos GPT [root@host ~]# parted /dev/vdb mklabel gpt The mklabel subcommand wipes the existing partition table. Only use mklabel when the intent is to reuse the disk without regard to the existing data. If a new label changes the partition boundaries, all data in existing file systems will become inaccessible. Creating MBR Partitions Specify the disk device to create the partition on using parted # parted /dev/vdb Use the mpart subcommand to create new primary or extented partition (parted) mkpart Partition type? primary/extended? primary Indicate the file-system type, to get the list of all types use parted /dev/vdb help mkpart File system type? [ext2]? xfs Specify the sector on disk Start? 2048s Specify where the new partition will end End? 1000MB Exit using quit Run the udevadm settle for the system detect the new partition Option: if we have all the details we can use only one command to perform all actions [root@host ~]# parted /dev/vdb mkpart primary xfs 2048s 1000MB Creating GPT Partitions Specificy the disk device to create the partition on # parted /dev/vdb Use the mkpart to start creating the new partition, with GPT each partition is given a name (parted) mkpart Partition name? []? usersdata Indicate the type File system type? [ext2]? xfs Specify the sector on disk that the new partition starts on Start? 2048s Specify the end End? 1000MB Exit using quit Run the udevadm settle for the system detect the new partition Option: if we have all the details we can use only one command to perform all actions [root@host ~]# parted /dev/vdb mkpart usersdata xfs 2048s 1000MB Deleting Partitions Specify the disk [root@host ~]# parted /dev/vdb Identify the partition number of the partition to delete (parted) print Delete using rm subcomand The rm subcommand immediately deletes the partition from the partition table on the disk. (parted) rm &lt;number&gt; Exit using quit Creating File Systems Check if fdisk for MBR and gdisk for GPT are installed $ which fdisk # check the package $ rpm -qf /sbin/fdisk # install gdisk yum -y install gdisk Sample of using gdisk $ gdisk /dev/vdd ? : help p : print n : new partition Specify the partition number, First sector and Last sector or size Specify the GUID Label L : show all label codes print to check c : change the partition name w : write Confirm Check with lsblk Create the file system using mkfs.xfs $ mkfs.xfs /dev/vdb1 To check the file system creation using blkid will show the UUID $ blkid To mount temporarily mount mount /dev/vdb1 /mnt Persistently mount update /etc/fstab Reload the daemon systemctl daemon-reload check fs lsblk --fs create the dir mkdir &lt;dir&gt; mount the fs mount &lt;fs&gt; Add label to FS tune2fs -L &lt;label&gt; /dev/sda&lt;number&gt; Show dump device xxd -l 512 /dev/sda | less 3.6.2 Managing Swap Space A swap space is an area of a disk under the control of the Linux kernel memory management subsystem. The kernel uses swap space to supplement the system RAM by holding inactive pages of memory. The combined system RAM plus swap space is called virtual memory. Creating a Swap Space Create a partition with a file system type of linux-swap Run udevadm settle Formatting the device mkswap /dev/vdb2 Add on /etc/fstab #sample UUID=39e2667a-9458-42fe-9665-c5c854605881 swap swap defaults 0 0 Activate the swap swappon /dev/vdb2 # activate all the swap spaces swapon -a # check swap swapon -s Setting Swap Space Priority default is -2 Update /etc/fstab and specify pri=priority number instead of defaults 3.7 Managing Logical Volumes 3.7.1 Creating Logical Volumes Using Logical volumes is easier to manage disk space, we can allocate to logical volume free space from volume group and file system can be resized LVM Definitions Physical devices Storage devices used to save data stored in a lofical volume Could be a disk partition, whole disk, RAID arrays or SAN Disk Device must be initialized as an LVM Physical volumes (PVs) “Physical” storage used with LVM We must initilize a device as a physical volume before use as LVM PV can only be allocated to a single VG Volume group (VGs) Storage pool made up of one or more physical volumes A VG can consiste of unused space an any number of logical Volumes Logical volumes (LVs) Create from free physical extents in a volume group and provide the storage used by applications, users and OS Collection of logical extents (LEs), which map to physical extents Steps to create logical volumes Summary: 1. Create partition 2. pvcreate 3. vgcreate 4. lvcreate 5. mkfs or mkswap 6. mount Use lsblk, blkid or cat /proc/partition to identify the devices Prepare the physical device Use parted, gdisk or fdisk tp create a new partition for use with LVM Type of Linux LVM on LVM partitions Use 0x8e for MBR Use partprobe to register the new partition with the kernel Create a physical volume pvcreate to label the partition as a physical volume, it divides the physical volume into physical extents (PEs) of fixed size of 4MB block. pvcreate /dev/vdb2 /dev/vdb1 Create the volume group vgcreate used to collect one or more physical volumes into a volume group. It is equivalent of hard disk -s option specify the extend size vgcreate vg01 /dev/vdb2 /dev/vdb1 -s 4M This creates a VG called vg01 that is the combined size, in PE units, of the two PVs /dev/vdb2 and /dev/vdb1 Create a logical volume Use lvcreate to create a new logical volume from available physical extents in a volume group -n : to set the LV name -L : to set the LV size in bytes -l : to set the LV size in extents lvcreate -n lv01 -L 700M vg01 Add the file system Use mkfs to create an XFS file system on the new logical volume # create FS mkfs -t xfs /dev/vg01/lv01 # create dir mount point mkdir /mnt/data # Update /etc/fstab /dev/vg01/lv01 /mnt/data xfs defaults 1 2 # mount mount /mnt/data Remove a Logical Volume Umount the fs and remove the info from /etc/fstab umount /mnt/data Remove the logical volume lvremove /dev/vg01/lv01 Remove the volume group vgremove vg01 Remove the physical volumes pvremove /dev/vdb2 /dev/vdb1 Review LVM Status Info Physical Volumes pvdisplay /dev/vdb1 Volumes Groups vgdisplay vg01 Logical Volumes lvdisplay /dev/vg01/lv01 3.7.2 Extending and Reducing Logical Volumes Extending Volume groups : We can add more disk space to a volume group by adding additional physical volumes. Then assign the new physical extents to logical volumes. Reducing the volume group : We also can remove unused physical volume from a volume group * First use pvmove to move data from extents on one physical volume to extents on another physical extents Extending a Volume group Prepare the physical device and create the physical volume [root@host ~]# parted -s /dev/vdb mkpart primary 1027MiB 1539MiB [root@host ~]# parted -s /dev/vdb set 3 lvm on [root@host ~]# pvcreate /dev/vdb3 A PV only needs to be created if there are no PVs free to extend the VG. Use vgextend to add the new physical volume to the volume group [root@host ~]# vgextend vg01 /dev/vdb3 vgdisplay to confirm the additional physical extents are available [root@host ~]# vgdisplay vg01 Inspect the Free PE / Size Usually after that you allocated the new space on FS, the -r option will extend the file system lvextend -r -L 100G /dev/volume_name/lv_name To allocated all free space can use -l +100%FREE Reducing a Volume Group Use pvmode PV_DEVICE_NAME to relocate any physical extents [root@host ~]# pvmove /dev/vdb3 This command moves the PEs from /dev/vdb3 to other PVs with free PEs in the same VG. Always backup the data before pvmove Reduce the volume using vgreduce [root@host ~]# vgreduce vg01 /dev/vdb3 This removes the /dev/vdb3 PV from the vg01 VG and it can now be added to another VG. Alternatively, pvremove can be used to permanently stop using the device as a PV Extending a Logical Volume and XFS File System Verify that the volume group has space available. [root@host ~]# vgdisplay vg01 Extend the logical volume with lvextendLV_DEVICE_NAME [root@host ~]# lvextend -L +300M /dev/vg01/lv01 Extend the file system using xfs_growfs mountpoint [root@host ~]# xfs_growfs /mnt/data Verify the new size of file system [root@host ~]# df -h /mountpoint Extending a Logical Volume and ext4 File System Verify that the volume group has space available. [root@host ~]# vgdisplay vg01 Extend the logical volume with lvextend LV_DEVICE_NAME [root@host ~]# lvextend -L +300M /dev/vg01/lv01 Extend the file system [root@host ~]# resize2fs /dev/vg01/lv01 Extend a logical volume and swap space Verify that the volume group has space available. [root@host ~]# vgdisplay vg01 Deactivate the swap space. swapoff -v /dev/vgname/lvname Extend the logical volume with lvextend LV_DEVICE_NAME [root@host ~]# lvextend -L +300M /dev/vg01/lv01 Format the logical volume as swap space. mkswap /dev/vgname/lvname Activate the swap space swapon -va /dev/vgname/lvname 3.8 Implementing Advanced Storage Features 3.8.1 Managing Storage with Stratis STRATIS is a new storage-management solution for Linux, runs as a service that manages pools of physical storage devices and transparently creates and manages volumes for the newly created file system. Instead of immediately allocating physical storage space to the file system when it is created, Stratis dynamically allocates that space from the pool as the file system stores more data We can create multiple pools from different storage devices. File systems created by Stratis should only be reconfigured with Stratis tools and commands. To use we need to install the stratis-cli and stratisd To install Stratis [root@host ~]# yum install stratis-cli stratisd [root@host ~]# systemctl enable --now stratisd Create pools of one or more block devices using the stratis pool create command. [root@host ~]# stratis pool create pool1 /dev/vdb To view the list of available pools [root@host ~]# stratis pool list To add additional block devices to a pool [root@host ~]# stratis pool add-data pool1 /dev/vdc To view the block devices of a pool [root@host ~]# stratis blockdev list pool1 To create a file system from a pool [root@host ~]# stratis filesystem create pool1 fs1 To view the list of available file systems [root@host ~]# stratis filesystem list To create a snapshot [root@host ~]# stratis filesystem snapshot pool1 fs1 snapshot1 To mount the Stratis file system persistently # get UID [root@host ~]# lsblk --output=UUID /stratis/pool1/fs1 # add on /etc/fstab UUID=31b9363b-add8-4b46-a4bf-c199cd478c55 /dir1 xfs defaults,x-systemd.requires=stratisd.service 0 0 The x-systemd.requires=stratisd.service mount option delays mounting the file system until after systemd starts the stratisd.service during the boot process 3.8.2 Compressing and Deduplicating Storage with VDO Virtual Data Optimizer (VDO) is a Linux device mapper driver that reduces disk space usage on block devices, and minimizes the replication of data. Kernel modules kvdo : Control data compression uds : Deduplication VDO phases to reduce the footprint on storage Zero-block Elimination filter out data that contain only zeros Deduplication eliminate redudant blocks Compression the kvdo compress the data block using LZ4 Install the vdo and kmod-kvdo [root@host ~]# yum install vdo kmod-kvdo Check status of vdo service systemctl status vdo Creating VDO volume [root@host ~]# vdo create --name=vdo1 --device=/dev/vdd --vdoLogicalSize=50G Analyzing a VDO Volume [root@host ~]# vdo status --name=vdo1 Display the list of VDO vdo list Stop / Start vdo vdo stop vdo start When the logical size of a VDO volume is more than the actual physical size, you should proactively monitor the volume statistics to view the actual usage using the vdostats –verbose command. vdostats --human-readable TIPS To test we might need to create a 2G file, below a sample dd if=/dev/urandom of=&lt;path&gt;/&lt;file_name&gt; bs=1M count=2048 If made a mistake on fstab and need to access the server with root and root home dir is mounted as read only we can remount using below command mount -o remount, rw / 3.9 Accessing Network-Attached Storage 3.9.1 Mounting Network-Attached Storage with NFS NFS servers export shares (directories). NFS clients mount an exported share to a local mount point (directory), which must exist. NFS shares can be mounted a number of ways: Manually, using the mount command Automatically at boot time using /etc/fstab On demand, using autofs or systemd.automount. Mounting NFS Shares Identify NFS Shares [user@host ~]$ sudo mkdir mountpoint [user@host ~]$ sudo mount serverb:/ mountpoint [user@host ~]$ sudo ls mountpoint Mount point use mkdir to create a mount point [user@host ~]$ mkdir -p mountpoint Mount Temporarily [user@host ~]$ sudo mount -t nfs -o rw,sync serverb:/share mountpoint -t nfs : file system type for NFS Share -o sync: immediately sync write operations with server Persistently Configure /etc/fstab [user@host ~]$ sudo vim /etc/fstab serverb:/share /mountpoint nfs rw,soft 0 0 Mount the NFS Share [user@host ~]$ sudo mount /mountpoint Unmounting NFS Shares [user@host ~]$ sudo umount mountpoint 3.9.2 Automounting Network-Attached Storage The automounter is a service (autofs) that automatically mounts NFS shares “on-demand”, and will automatically unmount NFS shares when they are no longer being used. Create an automount Install autofs [user@host ~]$ sudo yum install autofs Add a master map file to /etc/auto.master.d [user@host ~]$ sudo vim /etc/auto.master.d/demo.autofs ## add /shares /etc/auto.demo Create the mapping files [user@host ~]$ sudo vim /etc/auto.demo # add work -rw,sync serverb:/shares/work Start and enable the automounter service [user@host ~]$ sudo systemctl enable --now autofs Direct Maps Direct maps are used to map an NFS share to an existing absolute path mount point. To use the master map file should be like : /- /etc/auto.direct The content for the /etc/auto.direct will be : /mnt/docs -rw,sync serverb:/shares/docs Indirect Wildcard Maps * -rw,sync serverb:/shares/&amp; 3.10 Controlling The Boot Process 3.10.1 Selecting the boot target Describing the Red Hat Enterprise Linux 8 Boot Process The machine is powered on the UEFI or BIOS runs Power On Self Test (POST) and start, F2 or some system Esc usually to access configuartion and E to edit Firmware search for a bootable device The firmware read a boot loader and passes control of system to the boot loader, in RHEL 8 GRand Unified Bootloader version 2 (GRUB2) configured using grub2-install GRUB2 load the config from /boot/grub2/grub.cfg and display the menu, where we can select the kernel. Configured using the /etc/grub.d/ directory, the /etc/default/grub file, and the grub2-mkconfig command to generate the /boot/grub2/grub.cfg file After timeout or select the kernel boot loader load the kernel and initramfs set of initialization configured on /etc/dracut.conf.d/ using dracut and lsinitrd command Kernel initializes all hardware and execute the /sbin/init from initramfs as PID 1. The systemd execute all instances for the initrd.target such as mount root file system. Rebooting and Shutting Down systemctl poweroff : Stops all running services, unmounts all file system and power down the system systemctl reboot : Stops all running services, unmount al file system and then reboot the system systemctl halt : Stop the system, but do not power off the system Selecting a System Target systemd targets is a set of systemd units that system should start to reach the desired state graphical.target : supports multiple users, graphical-and text logins multi-user.target: system supports multiple users, text logins only rescue.target : sulogin prompt, basic system initialization completed emergency.target: sulogin prompt, initramfs pivot complete and system root mounted on / read only List dependencies for graphical.target systemctl list-dependencies graphical.target | grep target # output graphical.target ● └─multi-user.target ● ├─basic.target ● │ ├─paths.target ● │ ├─slices.target ● │ ├─sockets.target ● │ ├─sysinit.target ● │ │ ├─cryptsetup.target ● │ │ ├─local-fs.target ● │ │ └─swap.target ● │ └─timers.target ● ├─getty.target ● └─remote-fs.target To list the available targets : systemctl list-units --type=target --all Switch to a different target using systemctl isolate [root@host ~]# systemctl isolate multi-user.target Isolating a target stops all services not required by that target (and its dependencies), and starts any required services not yet started Get the default target [root@host ~]# systemctl get-default Setting a Default Target [root@host ~]# systemctl set-default graphical.target To select a different target at boot time Boot the system Interrupt the boot loader Move cursor to kernel entry Press e to edit Move the cursos to the line that starts with linux append the option for instance systemd.unit=emergency.target Ctrl+x to boot with changes 3.10.2 Resetting the Root Password Option 1 Boot the system using a Live CD Mount the root file system Edit /etc/shadow Option 2 Reboot the system Interrupt the boot loader Move the cursor to the kernel Press e to edit Move the cursor to kernel command line, starting with linux Append rd.break Press Ctrl+x to boot System will presents a root shell Change the root file system to read/write switch_root:/# mount -o remount,rw /sysroot Switch into a chroot switch_root:/# chroot /sysroot Set the new password passwd root run sh-4.4# touch /.autorelabel to include /etc/shadow Exit;Exit to system continue the boot Option 3 Reboot the system Interrupt the boot loader Move the cursor to the kernel Press e to edit Move the cursor to kernel command line, starting with linux Enable the debug using adding systemd.debug-shell option Press Ctrl+x to boot On login page Crtl+Alt+F9 and we will be on root shell Change the password Remove the debug sudo systemctl stop debug-shell.service 3.10.3 Repairing File system Issues at Boot Errors in /etc/fstab and corrupt file systems can stop a system from booting. Common File system issues: Corrupt file system Nonexistent device or UUID referenced in /etc/fstab Nonexistent mount point in / etc/fstab Incorrect mount option specified in /etc/fstab 3.10.4 Create or alter grub The default path is /etc/default/grub grup2-mkconfig -o /boot/grub2/grub.cfg reboot 3.11 Managing Network Security 3.11.1 Managing Server Firewall netfilter allows other kernel modules to interface with kernel’s network stack. Any incoming, outgoing, or forwarded network packet can be inspected, modified, dropped, or routed programmatically before reaching user space components or applications. nftables : a new filter and packet classification subsystem that has enhanced portions of netfilter’s code. Nftables uses the single nft user-space utility, allowing all protocol management to occur through a single interface, eliminating historical contention caused by diverse front ends and multiple netfilter interfaces. firewalld : a dynamic firewall manager. With firewalld, firewall management is simplified by classifying all network traffic into zones. Each zone has its own list of ports and services that are either open or closed. Pre-defined Zones all zones permit any incoming traffic which is part of a communication initiated by the system, and all outgoing traffic trusted home internal work public external dmz block drop Pre-defined Services These service definitions help you identify particular network services to configure. ssh dhcpv6-client ipp-client samba-client mdns To list services: firewall-cmd --get-services Configure the Firewall from the Command Line firwewall-cmd interacts with firewalld dynamic firewall manager, most of commands will work on the runtime config, unless the –permanent option is specified, we also must activate the setting using firewall-cmd –reload Some commanda: –get-default-zone : query the current default zone –set-default-zone=ZONE –get-zones –get-active-zones –add-source=CIDR [–zone=ZONE] –remove-source=CIDR [–zone=ZONE] –add-interface=INTERFACE [–zone=ZONE] —change-interface=INTERFACE [–zone=ZONE] –list-all [–zone=ZONE] –list-all-zones –reload Sample of commands setting the default zone to dmz, assign all traffic coming from the 192.168.0.0/24 network to the internal zone, and open the network ports from the mysql service on the internal zone [root@host ~]# firewall-cmd --set-default-zone=dmz [root@host ~]# firewall-cmd --permanent --zone=internal --add-source=192.168.0.0/24 [root@host ~]# firewall-cmd --permanent --zone=internal --add-service=mysql [root@host ~]# firewall-cmd --reload 3.11.2 Controlling SELinux Port Labeling One of the methods that SELinux uses for controlling network traffic is labeling network ports Get list of all current port label assignements semanage port -l To add a port to an existing port label (type) semanage port -a -t port_label -p tcp|udp PORTNUMBER To allow a gopher service to listen on port 71/TCP semanage port -a -t gopher_port_t -p tcp 71 To remove the binding of port 71/TCP to gopher_port_t: semanage port -d -t gopher_port_t -p tcp 71 To view local changes to the default policy semanage port -l -C To modify port 71/TCP from gopher_port_t to http_port_t semanage port -m -t http_port_t -p tcp 71 Service specific SELinux man pages found in the selinux-policy-doc package include documentation on SELinux types, booleans, and port types. If these man pages are not yet installed on your system, follow this procedure: [root@host ~]# yum -y install selinux-policy-doc [root@host ~]# man -k _selinux 3.12 Installing Red Hat Enterprise Linux 3.12.1 Installing Red Hat Enterprise Linux Supported processor architectures: x86 64-bit (AMD and Intel), IBM Power Systems (Little Endian), IBM Z, and ARM 64-bit. After downloading, create bootable installation media based on the instructions 3.12.2 Automating Installation with Kickstart Text file with specification of how the machine should be configured Using Kickstart, you specify everything Anaconda needs to complete an installation, including disk partitioning, network interface configuration, package selection, and other parameters, in a Kickstart text file. The %packages section specifies the software to be installed on the target system There are two additional sections, %pre and %post, which contain shell scripting commands that further configure the system. You must specify the primary Kickstart commands before the %pre, %post, and %packages sections, but otherwise, you can place these sections in any order in the file Sample: #version=RHEL8 ignoredisk --only-use=vda # System bootloader configuration bootloader --append=&quot;console=ttyS0 console=ttyS0,115200n8 no_timer_check net.ifnames=0 crashkernel=auto&quot; --location=mbr --timeout=1 --boot-drive=vda # Clear the Master Boot Record zerombr # Partition clearing information clearpart --all --initlabel # Use text mode install text repo --name=&quot;appstream&quot; --baseurl=http://classroom.example.com/content/rhel8.2/ x86_64/dvd/AppStream/ # Use network installation url --url=&quot;http://classroom.example.com/content/rhel8.2/x86_64/dvd/&quot; # Keyboard layouts # old format: keyboard us # new format: keyboard --vckeymap=us --xlayouts=&#39;&#39; # System language lang en_US.UTF-8 # Root password rootpw --plaintext redhat # System authorization information auth --enableshadow --passalgo=sha512 # SELinux configuration selinux --enforcing firstboot --disable # Do not configure the X Window System skipx # System services services --disabled=&quot;kdump,rhsmcertd&quot; --enabled=&quot;sshd,rngd,chronyd&quot; # System timezone timezone America/New_York --isUtc # Disk partitioning information part / --fstype=&quot;xfs&quot; --ondisk=vda --size=10000 %packages @core chrony cloud-init dracut-config-generic dracut-norescue firewalld grub2 kernel rsync tar -plymouth %end %post --erroronfail # For cloud images, &#39;eth0&#39; _is_ the predictable device name, since # we don&#39;t want to be tied to specific virtual (!) hardware rm -f /etc/udev/rules.d/70* ln -s /dev/null /etc/udev/rules.d/80-net-name-slot.rules # simple eth0 config, again not hard-coded to the build hardware cat &gt; /etc/sysconfig/network-scripts/ifcfg-eth0 &lt;&lt; EOF DEVICE=&quot;eth0&quot; BOOTPROTO=&quot;dhcp&quot; ONBOOT=&quot;yes&quot; TYPE=&quot;Ethernet&quot; USERCTL=&quot;yes&quot; PEERDNS=&quot;yes&quot; IPV6INIT=&quot;no&quot; EOF %end The Kickstart Generator website at presents dialog boxes for user inputs, and creates a Kickstart directives text file with the user’s choices. Each dialog box corresponds to the configurable items in the Anaconda installer. ksvalidator is a utility that checks for syntax errors in a Kickstart file. The pykickstart package provides ksvalidator. To find the provides of ksvalidation yum provides */ksvalidator We will see the pykickstart and install yum -y install pykickstart Checking for packages that have the kickstart in the name rpm -qad &#39;*kickstart&#39; ... ... /usr/share/doc/python3-kickstart/kickstart-docs.txt In this file kickstart-docs.txt we going to have all docs about kickstart To boot anaconda and point it to the kickstart file, press TAB and add inst.ks=LOCATION 3.12.3 Installing and Configuring Virtual Machine Red Hat Enterprise Linux 8 supports KVM (Kernel-based Virtual Machine), a full virtualization solution built into the standard Linux kernel. KVM can run multiple Windows and Linux guest operating systems virsh command is used to manage KVM Red Hat Virtualization (RHV) provides a centralized web interface that allows administrators to manage an entire virtual infrastructure Red Hat OpenStack Platform (RHOSP) provides the foundation to create, deploy, and scale a public or a private cloud Install the virtualizaation tools yum module list virt yum module install virt Verify the system requirements virt-host-validate To Manage virtual machines with cockpit Install the cockpit-machines package to add the Virtual Machines menu to Cockpit. yum install cockpit-machines Start and enable cockpit systemctl enable --now cockpit.socket 3.13 Running Containers 3.13.1 Intro Containers and Virtual Machines are different in the way they interact with hardware and the underlying operating system Virtualization: Enables multiple operating systems to run simultaneously on a single hardware platform. Uses a hypervisor to divide hardware into multiple virtual hardware systems, allowing multiple operating systems to run side by side Requires a complete operating system environment to support the application Containers: Run directly on the operating system, sharing hardware and OS resources across all containers on the system. This enables applications to stay lightweight and run swiftly in parallel. Share the same operating system kernel, isolate the containerized application processes from the rest of the system, and use any software compatible with that kernel Require far fewer hardware resources than virtual machines, which also makes them quick to start and stop and reduces storage requirements Running Containers from Container Images Container images are unchangeable, or immutable, files that include all the required code and dependencies to run a container Container images are built according to specifications, such as the Open Container Initiative (OCI) image format specification Managing Containers with Podman podman : manage containers and container image skopeo : used to inspect, copy, delete and sign images buildah: used to create new container images These tools are compatible with the Open Container Initiative (OCI). They can be used to manage any Linux containers created by OCI-compatible container engines, such as Docker 3.13.2 Running a Basic Conatiner The Red Hat Container Catalog provides a web-based interface that you can use to search these registries for certified content. The Container naming conventions registry_name/user_name/image_name:tag registry_name : name of the registry storing the image user_name : represents the user or organization to which the image belongs image_name : must be unique in the user namespace tag : image version Installing Container Managemnt Tools [root@host ~]# yum module install container-tools To pull or download the a container the image podman pull registry.access.redhat.com/ubi8/ubi:latest To retrieval, podman stores images podman images To run a container from image podman run -it registry.access.redhat.com/ubi8/ubi:latest To run and delete the image after use using podman run –rm [user@host ~]$ podman run --rm registry.access.redhat.com/ubi8/ubi cat /etc/os-release 3.13.3 Finding and Managing Container Images Podman uses a registreis.conf file : grep ^[^#] /etc/containers/registreis.conf podman info command displays configuration information for Podman podman info Search command to search conatiner registreis for a specific container image –no-trunc : option to see longer image descriptions podman search registry.redhat.io/rhel8 Inspect images skopeo inspect docker://registry.redhat.io/rhel8/python-36 podman inspect registry.redhat.io/rhel8/python-36 remove image podman rmi registry.redhat.io/rhel8/python-36:latest 3.13.4 Performing Advanced Container Management Mapping Container Hosts Ports to the container using -p [user@host ~]$ podman run -d -p 8000:8080 registry.redhat.io/rhel8/httpd-24 List the ports of cntainer [user@host ~]$ podman port -a Make sure that firewall on container allow external clients [root@host ~]# firewall-cmd --add-port=8000/tcp Check the logs of container podman logs &lt;container id&gt; Stop / restart podman [stop | restart] &lt;container id&gt; Delete container podman run &lt;container id&gt; Passing parameters with -e, -d to detach, -p to specify ports podman run -d --name container_name -e MYSQL_USER=user_name -e MYSQL_PASSWORD=user_password -e MYSQL_DATABASE=database_name -e MYSQL_ROOT_PASSWORD=mysql_root_password -p 3306:3306 registry.redhat.io/rhel8/mariadb-103:1-102 # checking mysql -h127.0.0.1 -udupsy -pbongle -P3306 show databases; 3.13.5 Attaching Persistent Storage to a Container Mounting Volume --volume host_dir:container_dir:Z For example, to use the /home/user/dbfiles host directory for MariaDB database files as /var/lib/mysql inside the container, use the following command. The following podman run command is very long and should be entered as a single line. podman run -d --name mydb -v /home/user/dbfiles:/var/lib/mysql:Z -e MYSQL_USER=user -e MYSQL_PASSWORD=redhat -e MYSQL_DATABASE=inventory registry.redhat.io/rhel8/mariadb-103:1-102 The dir must have the SELinux context container_file_t mke sure to use :Z 3.13.6 Managing Containers as Service First step is open a new session with user ssh user@localhost Enable linger loginctl enable-linger &lt;user&gt; # to check loginctl show-user user Create a dir to store the unit files mkdir -pv ~/.config/systemd/user/ Generate the service podman generate systemd --name &lt;name of service&gt; --files --new use --user to control new user services [user@host ~]$ systemctl --user daemon-reload [user@host ~]$ systemctl --user enable &lt;name of service&gt; [user@host ~]$ systemctl --user start &lt;name of service&gt; "],["server-administrator-iii-rh294.html", "# 4 Server Administrator III - RH294 4.1 Intro to Ansible 4.2 Deploying Anisble and Implementing Playbooks 4.3 Managing Variables and Facts 4.4 Implementing Task Control 4.5 Deploying Files to Managed Hosts 4.6 Managing Complex Plays and Playbooks 4.7 Simplifying Playbooks with Roles 4.8 Troubleshooting Ansible 4.9 Automating Linux Administration Tasks", " # 4 Server Administrator III - RH294 4.1 Intro to Ansible Ansible is an open source automation platform. It is a simple automation language that can perfectly describe an IT application infrastructure in Ansible Playbooks. It is also an automation engine that runs Ansible Playbooks. 4.1.1 Ansible Concepts and Architecture Control nodes : Where Ansible is installed and runs and has copies of Ansible project files, also can be an Administrator server, where Tower will run. Managed hosts : list of servers organized in inventory list Inventory : Static : List of servers Dynamic : Program that connect to provider and search for list of machines Playbook : List of tasks that going to be converted in python script to run in each host, those tasks are expressed in YAML format in a text file 4.1.2 Install Ansbile To install ansible yum install ansbile To check version ansbile --version RHEL8 can use the plataform python package yum list installed plataform-python To register on RedHat and Enable repository subscription-manager register ubscription-manager repos --enable ansible-2-for-rhel-8-x86_64-rpms To install python36 yum module install python36 To list the modules ansible-doc -l 4.1.3 Implementing an Ansible Playbook 4.2 Deploying Anisble and Implementing Playbooks 4.2.1 Building an Ansible Invetory Static inventory file is a text file that specifies the managed hosts that Ansilbe targets, it is located on /etc/ansible/hosts as default Sample of YAML file web1.example.com web2.example.com db1.example.com db2.example.com 192.0.2.42 We can also organize the inventory in groups using [], ,hosts can be in multiple groups [webservers] web1.example.com web2.example.com 192.0.2.42 [db-servers] db1.example.com db2.example.com We also can configure nested groups with :children sufix [usa] washington1.example.com washington2.example.com [canada] ontario01.example.com ontario02.example.com [north-america:children] canada usa The hosts can also be specified with Ranges [START:END] [usa] washington[1:2].example.com [canada] ontario[01:02].example.com To verify the inventory we can use the commands below # his command verify if machine is present in inventory ansible washington1.example.com --list-hosts # List all hosts in canada group ansible canada --list-hosts To list from an specific inventory file, -i makes ansible use your inventory file in the current working directory instead of the system /etc/ansible/hosts inventory file ansible all -i inventory --list-hosts List ungrouped hosts ansible ungrouped -i inventory --list-hosts List hosts from specific group called us ansible us -i inventory --list-hosts List inventory as a graph ansible-invetory --graph -i /etc/anisble/hosts 4.2.2 Managing Ansible Configuration Files The ansible configuration file is located at /etc/ansible/ansible.cfg as default, but ansible looks at ~/.ansible.cfg that overight the default, however if the ./ansible.cfg exists in the directory in which the ansible command is executed, it is used instead of the global file or the user personal file. We can also configure the environment variable ANSIBLE_CONFIG to set the ansible.cfg , in this case all commands going to point to this config file. To list the config file ansible --version ansible servers --list-hosts -v To list the ansible config anisble config Sample of config file [defaults] inventory = ./inventory remote_user = user ask_pass = fals [privilege_escalation] become = true become_method = sudo become_user = root become_ask_pass = false inventory : Specifies the path to the inventory file. remote_user : The name of the user to log in as on the managed hosts. If not specified, the current user’s name is used. ask_pass : Whether or not to prompt for an SSH password. Can be false if using SSH public key authentication. become : Whether to automatically switch user on the managed host (typically to root) after connecting. This can also be specified by a play. become_method : How to switch user (typically sudo, which is the default,but su is an option). become_user : The user to switch to on the managed host (typically root, which is the default). become_ask_pass : Whether to prompt for a password for your become_method. Defaults to false. To list all the config options we can read the /etc/ansible/ansible.cfg or run the command below to dump ansible-config dump Sample of ansible playbook to deploy a public key - name: Public key is deployed to managed hosts for Ansible hosts: all tasks: - name: Ensure key is in root&#39;s ~/.ssh/authorized_hosts authorized_key: user: root state: present key: &#39;{{ item }}&#39; with_file: - ~/.ssh/id_rsa.pub 4.2.3 Running Ad Hoc Commands Sample date command to a host ansible servera.lab.example.com -m command -a date List the modules ansile-doc -l Check documentation for a module ansible-doc &lt;module_name&gt; Using the module user to create and remove a user # create ansible &lt;server&gt; -m user -a name=&lt;name_of_user&gt; # remove ansible &lt;server&gt; -m user -a &quot;name=&lt;name_of_user&gt; state=absent&quot; Specfing user and become to copy as root ansible all -m copy -a &#39;content=&quot;Managed by Ansible\\n&quot; dest=/etc/motd&#39; -u devops --become 4.2.4 Writing and Running Playbooks Using command [student@workstation ~]$ ansible -m user -a &quot;name=newbie uid=4000 state=present&quot; servera.lab.example.com Using playbook --- - name: Configure important user consistently hosts: servera.lab.example.com tasks: - name: newbie exists with UID 4000 user: name: newbie uid: 4000 state: present Tip for configure vim as editor vim ~/.vimrc # add autocmd FileType yaml setlocal ai ts=2 sw=2 et nu cuc autocmd FileType yaml colo desert 4.2.5 Running Playbooks Simple Command ansible-playbook site.yml We can increase the verbosity of output using -v, -vv , -vvv or -vvvv and also **check the syntax* like: ansible-playbook --syntax-check xxx.yml Another option is execute as a Dry Run using option -C ansible-playbook -C xxx.yml 4.2.6 Implementing Muliple Plays A playbook is a YAML file containing a list of one or more plays, if a playbook contains multiple plays, each play may apply its tasks to a separate set of hosts. Sample --- # This is a simple playbook with two plays - name: first play hosts: web.example.com tasks: - name: first task yum: name: httpd status: present - name: second task service: name: httpd enabled: true - name: second play hosts: database.example.com tasks: - name: first task service: name: mariadb enabled: true Privilege Escalation Those configuration can be set on ansible.cfg configuration file or at task level become : True or False to enable or disable escalation become_method : sudo/pbrun method of escalation become_user : privilege user remote_user : User that runs the tasks 4.2.7 Finding Modules for Task The command ansible-doc -l will list all the modules on the current version ansible-doc -l To list detail about documentation, also to access the examples of playbooks go to ansible-doc moudule and run the /EXAMPLES ansible-doc &lt;module&gt; # check examples /EXAMPLES 4.3 Managing Variables and Facts We can set a variable that affects a group of hosts or only individual hosts. Some variables are facts that can be set by Ansible based on the configuration of a system. Other variables can be set inside the playbook, and affect one play in that playbook, or only one task in that play. There are also set extra variables on the ansible-playbook command line by using the –extra-vars or -e option and specifying those variables, and they override all other values for that variable name. Simple list of ways to define a variable, ordered from lowest precedence to highest : Group variable in inventory Group variable in files in a group_vars sub dir in the same dir as inventory or playbook Host variable in the inventory Host variables in files in a host_var sub dir in the same dir as the inventory or playbook Host facts, discoverd at runtime Play variables in the playbook(vras and var_files) Task variables Extra variables on the command line A variable that is set to affect the all host group will be overridden by a variable that has the same name and is set to affect a single host. 4.3.1 Variables in playbook - hosts: all vars: user: joe home: /home/joe Using external files in the vars_files directive may be used - hosts: all vars_files: - vars/users.yml Using variables {{ var_name }}, using quotes is mandatory if the variable is the first element to start a value vars: user: joe tasks: # This line will read: Creates the user joe - name: Creates the user {{ user }} user: # This line will create the user named Joe name: &quot;{{ user }}&quot; 4.3.2 Host and group variables Defining the ansible_user host variable for demo.example.com: [servers] demo.example.com ansible_user=joe Defining the user group variable for the servers host group. [servers] demo1.example.com demo2.example.com [servers:vars] user=joe 4.3.3 Using directories to populate host and group variables The recommended practice is to define inventory variables using host_vars and group_vars directories, and not to define them directly in the inventory files [admin@station project]$ cat ~/project/inventory [datacenter1] demo1.example.com demo2.example.com [datacenter2] demo3.example.com demo4.example.com [datacenters:children] datacenter1 datacenter2 Variable for the databaceters group [admin@station project]$ cat ~/project/group_vars/datacenters package: httpd Variable for each datacenetr [admin@station project]$ cat ~/project/group_vars/datacenter1 package: httpd [admin@station project]$ cat ~/project/group_vars/datacenter2 package: apache Variable for each host [admin@station project]$ cat ~/project/host_vars/demo1.example.com package: httpd [admin@station project]$ cat ~/project/host_vars/demo2.example.com package: apache [admin@station project]$ cat ~/project/host_vars/demo3.example.com package: mariadb-server [admin@station project]$ cat ~/project/host_vars/demo4.example.com package: mysql-server 4.3.4 Overrding variable from command line [user@demo ~]$ ansible-playbook main.yml -e &quot;package=apache&quot; 4.3.5 Secrets Ansible Vault can be used to encrypt and decrypt any structured data file used by Ansible [student@demo ~]$ ansible-vault create secret.yml New Vault password: redhat Confirm New Vault password: redhat We can use view to view the content, encrypt and decrypt option . To run a playbook with vault [student@demo ~]$ ansible-playbook --vault-id @prompt site.yml Vault password (default): redhat # or [student@demo ~]$ ansible-playbook --vault-password-file=vault-pw-file site.yml 4.3.6 Managing Facts Ansible facts are variables that are automatically discovered by Ansible on a managed host, every play runs the setup module automatically before teh first task to gather facts, this is report on Gathering Facts task, for example ? hostname kernel version network interface IP OS info, CPUs, disk, memory, etc To turn off the facts we can set the option gather_facts: no and the facts will not be collected. To create custom facts we need to speficy on /etc/ansible/facts.d/.fact the name need to end with .fact below one example [packages] web_package = httpd db_package = mariadb-server [users] user1 = joe user2 = jane 4.3.7 Magic Variables Those variables are not facts or configured on setup but are also automatically set by Ansible hostvars : Contains the variables for managed hosts group_names : Lists all groups the current managed host is in. groups : Lists all groups and hosts in the inventory. inventory_hostname : Contains the host name for the current managed host as configured in the inventory. 4.4 Implementing Task Control 4.4.1 Loops Simple loop: - name: Postfix and Dovecot are running service: name: &quot;{{ item }}&quot; state: started loop: - postfix - dovecot The list used by loop can be provided by a variable : vars: mail_services: - postfix - dovecot tasks: - name: Postfix and Dovecot are running service: name: &quot;{{ item }}&quot; state: started loop: &quot;{{ mail_services }}&quot; Loop over hash or Dict - name: Users exist and are in the correct groups user: name: &quot;{{ item.name }}&quot; state: present groups: &quot;{{ item.groups }}&quot; loop: - name: jane groups: wheel - name: joe groups: root Loop Keywords : with_items : The loop variable item holds the list item used during each iteration. vars: data: - user0 - user1 - user2 tasks: - name: &quot;with_items&quot; debug: msg: &quot;{{ item }}&quot; with_items: &quot;{{ data }}&quot; with_file : The loop variable item holds the content of a corresponding file from the file list during each iteration. with_sequence : The loop variable item holds the value of one of the generated items in the generated sequence during each iteration Using Register variable The register keyword can also capture the output of a task that loops --- - name: Loop Register Test gather_facts: no hosts: localhost tasks: - name: Looping Echo Task shell: &quot;echo This is my item: {{ item }}&quot; loop: - one - two register: echo_results - name: Show echo_results variable debug: var: echo_results 4.4.2 Task Conditionally Ansible can use conditionals to execute tasks or plays when certain conditions are met. Boolean condiction using when --- - name: Simple Boolean Task Demo hosts: all vars: run_my_task: true tasks: - name: httpd package is installed yum: name: httpd when: run_my_task Condition to test if variable has a value --- - name: Test Variable is Defined Demo hosts: all vars: my_service: httpd tasks: - name: &quot;{{ my_service }} package is installed&quot; yum: name: &quot;{{ my_service }}&quot; when: my_service is defined Using data from Gathering Facts --- - name: Demonstrate the &quot;in&quot; keyword hosts: all gather_facts: yes vars: supported_distros: - RedHat - Fedora tasks: - name: Install httpd using yum, where supported yum: name: http state: present when: ansible_distribution in supported_distros Testing multiple conditions when: ansible_distribution == &quot;RedHat&quot; or ansible_distribution == &quot;Fedora&quot; # or when: ansible_distribution_version == &quot;7.5&quot; and ansible_kernel == &quot;3.10.0-327.el7.x86_64&quot; # or when: - ansible_distribution_version == &quot;7.5&quot; - ansible_kernel == &quot;3.10.0-327.el7.x86_64&quot; # or when: &gt; ( ansible_distribution == &quot;RedHat&quot; and ansible_distribution_major_version == &quot;7&quot; ) or ( ansible_distribution == &quot;Fedora&quot; and ansible_distribution_major_version == &quot;28&quot; ) Loop and conditions when is checking each item Sample 1: - name: install mariadb-server if enough space on root yum: name: mariadb-server state: latest loop: &quot;{{ ansible_mounts }}&quot; when: item.mount == &quot;/&quot; and item.size_available &gt; 300000000 Sample 2: --- - name: Restart HTTPD if Postfix is Running hosts: all tasks: - name: Get Postfix server status command: /usr/bin/systemctl is-active postfix ignore_errors: yes register: result - name: Restart Apache HTTPD based on Postfix status service: name: httpd state: restarted when: result.rc == 0 register: result : save info on the result variable when: result.rc == 0 : check the output of postfix task and restart httpd if systemctl command is 0 4.4.3 Handlers Handlers are tasks that respond to a notification triggered by other tasks. Tasks only notify their handlers when the task changes something on a managed host. Handlers can be considered as inactive tasks that only get triggered when explicitly invoked using a notify statement. The Apache server is only restarted by the restart apache handler when a configuration file is updated and notifies it. tasks: - name: copy demo.example.conf configuration template template: src: /var/lib/templates/demo.example.conf.template dest: /etc/httpd/conf.d/demo.example.conf notify: - restart apache handlers: - name: restart apache service: name: httpd state: restarted Managing Task Errors in Play Ignoring task failure - name: Latest version of notapkg is installed yum: name: notapkg state: latest ignore_errors: yes Forcing execution of handler after task failure using force_handlers --- - hosts: all force_handlers: yes tasks: - name: a task which always notifies its handler command: /bin/true notify: restart the database - name: a task which fails because the package doesn&#39;t exist yum: name: notapkg state: latest handlers: - name: restart the database service: name: mariadb state: restarted Specify task failure tasks: - name: Run user creation script shell: /usr/local/bin/create_users.sh register: command_result failed_when: &quot;&#39;Password missing&#39; in command_result.stdout&quot; Using fail module to force a task failure tasks: - name: Run user creation script shell: /usr/local/bin/create_users.sh register: command_result ignore_errors: yes - name: Report script failure fail: msg: &quot;The password is missing in the output&quot; when: &quot;&#39;Password missing&#39; in command_result.stdout&quot; Specifying when a task reports “changed” results tasks: - shell: cmd: /usr/local/bin/upgrade-database register: command_result changed_when: &quot;&#39;Success&#39; in command_result.stdout&quot; notify: - restart_database handlers: - name: restart_database service: name: mariadb state: restarted 4.4.4 Blocks blocks are clauses that logically group tasks, and can be used to control how tasks are executed - name: block example hosts: all tasks: - name: installing and configuring Yum versionlock plugin block: - name: package needed by yum yum: name: yum-plugin-versionlock state: present - name: lock version of tzdata lineinfile: dest: /etc/yum/pluginconf.d/versionlock.list line: tzdata-2016j-1 state: present when: ansible_distribution == &quot;RedHat&quot; Using block , rescue and always block: Defines the main tasks to run rescue: Defines the tasks to run if the tasks defined in the block clause fail. always: Defines the tasks that will always run independently of the success or failure of tasks defined in the block and rescue clauses. tasks: - name: Upgrade DB block: - name: upgrade the database shell: cmd: /usr/local/lib/upgrade-database rescue: - name: revert the database upgrade shell: cmd: /usr/local/lib/revert-database always: - name: always restart the database service: name: mariadb state: restarted 4.5 Deploying Files to Managed Hosts 4.5.1 Modifying and Copying Files to hosts Files Modules blockinfile : Insert, update, or remove a block of multiline text surrounded by customizable marker lines copy : Copy a file from the local or remote machine to a location on a managed host fetch : fetching files from remote machines to the control node and storing them in a file tree, organized by host name. file : Create/Remove/Set attributes for (perm, ownership, SELinux) of files, links, dirs lineinfile : when you want to change a single line in a file. stat : Retrieve status information for a file, similar to the Linux stat command synchronize : A wrapper around the rsync Examples Ensure file exists - name: Touch a file and set permissions file: path: /path/to/file owner: user1 group: group1 mode: 0640 state: touch Modify attributes - name: SELinux type is set to samba_share_t file: path: /path/to/samba_file setype: samba_share_t Make SELinux file context persistent - name: SELinux type is persistently set to samba_share_t sefcontext: target: /path/to/samba_file setype: samba_share_t state: present Copy and Edit files on managed hosts - name: Copy a file to managed hosts copy: src: file dest: /path/to/file To retrieve files from managed hosts use the fetch module - name: Retrieve SSH key from reference host fetch: src: &quot;/home/{{ user }}/.ssh/id_rsa.pub dest: &quot;files/keys/{{ user }}.pub&quot; To ensure a specific single line of text exists in an existing file, using lineinfile - name: Add a line of text to a file lineinfile: path: /path/to/file line: &#39;Add this line to the file&#39; state: present To add a block of text to an existing file, using blockinfile - name: Add additional lines to a file blockinfile: path: /path/to/file block: | First line in the additional block of text Second line in the additional block of text state: present Remove files from managed hosts - name: Make sure a file does not exist on managed hosts file: dest: /path/to/file state: absent Retrieving the status of a file on managed hosts - name: Verify the checksum of a file stat: path: /path/to/file checksum_algorithm: md5 register: result - debug msg: &quot;The checksum of the file is {{ result.stat.checksum }}&quot; Using stat - name: Examine all stat output of /etc/passwd hosts: localhost tasks: - name: stat /etc/passwd stat: path: /etc/passwd register: results - name: Display stat results debug: var: results Sync files between control node and managed node - name: synchronize local file to remote files synchronize: src: file dest: /path/to/file 4.5.2 Deploying Custom files with Jinja2 templates Jinja2 templates are a powerful tool to customize configuration files to be deployed on the managed hosts. When the Jinja2 template for a configuration file has been created, it can be deployed to the managed hosts using the template module. To use template module tasks: - name: template render template: src: /tmp/j2-template.j2 dest: /tmp/dest-config-file.txt Loops in Jinja2 {% for user in users %} {{ user }} {% endfor %} Generate /etc/hosts files from host facts - name: /etc/hosts is up to date hosts: all gather_facts: yes tasks: - name: Deploy /etc/hosts template: src: templates/hosts.j2 dest: /etc/hosts The below code templates/hosts.j2 template construct the file from all hosts in group all {% for host in groups[&#39;all&#39;] %} {{ hostvars[host][&#39;ansible_facts&#39;][&#39;default_ipv4&#39;][&#39;address&#39;] }} {{ hostvars[host] [&#39;ansible_facts&#39;][&#39;fqdn&#39;] }} {{ hostvars[host][&#39;ansible_facts&#39;][&#39;hostname&#39;] }} {% endfor %} Using conditionals {% if finished %} {{ result }} {% endif %} 4.6 Managing Complex Plays and Playbooks 4.6.1 Selecting Hosts with Host Patterns Host patterns are used to specify the hosts to target by a play or ad hoc command. We can use groups of hosts by [&lt;group_name&gt;] or group of groups [&lt;name&gt;:children] and specify the groups. We can use the variable hosts inside the playbook : we can affect all hosts add all or '*', Use the ungrouped part of names '*.exammple.com', filtering ip 192.168.2.* By part of groups 'datacenter*' or 'data*' Lists servera, serverb,192.168.2.2 Mixed 192.168.2*, lab, data* Using AND logical operator lab, $datacenter1 Using NOT logical operator lab , !datacenter1 4.6.2 Including and Importing Files There are two options to bring contenr into a playbook, using include or import Include : it is a dynamic operation. Ansible processes included content during the run of the playbook, as content is reached. Import : it is a static operation. Ansible preprocesses imported content when the playbook is initially parsed, before the run starts. Cannot use loops - name: Prepare the web server import_playbook: web.yml - name: Prepare the database server import_playbook: db.yml We can create a task file and import that task file task file [admin@node ~]$ cat webserver_tasks.yml - name: Installs the httpd package yum: name: httpd state: latest - name: Starts the httpd service service: name: httpd state: started Importing a task file --- - name: Install web server hosts: webservers tasks: - import_tasks: webserver_tasks.yml 4.7 Simplifying Playbooks with Roles Ansible roles have the following benefits: Roles group content, allowing easy sharing of code with others Roles can be written that define the essential elements of a system type: web server, database server, Git repository, or other purpose Roles make larger projects more manageable Roles can be developed in parallel by different administrators 4.7.1 Describing Role Structure Subdirectories defaults : DEfault value of role variables, low precedence files : static files that are referenced by role tasks handlers : role’s handler definitions meta : info about role, author, license, platforms, etc tasks : role’s task definitions templates : Jinja2 templates that are referenced by role tasks tests : can contain an inventory and test.yml playbook for test vars : define roleś variables, high precedence Using Roles --- - hosts: remote.example.com roles: - role1 - role2 Controlling Order of Execution The following play shows an example with pre_tasks, roles, tasks, post_tasks and handlers. It is unusual that a play would contain all of these sections - name: Play to illustrate order of execution hosts: remote.example.com pre_tasks: - debug: msg: &#39;pre-task&#39; notify: my handler roles: - role1 tasks: - debug: msg: &#39;first task&#39; notify: my handler post_tasks: - debug: msg: &#39;post-task&#39; notify: my handler handlers: - name: my handler debug: msg: Running my handler Using include_role - name: Execute a role as a task hosts: remote.example.com tasks: - name: A normal task debug: msg: &#39;first task&#39; - name: A task to include role2 here include_role: role2 4.7.2 Reusing Content with System Roles RHEL System Roles rhel-system-roles.kdump : configure kdump rhel-system-roles.network : configure network interfaces rhel-system-roles.selinux : Configure and manage SELinux rhel-system-roles.timesync : Confgure time sync using NTP or PTP rhel-system-roles.postfix : Configure each host as a mail transfer agent using postfix rhel-system-roles.firewall : configure firewall rhel-system-roles.tuned : configure tuned service Install RHEL Roles, after install the role are located at /usr/share/ansible/roles/ [root@host ~]# yum install rhel-system-roles Time Sync Role Example - name: Time Synchronization Play hosts: servers vars: timesync_ntp_servers: - hostname: 0.rhel.pool.ntp.org iburst: yes - hostname: 1.rhel.pool.ntp.org iburst: yes - hostname: 2.rhel.pool.ntp.org iburst: yes timezone: UTC roles: - rhel-system-roles.timesync tasks: - name: Set timezone timezone: name: &quot;{{ timezone }}&quot; 4.7.3 Creating Roles Process Create the role directory structure. Define the role content. Use the role in a playbook. Directory Structure By default, Ansible looks for roles in a subdirectory called roles in the directory containing your Ansible Playbook, if cannot find the role, it looks at roles_path ~/.ansible/roles:/usr/share/ansible/roles:/etc/ansible/roles The ansible-galaxy command line tool s used to manage Ansible roles, including the creation of new roles. cd roles ansible-galaxy init my_new_role Recommended Practices for Role Content Development Maintain each role in its own version control repository. Ansible works well with git-based repositories Sensitive information, such as passwords or SSH keys, should not be stored in the role repository Use ansible-galaxy init to start your role Create and maintain README.md and meta/main.yml files to document what your role is for, who wrote it, and how to use it Keep your role focused on a specific purpose or function Reuse and refactor roles often. Resist creating new roles for edge configurations Role Dependencies Dependencies are defined in the meta/main.yml file in the role directory hierarchy. --- dependencies: - role: apache port: 8080 - role: postgres dbname: serverlist admin_user: felix Using Role in playbook To access a role, reference it in the roles: section of a play. [user@host ~]$ cat use-motd-role.yml --- - name: use motd role playbook hosts: remote.example.com remote_user: devops become: true roles: - motd When the playbook is executed, tasks performed because of a role can be identified by the role name prefix [user@host ~]$ ansible-playbook -i inventory use-motd-role.yml PLAY [use motd role playbook] ************************************************** TASK [setup] ******************************************************************* ok: [remote.example.com] TASK [motd: deliver motd file] ************************************************ changed: [remote.example.com] PLAY RECAP ********************************************************************* remote.example.com : ok=2 changed=1 unreachable=0 failed=0 4.7.4 Deploying Roles with Ansible Galaxy Ansible Galaxy https://galaxy.ansible.com is a public library of Ansible content written by a variety of Ansible administrators and users. Using Ansible Galaxy Command-Line tool Search [user@host ~]$ ansible-galaxy search &#39;redis&#39; --platforms EL Get info [user@host ~]$ ansible-galaxy info geerlingguy.redis Install from Ansible Galaxy [user@host project]$ ansible-galaxy install geerlingguy.redis -p roles/ Install using requirements.yml [user@host project]$ ansible-galaxy install -r roles/requirements.yml -p roles List the roles locally [user@host project]$ ansible-galaxy list Remove role [user@host ~]$ ansible-galaxy remove nginx-acme-ssh 4.7.5 Roles and Modules from Content Collections Ansible content collections are a distribution format for Ansible content. A collection provides a set of related modules, roles, and plug-ins that you can download to your control node and then use in your playbooks. install Contect collections [user@controlnode ~]$ ansible-galaxy collection install community.crypto Install from collection path [root@controlnode ~]# ansible-galaxy collection install -p /usr/share/ansible/collections community.postgresql Install from tar file [user@controlnode ~]$ ansible-galaxy collection install /tmp/community-dns-1.2.0.tar.gz [user@controlnode ~]$ ansible-galaxy collection install http://www.example.com/redhat-insights-1.0.5.tar.gz Install from requirements.yml file --- collections: - name: community.crypto - name: ansible.posix version: 1.2.0 - name: /tmp/community-dns-1.2.0.tar.gz - name: http://www.example.com/redhat-insights-1.0.5.tar.gz [root@controlnode ~]# ansible-galaxy collection install -r requirements.yml 4.8 Troubleshooting Ansible 4.8.1 Troubleshooting Playbooks By default, Ansible is not configured to log its output to any log file. We can configure the log_path variale in ansible.cfg or using $ANSIBLE_LOG_PATH Relate debug we can increase the verbosity of output using -vvvv or inside the playbook using verbosity parameter name: Display the &quot;output&quot; variable debug: var: output verbosity: 2 Managing Errors We can use --syntax-check to verify the YAML syntax We can also use the --step to step through a playbook one task at a time [student@demo ~]$ ansible-playbook play.yml --step We also can star the playbook in a specific task using --start-at-task [student@demo ~]$ ansible-playbook play.yml --start-at-task=&quot;start httpd service&quot; Recommended Practices for Playbook Management Use a concise description of the play’s or task’s purpose to name plays and tasks Include comments to add additional inline documentation about tasks. Organize task attributes vertically to make them easier to read. Consistent horizontal indentation is critical. Use spaces, not tabs, to avoid indentation errors. Try to keep the playbook as simple as possible. ### Troubleshooting Managed Hosts We can use the ansible-playbook --check command to run smoke tests on a playbook. This option executes the playbook without making changes to the managed hosts’ configuration. [student@demo ~]$ ansible-playbook --check playbook.yml Another alternative is use --check and --diff, this option reports the changes made to the template files on managed hosts, those changes are displyed in the command but not actually made [student@demo ~]$ ansible-playbook --check --diff playbook.yml 4.9 Automating Linux Administration Tasks 4.9.1 Managing Software and Subscriptions We can use the yum module to manage package on manage hosts Install : --- - name: Install the required packages on the web server hosts: servera.lab.example.com tasks: - name: Install the httpd packages yum: name: httpd state: present Update all packages, on this case we need to use whild card * - name: Update all packages yum: name: &#39;*&#39; state: latest Install group, _must prefix group names with (_?) - name: Install Development Tools yum: name: &#39;@Development Tools&#39; state: presen Install module, _must use the module name with prefix (_?) - name: Inst perl AppStream module yum: name: &#39;@perl:5.26/minimal&#39; state: present Optimizing Multiple Package Installation, using list is more efficient then loop --- - name: Install the required packages on the web server hosts: servera.lab.example.com tasks: - name: Install the packages yum: name: - httpd - mod_ssl - httpd-tools state: present Gathering Facts about Installed Packages The package_facts Ansible module collects the installed package details on managed hosts --- - name: Display installed packages hosts: servera.lab.example.com tasks: - name: Gather info on installed packages package_facts: manager: auto - name: List installed packages debug: var: ansible_facts.packages - name: Display NetworkManager version debug: msg: &quot;Version {{ansible_facts.packages[&#39;NetworkManager&#39;][0].version}}&quot; when: &quot;&#39;NetworkManager&#39; in ansible_facts.packages&quot; To register a Subscription - name: Register and subscribe the system redhat_subscription: username: yourusername password: yourpassword pool_ids: poolID state: present To enabling Red Hat Software Repositories - name: Enable Red Hat repositories rhsm_repository: name: - rhel-8-for-x86_64-baseos-rpms - rhel-8-for-x86_64-baseos-debug-rpms state: present 4.9.2 Managing Users and Authenticatin User Module Add New user - name: Add new user to the development machine and assign the appropriate groups. user: name: devops_user shell: /bin/bash groups: sys_admins, developers append: yes Add new user generating an ssh key name: Create a SSH key for user1 user: name: user1 generate_ssh_key: yes ssh_key_bits: 2048 ssh_key_file: .ssh/id_my_rsa Group Module Verify a group module - name: Verify that auditors group exists group: name: auditors state: present Known Hosts Module Copy host key - name: copy host keys to remote servers known_hosts: path: /etc/ssh/ssh_known_hosts name: host1 key: &quot;{{ lookup(&#39;file&#39;, &#39;pubkeys/host1&#39;) }}&quot; Authorized Key Module Set authorized key - name: Set authorized key authorized_key: user: user1 state: present key: &quot;{{ lookup(&#39;file&#39;, &#39;/home/user1/.ssh/id_rsa.pub&#39;) }} 4.9.3 Managing the Boot Process and Scheduled Processes We can schedule jobs with at or cron - name: remove tempuser. at: command: userdel -r tempuser count: 20 units: minutes unique: yes - cron: name: &quot;Flush Bolt&quot; user: &quot;root&quot; minute: 45 hour: 11 job: &quot;php ./app/nut cache:clear&quot; Manage service with systemd and service - name: start nginx service: name: nginx state: started&quot; - name: reload web server systemd: name: apache2 state: reload daemon-reload: yes Reboot - name: &quot;Reboot after patching&quot; reboot: reboot_timeout: 180 - name: force a quick reboot reboot: Use Shell and Command - name: Run a templated variable (always use quote filter to avoid injection) shell: cat {{ myfile|quote }} - name: This command only command: /usr/bin/scrape_logs.py arg1 arg2 args: chdir: scripts/ creates: /path/to/script 4.9.4 Managing Storage Configure using parted name: New 10GB partition parted: device: /dev/vdb number: 1 state: present part_end: 10GB Using lvg and lvol modules Create volume group - name: Creates a volume group lvg: vg: vg1 pvs: /dev/vda1 pesize: 32 Resize volume group - name: Resize a volume group lvg: vg: vg1 pvs: /dev/vdb1,/dev/vdc1 Create logical volume name: Create a logical volume of 2GB lvol: vg: vg1 lv: lv1 size: 2g Create XFS filesytem - name: Create an XFS filesystem filesystem: fstype: xfs dev: /dev/vdb1 Mount Mount device with ID - name: Mount device with ID mount: path: /data src: UUID=a8063676-44dd-409a-b584-68be2c9f5570 fstype: xfs state: present Mount NFS - name: Mount NFS share mount: path: /nfsshare src: 172.25.250.100:/share fstype: nfs opts: defaults dump: &#39;0&#39; passno: &#39;0&#39; state: mounted Configure Swap with Modules - name: Create new swap VG lvg: vg: vgswap pvs: /dev/vda1 state: present - name: Create new swap LV lvol: vg: vgswap lv: lvswap size: 10g - name: Format swap LV command: mkswap /dev/vgswap/lvswap when: ansible_swaptotal_mb &lt; 128 - name: Activate swap LV command: swapon /dev/vgswap/lvswap when: ansible_swaptotal_mb &lt; 128 Ansible Facts for Storage Config We can use ansible setup to retrieve all the facts for manage hosts [user@controlnode ~]$ ansible webservers -m setup or [user@controlnode ~]$ ansible webservers -m setup -a &#39;filter=ansible_devices&#39; or [user@controlnode ~]$ ansible webservers -m setup -a &#39;filter=ansible_device_links&#39; or [user@controlnode ~]$ ansible webservers -m setup -a &#39;filter=ansible_mounts&#39; 4.9.5 Managing Network Configuration Configuring Network with Network System Role list the currently installed system roles with the ansible-galaxy list command. [user@controlnode ~]$ ansible-galaxy list Roles are located in the /usr/share/ansible/roles directory. The network role is configured with two variables, network_provider and network_connections. The network_provider variable configures the back end provider, either nm (NetworkManager) or initscripts. On Red Hat Enterprise Linux 8, the network role uses the nm (NetworkManager) as a default networking provider --- network_provider: nm network_connections: - name: ens4 type: ethernet ip: address: - 172.25.250.30/24 The network_connections variable configures the different connections, specified as a list of dictionaries, using the interface name as the connection name. network_connections: - name: eth0 persistent_state: present type: ethernet autoconnect: yes mac: 00:00:5e:00:53:5d ip: address: - 172.25.250.40/24 zone: external To use the network system role, you need to specify the role name under the roles clause in your playbook as follows: - name: NIC Configuration hosts: webservers vars: network_connections: - name: ens4 type: ethernet ip: address: - 172.25.250.30/24 roles: - rhel-system-roles.network Configure Network with modules The nmcli module supports the management of both network connections and devices - name: NIC configuration nmcli: conn_name: ens4-conn ifname: ens4 type: ethernet ip4: 172.25.250.30/24 gw4: 172.25.250.1 state: present Hostname module Change the hostname with hostname module name: Change hostname hostname: name: managedhost1 Firewalld module The firewalld module supports the management of FirewallD on managed hosts - name: Enabling http rule firewalld: service: http permanent: yes state: enabled Configures the eth0 in the external FirewallD zone. - name: Moving eth0 to external firewalld: zone: external interface: eth0 permanent: yes state: enabled Ansible Facts for Network Configuration [user@controlnode ~]$ ansible webservers -m setup or [user@controlnode ~]$ ansible webservers -m setup -a &#39;gather_subset=network filter=ansible_interfaces&#39; or [user@controlnode ~]$ ansible webservers -m setup -a &#39;gather_subset=network filter=ansible_ens4&#39; "],["red-hat-openshift-i-containers-kubernetes-do180.html", "# 5 Red Hat OpenShift I: Containers &amp; Kubernetes - DO180 5.1 Intro to Container 5.2 Creating Containerized Services 5.3 Managing Containers 5.4 Managing Container Images 5.5 Creating Custom Container Images 5.6 Deploying Containerized Applications on OpenShift 5.7 Deploying Multi-Container Applications 5.8 Troubleshooting Containerized Applications", " # 5 Red Hat OpenShift I: Containers &amp; Kubernetes - DO180 5.1 Intro to Container Containers ares a set of one or more processes that are isolated from the rest of the system. Containers provide many of the same benefits as virtual machines, such as security, storage, and network isolation. Containers require far fewer hardware resources and are quick to start and terminate. They also isolate the libraries and the runtime resources (such as CPU and storage) for an application to minimize the impact of any OS update to the host OS. Advantages of using container: Low hardware footprint : Use OS internal features to create and isolate environment, minimizing the use of cpu and memory Environment isolation : changes made to the host OS do not affect the container Quick deployment : no need to install the entire OS Multiple environment deployment : all appls dependencies and environment settings are encapsulated in the container image Reusability : container can be reused without need to set up a full OS 5.1.1 Linux Conetainer Architecture An image is a template for containers that include a runtime environment and all the libs are configuration files, container images need to be locally available for the container runtime, below image repositories available : Red Hat Container Catalog : https://registry.redhat.io Docker Hub : https://hub.docker.com Red Hat Quay : https://quay.io Google Container Registry : https://cloud.google.com/container-registry/ Amazon Elastic Container Registry : https://aws.amazon.com/ecr/ To manage the container we can use podman an open source tool for managing containers and container image if need to install podman : yum install podman 5.1.2 Overview of kubernetes and OpenShift Think : as the number of containers managed by an organization grows, the work of manually starting them rises exponentially along with the need to quickly respond to external demands Enterprise needs: Easy communication between a large number of services Resources limits on applications regardless of the number fo containers running them To respond to application usage spikes to increase or decrease running containers Reacs to service deterioration with health checks Gradual roll out fo a new release to a set of users Kubernetes is an orchestration service that simplifies the deployment, management, and scaling of containerized applications, the smallest unit if kunernetes is a pod that consist of one or more containers. Kubernetes features of top of a container infra: Service discovery and loading balancing : communication by a single DNS entry to each set of container, permits the load balancing across the pool of container. Horizontal scaling : Appl can scale up and down manually or automatically Self-Healing: user-defined health checks to monitor containers to restart in case of failure Automated rollout and rollback : roll updates out to appl containers, if something goes wrong kubernetes can rollback to previous integration of the deployment Secrets and configuration management : can manage the config settings fo application without rebuilding container Operators : use API to update the cluster state reacting to change in the app state Red Hat OpenShift Container Plataform (RHOCP) is a set of modular components and services build on top of Kubernetes, adds the capabilities to provide PaaS platform. OpenShift features to kubernetes cluster : Itegrated developer workflow : integrates a build in container registry, CI/CD pipeline and S2I, a tool to build artifacts from source repositories to container image Routes : expose service to the outside world Metrics and logging : Metric service and aggregated logging Unified UI : UI to manage the different capabilities 5.2 Creating Containerized Services The podman is designed to be a rootless container running as a non-root user, however we can run the container as root if necessary using sudo, but it is a risk and not recommenced. The container image are named based on the syntax registry_name/user_name/image_name:tag registry_name : FQDN or the registry user_name : name of user or organization to which images belongs tag : identifies image version Basic Commands: To search an image podman search &lt;image&gt; To download/pull an image podman pull &lt;image&gt; To retrieve the images podman images To run a Hello World container [user@demo ~]$ podman run ubi8/ubi:8.3 echo &#39;Hello world!&#39; Hello world! To start a container image as a background we can use -d option and to expose a port -p &lt;container port&gt; [user@demo ~]$ podman run -d -p 8080 registry.redhat.io/rhel8/httpd-24 # retrieve the local port on which the container listens [user@demo ~]$ podman port -l # test [user@demo ~]$ curl http://0.0.0.0:44389 To start a bash terminal inside the container [user@demo ~]$ podman run -it ubi8/ubi:8.3 /bin/bash Using variables with -e option [user@demo ~]$ podman run --name mysql-custom \\ &gt; -e MYSQL_USER=redhat -e MYSQL_PASSWORD=r3dh4t \\ &gt; -e MYSQL_ROOT_PASSWORD=r3dh4t \\ &gt; -d registry.redhat.io/rhel8/mysql-80 # test sudo podman exec -ti msql-custom /bin/bash msql -uroot show databases Give a container a name --name, it is important because you can manage your container by name Other common option is -t for pseudo terminal an -i keeps stdin open even if not attached 5.3 Managing Containers 5.3.1 Container Life Cybe management with podman Podman provides a set of subcomands to create and manage containers ? Also subcommands to extract information from containers ? 5.3.2 Creating containers Using podman run command to create containers # sample 1 [user@host ~]$ podman run registry.redhat.io/rhel8/httpd-24 # sample 2 [user@host ~]$ podman run --name my-httpd-container -d registry.redhat.io/rhel8/httpd-24 # sammple 3 [user@host ~]$ podman run -it registry.redhat.io/rhel8/httpd-24 /bin/bash 5.3.3 Run commands in a container We can use exec option to submit the command sample 2 [user@host ~]$ podman exec 7ed6e671a600 cat /etc/hostname # sample 2 this l means latest, last container used [user@host ~]$ podman exec -l cat /etc/hostname 5.3.4 Managing containers List containers running : podman ps List all containers podman ps -a Stop, start or restart a container [user@host ~]$ podman stop|start|restart &lt;container_name&gt; Kill or remove a container [user@host ~]$ podman rm|kill &lt;container_name&gt; Remove or stop all containers podman rm|stop -a Format the output [student@workstation ~]$ podman ps --format=&quot;{{.ID}} {{.Names}} {{.Status}}&quot; a49dba9ff17f mysql Up About a minute ago 5.3.5 Attaching persistent storage to containers Create dir mkdir &lt;dir&gt; The user running the process in the container must be capable of writing files to the dir, for example in MYSQL the UID 27 podman unshare chown -R 27:27 &lt;dir&gt; Apply the container_file_t context to allow container access sudo semanage fcontext -a -t container_file_t &#39;/home/student/dbfiles(/.*)?&#39; Apply the SELinux container policy sudo restorecon -Rv /home/student/dbfiles Mount volume [user@host ~]$ podman run -v /home/student/dbfiles:/var/lib/mysql rhmap47/mysql 5.3.6 Accessing containers To manage the port we use the option -p [&lt;IP address&gt;:][&lt;host port&gt;:]&lt;container port&gt; podman run -d --name apache1 -p 8080:80 registry.redhat.io/rhel8/httpd-24 To see the port assigned podman port &lt;container name&gt; 5.4 Managing Container Images 5.4.1 Accessing Registries Image registries are services offering container images to download. They allow image creators and maintainers to store and distribute container images to public or private audiences. To configure registreis for podman command we need to update /etc/containers/registries.conf To search : [user@host ~]$ podman search [OPTIONS] &lt;term&gt; To authenticate podman login &lt;registry&gt; Pull images [user@host ~]$ podman pull [OPTIONS] [REGISTRY[:PORT]/]NAME[:TAG] [user@host ~]$ podman pull quay.io/bitnami/nginx List local copies podman images Images Tags An image tag is a mechanism to support multiple releases of the same image [user@host ~]$ podman pull registry.redhat.io/rhel8/mysql-80:1 [user@host ~]$ podman run registry.redhat.io/rhel8/mysql-80:1 5.4.2 Manipulating Container Images Save and load an image Images can be saved as .tar file : # Save podman save [-o FILE_NAME] IMAGE_NAME[:TAG] podman save -o mysql.tar registry.redhat.io/rhel8/mysql-80 #Load podman load [-i FILE_NAME] podman load -i mysql.tar Delete an image from local storage podman rmi [OPTIONS] IMAGE [IMAGE...] # To delete all podman rmi -a To modify an image [user@host ~]$ podman commit [OPTIONS] CONTAINER [REPOSITORY[:PORT]/]IMAGE_NAME[:TAG] To see the difference that we have made on container we can use podman diff &lt;image&gt; To commit changes [user@host ~]$ podman commit mysql-basic mysql-custom Tagging Images podman tag [OPTIONS] IMAGE[:TAG] [REGISTRYHOST/][USERNAME/]NAME[:TAG] # sample 1 podman tag mysql-custom devops/mysql # sample 2 podman tag mysql-custom devops/mysql:snapshot Push images to registry [user@host ~]$ podman push [OPTIONS] IMAGE [DESTINATION] # sample [user@host ~]$ podman push quay.io/bitnami/nginx To remove tags from image podman rmi devops/mysql:snapahot 5.5 Creating Custom Container Images 5.5.1 Designing Custom Container Images One method to create a container image is modify the existing one to meet the requirements of the application. Containerfiles are another option that make this task easy to create, share and control the image. Red Hat Software Collections Library (RHSCL) : solution for developers who require the latest development tools that usually do not fit the standard RHEL release schedule. Red Hat Enterprise Linux (RHEL) : stable environment for enterprise applications. We can finding Containerfiles from Red HAt Collections Library, RHSCL is the source of most container images provided by the Red Hat image registry for use by RHEL Atomic Host and OpenShift Container Platform customers. Also Red Hat Container Catalog RHCC is a repository of reliable, tested, certified, and curated collection of container images built on versions of Red Hat Enterprise Linux (RHEL) and related systems Quay.io is an advanced container repository from CoreOS, we can search for container images using httpds://quay.io/search Docker Hub is a repository that anyone can crete and share an image, need to be carreful with images from Docker Hub Source-to-Image (S2I) the OpenShift source-to-image tool is an alternative to using Containerfiles to create new containers that can be use from OpenShift or as standalone s2i utility, The S2I use the follow process to build a custom container image: Start a container from a base container image called the builder image. Fetch the application source code, usually from a Git server, and send it to the container. Build the application binary files inside the container. Save the container, after some clean up ### Building Custom Container images with Containerfiles A Containerfile is a mechanism to automate the building of container images, to build we have three steps: Create a working directory Write the Containerfile Build the image with Podman # This is a comment line FROM ubi8/ubi:8.3 LABEL description=&quot;This is a custom httpd container image&quot; MAINTAINER John Doe &lt;jdoe@xyz.com&gt; RUN yum install -y httpd EXPOSE 80 ENV LogLevel &quot;info&quot; ADD http://someserver.com/filename.pdf /var/www/html COPY ./src/ /var/www/html/ USER apache ENTRYPOINT [&quot;/usr/sbin/httpd&quot;] CMD [&quot;-D&quot;, &quot;FOREGROUND&quot;] FROM : declares that the new container image extends ubi8/ubi:8.3 container base image LABEL: is responsible for adding generic metadata to an image MAINTAINER : Indicates the author RUN : executes commands in a new layer on top of the current image EXPOSE : indicates that the container listens on the specified network port at runtime ENV : is responsible for defining environment variables that are available in the container ADD : copies files or folders from a local or remote source and adds them to the container’s file system, ADD also unpack local .tar files COPY : copies files from the working directory and adds them to the container’s file system USER : specifies the username or the UID to use when running the container image for the RUN, CMD, and ENTRYPOINT instructions ENTRYOINT : specifies the default command to execute when the image runs in a container. CMD : provides the default arguments for the ENTRYPOINT instruction Building Image with Podman Podman build command process the Containerfile and build a new image podman build -t NAME:TAG DIR Image layering Each instruction in a Containerfile create a new layer Sample Creating a container file FROM rhel7:7.5 MAINTAINER Bruno Machado &lt;bmachado@kyndryl.com&gt; LABEL description = &quot;A custom Apache image&quot; ADD training.repo /etc/yum.repos.d/training.repo RUN yum install -y htppd &amp;&amp; \\ yum clean all RUN echo &quot;Hello from Containerfile &quot; &gt; /usr/share/httpd/noindex/index.html EXPOSE 80 ENTRYPOINT [&quot;httpd&quot;,&quot;-D&quot;,&quot;FOREGROUND&quot;] Build # Build sudo podman build -t d080/apache . # Check images: sudo pdman images # Run the container sudo podman run --name lab-apache -d -p 10080:80 do080/apache # Test with curl curl 127.0.0.1:10080 5.6 Deploying Containerized Applications on OpenShift 5.6.1 Describing Kubernetes and OpenShift Architecture Kubernetes and OpenShift Kubernetes is an orchestration service that simplifies the deployment, management, and scaling of containerized applications. Kubernetes Terminology : Node : A server that hosts applications in a Kubernetes cluster. Control Plane : Provides basic cluster services such as APIs or controllers. Compute Node : This node executes workloads for the cluster. Application pods are scheduled onto compute nodes. Resource : kind of component definition managed by Kubernetes. Resources contain the configuration of the managed component and the current state of the component Controller : A controller is a Kubernetes process that watches resources and makes changes attempting to move the current state towards the desired state. Label : A key-value pair that can be assigned to any Kubernetes resource. Selectors use labels to filter eligible resources for scheduling and other operations. Namespace : A scope for Kubernetes resources and processes, so that resources with the same name can be used in different boundaries. Red Hat OpenShift Container Platform is a set of modular components and services built on top of Red Hat CoreOS and Kubernetes. RHOCP adds PaaS capabilities such as remote management, increased security, monitoring and auditing, application lifecycle management, and self-service interfaces for developers. OpenShift Terminology Infra Node : A node server containing infrastructure services like monitoring, logging, or external routing Console : A web UI provided by the RHOCP cluster that allows developers and administrators to interact with cluster resources Project : OpenShift extension of Kubernetes’ namespaces. Allows the definition of user access control (UAC) to resources. CoreOS is a Linux distribution focused on providing an immutable operating system for container execution CRI-O is an implementation of the Kubernetes Container Runtime Interface CRI Kubernetes manages a cluster of hosts, physical or virtual that run containers. Etcd : Key-value store to store config and state information about container and other resources CRD C_ustom Resource Definition_ are resource types stored in Etcd and managed by Kubernetes Containerized services fulfill many PaaS infrastructure functions, such as networking and authorization. Runtimes and xPaaS based container images ready for use for dev RHOCP provides web UI and CLI tools for managing user application OpenShift and Kubernetes architecture illustration On the below fig we can see the control plane that control de cluster, runs on CoreOS, and Node e Infra Pods to do the own work on OpenShift. We can have storage on Ceph, Gluster or from vendor. These runs on Bare metal or on cloud Describing Kubernetes Resource Types Pods (po) : collection of containers that share resources Services (svc) : How pods talk with each other. Single IP/port combination that provides access to a pool of pods Replication Controllers (rc) : how pods are replicated into different notes Persistent Volumes (pv) : Define storage areas to be used by Kubernetes pods. Persistent Volume Claims (pvc) : Represent a request for storage by a pod. ConfigMaps (cm) and Secrets : Contains a set of keys and values that can be used by other resources OpenShift Resource Types Deployment and Deployment Config (dc) : Both are the representation of a set of containers, it contains the configuration to be applied to all containers of each pod replica (images, tags, storage definitions, etc) Build config (bc) : Process to be executed in the OpenShift project. A bc works together with a dc to provide a basic CI/CD workflows. Routes : Represent a DNS host name recognized by the OpenShift router as an ingress point for applications and microservices. 5.6.2 Creating Kubernetes Resources The main method to interacting with an RHOCP is using oc command line oc &lt;command&gt; oc login &lt;clusterURL&gt; The pod resource definition syntax can be provided by JSON or YAML Sample of YAML format apiVersion: v1 kind: Pod metadata: name: wildfly labels: name: wildfly spec: containers: - resources: limits: cpu: 0.5 image: do276/todojee name: wildfly ports: - containerPort: 8080 name: wildfly env: - name: MYSQL_ENV_MYSQL_DATABASE value: items - name: MYSQL_ENV_MYSQL_USER value: user1 - name: MYSQL_ENV_MYSQL_PASSWORD value: mypa55 Discovering services Using IP and port : SVC_NAME_SERVICE_HOST is the service IP address. SVC_NAME_SERVICE_PORT is the service TCP port. Using DNS: SVC_NAME .PROJECT_NAME.svc.cluster.local To create a tunel to our machine [user@host ~]$ oc port-forward mysql-openshift-1-glqrp 3306:3306 Creating application # sample 1 [user@host ~]$ oc new-app mysql MYSQL_USER=user MYSQL_PASSWORD=pass MYSQL_DATABA SE=testdb -l db=mysql Create using docker image # sample 2 using docker img oc new-app --docker-image=myregistry.com/mycompany/myapp --name=myapp Create using Git Repo # sample 3 from Git oc new-app https://github.com/openshift/ruby-hello-world --name=ruby-hello Creating from template # from template $ oc new-app \\ --template=mysql-persistent \\ -p MYSQL_USER=user1 -p MYSQL_PASSWORD=mypa55 -p MYSQL_DATABASE=testdb \\ -p MYSQL_ROOT_PASSWORD=r00tpa55 -p VOLUME_CAPACITY=10Gi ...output omitted.. Managing Persistent Storage List persistent volume objects [admin@host ~]$ oc get pv See the YAML definition for a PersistentVolume [admin@host ~]$ oc get pv pv0001 -o yaml Add more PersistentVolume objects [admin@host ~]$ oc create -f pv1001.yaml Request persistent volume Create a pvc object request : We create a PersistentVolumeClaim (PVC) object to request : apiVersion: v1 kind: PersistentVolumeClaim metadata: name: myapp spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi To create the PVC : [admin@host ~]$ oc create -f pvc.yaml To list the PVCs [admin@host ~]$ oc get pvc Managing OpenShift Resources at the command line The oc get RESOURCE_TYPE command display the summary of all resources oc get all : retrieve a summary of the most important components oc describe RESOURCE_TYPE|RESOURCE_NAME : retrieve additional information oc get RESOURCE_TYPE|RESOURCE_NAME : export a resource definition. oc create RESOURCE_TYPE|RESOURCE_NAME : create resources oc edit: edit resource definitions oc delete RESOURCE_TYPE : remove resource from openshift oc exec CONTAINER_ID : executes commands inside a container. Tip : If we use label we can make reference by label $ oc get svc,deployments -l app=nexus 5.6.3 Creating Routes A route connects a public-facing IP address and DNS host name to an internal-facing service IP. It uses the service resource to find the endpoints; that is, the ports exposed by the service. OpenShift routes are implemented by a cluster-wide router service, sample of route defined using JSON: { &quot;apiVersion&quot;: &quot;v1&quot;, &quot;kind&quot;: &quot;Route&quot;, &quot;metadata&quot;: { &quot;name&quot;: &quot;quoteapp&quot; }, &quot;spec&quot;: { &quot;host&quot;: &quot;quoteapp.apps.example.com&quot;, &quot;to&quot;: { &quot;kind&quot;: &quot;Service&quot;, &quot;name&quot;: &quot;quoteapp&quot; } } } Creating routes $ oc expose service quotedb --name quote Inspect the routes $ oc get pod --all-namespaces | grep router &gt; openshift-ingress router-default-746b5cfb65-f6sdm 1/1 Running 1 4d # describe : $ oc describe pod router-default-746b5cfb65-f6sdm 5.6.4 Creating Applications with Source-to-Image Source-to-Image (S2I) : This tool takes an application’s source code from a Git repository, injects the source code into a base container and produces a new container image that runs the assembled application. Image stream resource is configuration that names specific container images to get the list of default images streams populated by OpenShift $ oc get is -n openshift Buyild an application with S2I and CLI From Git $ oc new-app php~http://my.git.server.com/my-app # or another syntax $ oc new-app -i php http://services.lab.example.com/app --name=myapp From path oc new-app . Using contect $ oc new-app https://github.com/openshift/sti-ruby.git --context-dir=2.0/test/puma-test-app Specific branch $ oc new-app https://github.com/openshift/ruby-hello-world.git#beta4 After creating a new application, the build process starts we can check using get builds command $ oc get builds Check the logs $ oc logs build/myapp-1 Trigger a new build with the oc start-build build_config_name command $ oc get buildconfig $ oc start-build myapp 5.7 Deploying Multi-Container Applications 5.7.1 Considerations for Multi-Container Applications Complex applications have different components such as fron-end, REST back end and database server, it is possible to orchestrate multi-conatiner applications manually, but openshift and kubernets provide tools to facilite that. 5.7.2 Deploying A Multi-Container App on OpenShift Pods are attached to a Kubernetes namespace, which OpenShift calls a project When a pod starts, Kubernetes automatically adds a set of environment variables for each service defined on the same namespace. Any service defined on Kubernetes generates environment variables for the IP address and port number where the service is available Kubernetes automatically injects these environment variables into the containers from pods in the same namespace &lt;SERVICE_NAME&gt;_SERVICE_HOST: Represents the IP address enabled by a service to accessa pod. &lt;SERVICE_NAME&gt;_SERVICE_PORT: Represents the port where the server port is listed. &lt;SERVICE_NAME&gt;_PORT: Represents the address, port, and protocol provided by the service for external access. &lt;SERVICE_NAME&gt;_PORT_&lt;PORT_NUMBER&gt;_&lt;PROTOCOL&gt;: Defines an alias for the &lt;SERVICE_NAME&gt;_PORT. &lt;SERVICE_NAME&gt;_PORT_&lt;PORT_NUMBER&gt;_&lt;PROTOCOL&gt;_PROTO: Identifies the protocol type (TCP or UDP). &lt;SERVICE_NAME&gt;_PORT_&lt;PORT_NUMBER&gt;_&lt;PROTOCOL&gt;_PORT: Defines an alias for &lt;SERVICE_NAME&gt;_SERVICE_PORT. • &lt;SERVICE_NAME&gt;_PORT_&lt;PORT_NUMBER&gt;_&lt;PROTOCOL&gt;_ADDR: Defines an alias for &lt;SERVICE_NAME&gt;_SERVICE_HOST. 5.7.3 Deploying a Multi-container Application on OpenShift Using a Template A template defines a set of related resources to be created together, as well as a set of application parameters. The OpenShift installer creates several templates by default in the openshift namespace. [user@host ~]$ oc get templates -n openshift Check the YAML definition of template [user@host ~]$ oc get template mysql-persistent -n openshift -o yaml Assuming the template is defined in the todo-template.yaml file, use the oc create command to publish the application template: [user@host deploy-multicontainer]$ oc create -f todo-template.yaml list available parameters from a template. [user@host ~]$ oc describe template mysql-persistent -n openshift or [user@host ~]$ oc process --parameters mysql-persistent -n openshift Processing a Template Using the CLI [user@host ~]$ oc process -o yaml -f &lt;filename&gt; Templates often generate resources with configurable attributes that are based on the template parameters. To override a parameter, use the -p option followed by a = pair. [user@host ~]$ oc process -o yaml -f mysql.yaml -p MYSQL_USER=dev -p MYSQL_PASSWORD=$P4SSD -p MYSQL_DATABASE=bank -p VOLUME_CAPACITY=10Gi &gt; mysqlProcessed.yaml To create an application use the generated YAML [user@host ~]$ oc create -f mysqlProcessed.yaml Alternatively, it is possible to process the template and create the application without saving a resource definition file by using a UNIX pipe: [user@host ~]$ oc process -f mysql.yaml -p MYSQL_USER=dev -p MYSQL_PASSWORD=$P4SSD -p MYSQL_DATABASE=bank -p VOLUME_CAPACITY=10Gi | oc create -f - 5.8 Troubleshooting Containerized Applications 5.8.1 Troubleshooting S2I Builds and Deployments Introduction to the S2I Process S2I is simple way to create image, however problems can heppens is important to keep in mind the workflow for most of the program languages : Build step : compiling source code, packaging the application as a container image push the image to the OpenShift registry for deployment step BC (BuildConfig) drive the build step Deployment step : starting a pod and making the application available executes after the build step Check the logs $ oc logs bc/&lt;application-name&gt; request a new build: $ oc start-build &lt;application-name&gt; Check the deployment logs $ oc logs deployment/&lt;application-name&gt; Common Problems The oc logs command provides important information about the build, deploy, and run processes of an application during the execution of a pod. The logs may include : * missing values * options that must be enabled * incorrect parameters or flags * environment incompatibilities,etc 5.8.2 Troubleshooting Containerized Applications Sometimes sysadmin need special access to container for example the dba need access the database container to check the database or sysadmin need to restart an specific service on a container, podman provides port forwarding features by using -p option with the `run`` subcommand $ podman run --name db -p 30306:3306 mysql OpenShift provide the oc port-foward command for fowarding a local port to a pod port. $ oc port-forward db 30306 3306 To access the container logs $ podman logs &lt;containerName&gt; To access openshift logs $ oc logs &lt;podName&gt; To read openshift events $ oc get events Accessing Running Container $ podman exec [options] container command [arguments $ oc exec [options] pod [-c container] -- command [arguments] # sample $ oc exec -it myhttpdpod /bin/bash $ podman exec apache-container cat /var/log/httpd/error_log Transfer file to and from containers $ podman run -v /conf:/etc/httpd/conf -d do180/apache $ podman cp standalone.conf todoapi:/opt/jboss/standalone/conf/standalone.conf $ podman cp todoapi:/opt/jboss/standalone/conf/standalone.conf . "],["red-hat-openshift-administration-ii-operating-a-production-kubernetes-cluster-do280.html", "# 6 Red Hat OpenShift Administration II: Operating a Production Kubernetes Cluster - DO280 6.1 Describing the Red Hat OpenShift Container Plataform 6.2 Verifying the Health of a Cluster 6.3 Configuring Authentication and Authorization 6.4 Configuring Application Security 6.5 Configuring OpenShift Networking For Applications", " # 6 Red Hat OpenShift Administration II: Operating a Production Kubernetes Cluster - DO280 ** 6.1 Describing the Red Hat OpenShift Container Plataform 6.1.1 Describing OpenShift Container Platform RHOCP is based on Kubernetes and allow manage container at scale, a container orchestrator platform manages a cluster service that runs multiple containerized applications . Solutions : Red Hat OpenShift Container Platform : Enterprise-ready Kubernetes environment for building, deploying and managing container-based applications on any public or private data center. Red Hat decide when update to newer releases and which addition component to enable Red Hat OpenShift Dedicated : Managed OpenShift environment in a public cloud, AWS, GCP, Azure, or IBM Cloud, all features of RHOCP, however Red Hat manage the cluster, we have some control of decisions as when to update to a newer release or to install add-ons. Red Hat OpenShift Online : Public container platform shared across multiple customers, Red Hat manages the cluster life cycle. Red Hat OpenShift Kubernetes Engine : Subset of the features present in Red Hat OpenShift RCOP, such as coreOS, CRI-O engine, web console, etc Red Hat Code Ready Container : Minimal installation of OpenShift that we can run on a laptop to development and experimentation. Below the services and features of Openshift Introduction of OpenShift features Comparing OpenShift Container Platform vs OpenShift Kubernetes Engine: Features : High Availability : etc cluster store the state of the OpenShift Cluster and Applications Lightweight OS : CoreOS focuses on agility, portability and security Load Balancing : External via API, HAProxy load balance for external app and internal load balance Automating Scaling : can adapt to increased application traffic in real time by automatically starting new containers and terminate when the load decrease. Logging and Monitoring : Advanced monitoring solution based on Prometheus, also advanced logging solution based on ElasticSearch. Service Discovery : Internal DNS, application can rely on friendly names to find other app and services Storage : Allow automatic provisioning of storage on popular cloud providers and visualization platforms Application Management : Automate the development and deploy, automatic build containers based on source code using Source-To-Image (S2I) solution. Cluster Extensibility : Rely on standard extension from kubernetes, Openshift packages these extensions as operators for ease of installation, update, and management. Also include Operator Lifecycle Manager (OLM), which facilitates the discovery, installation, and update of applications and infrastructure components packaged as operators OpenShift also includes the Operator Lifecycle Manager (OLM) which facilitates the discovery, installation, and update of applications and infrastructure components packaged as operators 6.1.2 Architecture of OpenShift OpenShift architecture is based on declarative the nature of kubernetes. In a declarative architecture, you change the state of the system and the system updates itself to comply with the new state. Kubernetes cluster consists of a set of nodes that run the kubelet system service and a container engine. OpenShift runs exclusively the CRI-O container engine. Some nodes are control plane nodes that run the REST API, the etcd database, and the platform controllers OpenShift is a Kubernetes distribution that provides many of these components already integrated and configured, and managed by operators. OpenShift also provides preinstalled applications, such as a container image registry and a web console, managed by operators. 6.1.3 Cluster Operators Kubernetes operators are applications that invoke the Kubernetes API to manage Kubernetes resources. Custom resources (CR) : store settings and configurations Custom resource definition (CRD) : the syntax of a custom resource is defined by a custom resource definition Most operators manage another application; for example, an operator that manages a database server. The purpose of an operator is usually to automate tasks. Operator Framework Operator Software Development Kit (Operator SDK) : Golang library and source code. Also provide container image and ansible playbook examples. Operator Life Cycle Manager (OLM) : Application that manages the deployment, resource utilization, updates and deletion of operators. The OLM itself is an operator that comes preinstalled with OpenShift. OperatorHub OperatorHub provides a web interface to discover and publish operators that follow the Operator Framework standards. Red Hat Marketplace is a platform that allow access a curated set of enterprise operators that can be deployed on OpenShift or a kubernetes cluster OpenShift Cluster Operators : regular operators except that they are not managed by the OLM, they are managed by OpenShift Cluster Version Operators, also called as first level operator. 6.2 Verifying the Health of a Cluster 6.2.1 Intro to OpenShift Installtion Methods Full-stack Automation : Installe provisions all compute, storage and network, on cloud or virtualization Pre-existing Infrastructure : we can configure a set of compute, storage and network resources, can be configured on bare-metal, cloud or virtualizations providers Deploy process Install stages that results in a fully running OpenShift control plane : The bootstrap machine boots and starts hosting the remote resources for booting the control plane machine, “like a repo” Control plane machine fetch the remote resources from bootstrap machine Control plane form an Etcd cluster Bootstrap machine starts a temp kubernetes control plane The temp control plane schedule the control plane to the control plane machines The temp control plane shuts down Bootstraps injects components to OpenShift into control plane Installer tears down the bootstrap machine We can customize the installer by adding custom storage class, change custom resources, adding new operators and defining new machine sets. 6.2.2 Troubleshooting OpenShit Cluster and Applications Commands : oc get nodes : Status of each node oc adm top nodes : CPU and Memory of each node oc describe node &lt;my_node-name&gt; : Resources available and used pc get clusterversion : version of cluster oc describe clusterversion : mode details about cluster status oc get clusteroperators : list of all cluster operators oc adm node-logs -u &lt;unit&gt; &lt;my-node-name&gt; : view logs Unit can be : crio, kubelet, etc oc adm node-logs &gt;my-node-name&gt; : display all journal logs of a node oc logs &lt;my-pode-name&gt; show de logs of pod oc logs &lt;my-pod-name&gt; -c &lt;my-container-name&gt; : show logs of container Debug oc debug node/&lt;my-node-name&gt; chroot /host systemctl is-active kubelet oc debug node/&lt;my-node-name&gt; chroot /host crictl ps Debug as root [user@host ~]$ oc debug deployment/my-deployment-name --as-root Changing a running container oc rsh &lt;my-pod-name&gt; open shell inside the a pod oc cp /local/path my-pod-name:/conatiner/path : copy files oc port-forward my-pod-name local-port:remote-port : create a tcp tunel oc get pod --level 6 : Show logs on different levels oc whoami -t : Make a token that the oc command use 6.2.3 Introducing OpenShift Dynamic Storage Container offers two main ways of maitaining persistent storage, using volumes and bind mounts. Volumes are managed manuall by admin or dynamically via storage class Devs can mount a local directory into a container using bind mount OpenShift use Kubernetes persistent volume framework to manage persistent storage dynamic or static. A persistent volume claim (PVC), where appl going to request a type of storage, belongs to a specific project. To create a PVC, you must specify the access mode and size, among other options. Once created, a PVC cannot be shared between projects. Developers use a PVC to access a persistent volume (PV). Verify the Dynamic Provisioned storage [user@host ~]$ oc get storageclass Deploying Dynamically Provisioned Storage, to add volume to an application create a PersistentVolumeClaim resource and add it to application as a volume [user@host ~]$ oc set volumes deployment/example-application \\ --add --name example-storage --type pvc --claim-class nfs-storage \\ --claim-mode rwo --claim-size 15Gi --mount-path /var/lib/example-app \\ --claim-name example-storage Deleting Persistent Volume Claims [user@host ~]$ oc delete pvc/example-pvc-storage 6.3 Configuring Authentication and Authorization 6.3.1 Identity User : Interact with the API server Identity : keeps a record of successful authentication attempts from a specific user and identity provider. Only a single user resource is associated with an identity resource. Service Account : enable you to control API access without the need to borrow a regular user’s credentials Group : set of users Role : defines a set of permissions that enables a user to perform API operations over one or more resources The authentication and autorization security layers The authentication layer authenticates the userusing (OAuth Access tokens or X.509 Client Certification). Upon successful authentication, the authorization layer decides to either honor or reject the API request. The authorization layer use RBAC policies. Authentication Operator : OpenShift OAuth server can be configured to use many identity providers. HTPasswd : Validate user and password against a secret using htpasswd Keystone : Enables shared authentication with an OpenStack Keystone v3 server. LDAP : Configure LDAP`identity provider to validate against an LDAPv3 server GitHub or GitHub Enterprise : use OAuth github authentication OpenID Connect : Use Code Flow integrates with an OpenID connect During install OpenShift create a kubeconfig file that contains specific details and parameter to be used by CLI INFO Run &#39;export KUBECONFIG=root/auth/kubeconfig&#39; to manage the cluster with &#39;oc&#39;. To use kubeconfig file [user@host ~]$ export KUBECONFIG=/home/user/auth/kubeconfig [user@host ~]$ oc get nodes # or [user@host ~]$ oc --kubeconfig /home/user/auth/kubeconfig get nodes 6.3.2 Configuring the HTPasswd Identity Provider Need to update the OAuth custom resource add the .spec.identityProviders array : apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - name: my_htpasswd_provider mappingMethod: claim type: HTPasswd htpasswd: fileData: name: htpasswd-secret To export the current OAuth cluster resource in YAML format [user@host ~]$ oc get oauth cluster -o yaml &gt; oauth.yaml Open the file edit and replace on cluster [user@host ~]$ oc replace -f oauth.yaml Managing Users with the HTPasswd Identity Provider Create an HTPasswd File [user@host ~]$ htpasswd -c -B -b /tmp/htpasswd student redhat123 Add or update credentials [user@host ~]$ htpasswd -b /tmp/htpasswd student redhat1234 Create the HTPasswd Secret [user@host ~]$ oc create secret generic htpasswd-secret --from-file htpasswd=/tmp/htpasswd -n openshift-config Extracting Secret Data oc extract secret/htpasswd-secret -n openshift-config --to /tmp/ --confirm /tmp/htpasswd Updating the HTPasswd Secret oc set data secret/htpasswd-secret --from-file htpasswd=/tmp/htpasswd -n openshift-config Watch the redeploy pods [user@host ~]$ watch oc get pods -n openshift-authentication Deleting Users and Identities To delete the user from htpasswd [user@host ~]$ htpasswd -D /tmp/htpasswd manager Update the secret to remove all remnants of the user oc set data secret/htpasswd-secret --from-file htpasswd=/tmp/htpasswd -n openshift-config To remove the user resource [user@host ~]$ oc delete user manager To delete identity resource # check [user@host ~]$ oc get identities | grep manager # to delete [user@host ~]$ oc delete identity my_htpasswd_provider:manager Assigning Administrative Privileges # assigns the cluster-admin role to the student user. [user@host ~]$ oc adm policy add-cluster-role-to-user cluster-admin student 6.4 Configuring Application Security 6.4.1 Managing Sensitive Information with secrets Kubernetes and OpenShift use secret resources to hold sensitive information : Password Sensitive configuration files Credentials to an external resource, such as SSH Key or OAuth token Secrete is Base64-encoded, not stored in plain text, we can encrupt the Etcd and that encrypt secrets, config maps, routes, OAuth. Featuers of Secrets Can be shared within project namespace Administrators can create and manage secrets that other team cdan reference in thei deploy config Secret data is injected into pods After a secret value changes we must create a new pods to inject the new data OpenShift exposes sensitive data to a pod as environment variable Use cases for secrets Credentials If an application expects to read sensitive information from a file, then you mount the secret as a data volume to the pod. Some applications use environment variables to read configuration and sensitive data. You can link secret variables to pod environment variables in a deployment configuration. TLS and Key Pairs Used to to secure communication to a pod Developers can mount the secret as a volume and create a pass through route to the application. Creating Secret Using Key-value [user@host ~]$ oc create secret generic secret_name --from-literal key1=secret1 --from-literal key2=secret2 Using Key names [user@host ~]$ oc create secret generic ssh-keys --from-file id_rsa=/path-to/id_rsa --from-file id_rsa.pub=/path-to/id_rsa.pub Create a TLS secret [user@host ~]$ oc create secret tls secret-tls --cert /path-to-certificate --key /path-to-key Exposing Secrets to Pods Create the secret [user@host ~]$ oc create secret generic demo-secret --from-literal user=demo-user --from-literal root_password=zT1KTgk Modify the env variable section of the deploy config env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: demo-secret key: root_password Set application environment variables from either secrets or configuration maps. [user@host ~]$ oc set env deployment/demo --from secret/demo-secret --prefix MYSQL_ A secret can be mounted [user@host ~]$ oc set volume deployment/demo --add --type secret --secret-name demo-secret --mount-path /app-secrets Configuration Map Similar to secrets, configuration maps decouple configuration information from container images. Unlike secrets, the information contained in configuration maps does not require protection [user@host ~]$ oc create configmap my-config --from-literal key1=config1 --from-literal key2=config2 Updating Secrets and Configuration Maps Extract the last data [user@host ~]$ oc extract secret/htpasswd-ppklq -n openshift-config --to /tmp/ --confirm Update and save the files Use oc set data to update [user@host ~]$ oc set data secret/htpasswd-ppklq -n openshift-config --from-file /tmp/htpasswd 6.4.2 Controlling Application Permissions with Security Contect Contrainst Security Context Constraints (SCCs) : a security mechanism that restricts access to resources, but not to operations in OpenShift. Check the list of SCCs oc get scc Get additional info about an SCC [user@host ~]$ oc describe scc anyuid View the security contect constraint that pod uses [user@host ~]$ oc describe pod console-5df4fcbb47-67c52 -n openshift-console | grep scc Use the scc-subject-review subcommand to list all the security context constraints that can overcome the limitations of a container: [user@host ~]$ oc get pod podname -o yaml | oc adm policy scc-subject-review -f - To change the container to run using a different SCC, you must create a service account bound to a pod. [user@host ~]$ oc create serviceaccount service-account-name Associate the service account with an SCC [user@host ~]$ oc adm policy add-scc-to-user SCC -z service-account Modify an existing deployment or deploy config [user@host ~]$ oc set serviceaccount deployment/deployment-name service-account-name 6.5 Configuring OpenShift Networking For Applications 6.5.1 OpenShift Software-defined Networking SDN (software-defined network) is a networking model that allows you to manage network services through the abstraction of several networking layers. SDN use CNI plug-ins that allows containers inside pods share network resources Common CNI plug-ins : OpenShift SDN, OVN-Kubernetes and kuryr OpenShift implmenets the SDN to manage networking infrastructure of the cluster and user applications We can : Manage network traffic and decice how to expose the applications Manage communications between containers that run in the same projects Manage communication between pods Manage network from pod to a service Manage network from an external network to a service, or from containers to external networks Using Services for accessing pods Services rely on selectors (labels) that indicate which pods receive the traffic through the service. Each pod matching these selectors is added to the service resource as an endpoint. As pods are created and killed, the service automatically updates the endpoints. OpenShift uses two subnets (onde for pods and another one for services). The traffic is forwarded in a transparent way to the pods; an agent manages routing rules to route traffic to the pods that match the selectors. DNS Operator The DNS operator deploys and runs a DNS server managed by CoreDNS. The DNS operator provides DNS name resolution between pods To check [user@demo ~]$ oc describe dns.operator/default Cluster Network Operator OpenShift Container Platform uses the Cluster Network Operator for managing the SDN To consult the SDN configuration which is managed by the Network.config.openshift.io [user@demo ~]$ oc get network/cluster -o yaml 6.5.2 Exposing Applications for External Access Accessing Application from External Networks Can use : HTTP HTTPS TCP non-TCP Others using Ingress and Route OpenShift Route allow expose applicationn to external network Describing Methods for Managing Ingress Traffic OpenShift implements the Ingress Controller with a shared router service that runs as a pod inside the cluster. Route : Provide ingress traffic to services in the cluster Provide features that may not be supported by Kubernetes ingress such as TLS re-encryption, TLS passthrough and split traffic for blue-green deployments Ingress : A Kubernetes resource that provides some of the same features as routes Accept external requests and proxy them based on the route. Only allow HTTP, HTTPS and server name identification SNI and TLS with SNI Others External load balancer : A load balancer instructs OpenShift to interact with the cloud provider in which the cluster is running to provision a load balancer. Service external IP : This method instructs OpenShift to set NAT rules to redirect traffic from one of the cluster IPs to the container NodePort : With this method, OpenShift exposes a service on a static port on the node IP address. You must ensure that the external IP addresses are properly routed to the nodes. Creating Routes [user@host ~]$ oc expose service api-frontend --hostname api.apps.acme.com Securing Routes Edge Passthrough Re-encryption how to create a secure edge route with TLS certificate [user@host ~]$ oc create route edge --service api-frontend --hostname api.apps.acme.com --key api.key --cert api.crt 6.5.3 Configuring Network Policies Network policies allow you to configure isolation policies for individual pods To manage network communication between two namespaces, assign a label to the namespace that needs access to another namespace. The following command assigns the name=network-1 label to the network-1 namespace: [user@host ~]$ oc label namespace network-1 name=network-1 "],["references.html", "References", " References "],["cloud.html", "# 7 Cloud 7.1 Azure 7.2 AWS 7.3 GCP", " # 7 Cloud 7.1 Azure 7.2 AWS 7.2.1 Part 1 : Cloud Concepts Cloud computing : Agility Maintenance Reliability Security Performance Scalability Cost and Elasticity soft limit : 30 ec2 per month, if need more request to aws hard limit : data center limit According to NIST (The National Institute of Standards and Technology) On demand Self-service Broad network access Resource pooling Rapid elasticity Measured service Cloud models or Service Models IaaS : Infra as a service PaaS : Platform as a service FaaS : Function as a service Deployment models Private Cloud Public Cloud Hybrid Cloud : Using Private and Public Multicloud Community 7.2.2 Part 2 : Foundation Also known as On-boarding, but the this is the best practice to start use the cloud, or how to plan the use of AWS account. There are lot of options and layers. ROOT ACCOUNTS Principal account on AWS Used to start the new account in AWS Manager billing and other accounts First account to create on AWS cloud If you going to use Production on AWS you going to have more than one account OU Organization Unit Option to organize the account such as DEV, QA, PROD, we can apply rules to OU or for account, like production runs on Sao Paulo, DEV on USA, QA can only start machine from group X, etc. Can have OU for RH, Department , Business Unit, etc ACCOUNT Where the resource are started or created Tips: Not use a personal email to create an account, use a team email Read the emails and attention to account to have a good score Every account have IAM, to mange the users Create an account on aws.amazon.com Need two emails , Name and credict card Links AWS Landing Zone AWS Organizations AWS Account AWS SCP Example Service Control Policies AWS Calculator 7.2.3 Part 3 : AWS Well-Architected Framework that help you know the risks and how to fix, it contain papers and instructos to guide you on Best Practices ( pillars, design principles and questions) to apply on your account. AWS Well-Architected Framework link Best practice Framework Pillars Operational Excellence Pillar Security Pillar Reliability Pillar Performance Efficiency Pillar Cost Optimization Pillar Sustainability Pillar Design Principles Questions Why to use ? Speed Mitigate risks Make informed decisions Use AWS best practices Links: AWS Well-Architected Tool AWS Well-Architected Doc The 5 Pillars of the AWS Well-Architected Framework 7.2.4 Part 4 : IAM Manage Security , Identity and Account Define groups and attach the polices for that group Create the Users and associate to groups Always enable MFA for user authenticate Service users that going to be used via API its a good practice to have svc or service on name , like , svc-dev, this option will enable an access key ID and secret access key for the AWS API, CLI, SDK, and other development tools The normal user will be created when you select Password - AWS Management Console access, that enable a password that allows users to sign-in to the AWS Management Console. Role There are several roles defined by AWS but you can create your own role Allow you enable one resource , for instance EC2, to talk with another resource like IAM without access key, only using Roles and policies Policies There are several policies defined by AWS, but we can create our policy AdministratorAccess: Provide full access to AWS service and resources PowerUserAccess: Provide full access to AWS service and resources, but do not allow manage User and Groups Links: AWS IAM Best Practices Access keys best practices AWS CLI 7.2.5 Part 5 : IAM cont… Cross Account Roles : When you create a role (type another account) on the target account associate it with account if from your master, you also have to specify the permission PowerUser or Adminsitrator for instance, with that users on that group can move between accounts : Create a role CrossRoleDevOpsEngineers for instance on the target account Add the permission that you would like to have when you going to assume that role for instance AdministratorAccess You will need to add the Account ID from source Account to link the source and target account. On source or master account use the role to configure the cross account using Swhich Role Inform the ID from target Account and Role that you will use. With that you can create users in one IAM account and the users can go to others accounts using assume roles (switch account). On that IAM account the users will only login and jump to another account we should not create resources here. Links: Credential Reports Identity Providers and Federantion Habilitando ADFS para AWS Cross Account using IAM Roles 7.2.6 Part 6 : EC2 EC2 (Elastic Compute Cloud) : Service that offer cloud compute on AWS Amazon EC2 Tipos e preços Amazon EBS Montar o EBS no Linux Amazon Security Group Fazer um video criando EC2, volumes, securty group etc 7.2.7 Part 7 : EC2 cont … With EC2 stopped we can create a snapshot of the instance, which allow us restore the image or create a new one, for that option we create an AMI image and create an ec2 instance based on that AMI. IF I need to mirgate the EC2 to another region I need to create a spanshot of volume, perform a copy of snapshot and recreate the AMI image from that copy on target region and recreate the EC2 on target region. A second option to migrate EC2 from region is performing a copy on AMI source to target region. To get information about metdata about instance we need to access the ip 169.254.169.254 curl http://169.254.169.254/latest/meta-data/&lt;option&gt; AWS EC2 Metadata Snapshot AMIs Placement Groups Lifecycle Manager 7.2.8 Part 8 , 9 : VPC VPC (Virtual Private Cloud) Enable you to use your own isolated resources within the AWS account, it is like we have on AWS a dedicate/isolated network for our account. Imagine is you have on AWS a dedicate/privated network to you ? Yes that is VPC, part of AWS infra is dedicate to you and you going to receive an ip interval dedicated to you. Where you see a bit on like 1 1 1 1 1 1 1 1 or 255 is related a network, where you see the bit off 0 0 0 0 0 0 0 0 or 0 is related to hosts/ips, we can use the Subnet-Calculater to calculate the CIDR blocks. Related CIDR the first sample of network 10.0.0.0 have the CIDR 8 because only the first octet is on 10.0.0.0/8, on second example of 192.168.1.0/24 the 3 first octet is on as on the figure. Also the first IP is related to gateway 10.255.255.1 and last one is broadcast 10.255.255.255 in the example of 10.0.0.0/8 On AWS : 10.0.0.0 : Network address 10.0.0.1 : Reserved by AWS for VPC router 10.0.0.2 : Reserved by AWS for DNS 10.0.0.3 : Reserved for fugure use by AWS 10.0.0.255: Broadcast On AWS Expert training the instructor create this diagram to create the structure on AWS : Steps : Create a VPC , you need to provide the Name and IP CIDR block like (10.0.0.0/16) and click in create, this step will create automatically a Route table, Network ACL , security group and internet gateway. Create the subnet, select the VPC that we have just created and add : 1st : Name Tag like (public-10.0.1.0/24-az), select the Availability Zone and IPv4 CIDR block (10.0.1.0/24) 2nd : : Name Tag like (private-10.0.2.0/24-az), select the Availability Zone and IPv4 CIDR block (10.0.2.0/24) 3rd : : Name Tag like (mgmt-10.0.3.0/24-az), select the Availability Zone and IPv4 CIDR block (10.0.3.0/24) After create enable the auto assign IP settings and enable public IPv4 address to public subnet public-10.0.1.0/24-az by default the subnet is open to internet ATTENTION Create the security groups for each subnet : Click in create security group inform the names , description and vpc : 1st : : (public-sec-group) and associate with VPC 2nd : : (private-sec-group) and associate with VPC 3rd : : (mgnt-sec-group) and associate with VPC On public security group add the ssh on 22 port rule open to everyone Add a HTTP rule port 80 to everone Add a HTTPS rule port 443 to everyone On private security group add Rule for SSH source add the mgnt IP 10.0.3.0/24 or the security group info (mngt) security group, thi will allow ssh only from public subnet If there are MSQL/Aurora add a rule for port 3306 and add the security groups from public and mgnt On management security group add ssh rule and ip for admin, idealy have a vpn to access this server Create internet gateway add the name tag, click in action and attach to our VPC Create a route table add the Name tag and vpc one for each subnet (private, public and mgnt) On public add the route to internet (Destination) 0.0.0.0/0 (Target) internet gateway , click in edit subnet associations and add the public subnet On private edit the subnet associations and add the private subnet On mgmt edit and add the mgmt subet, with internet route 0.0.0.0/0 with Internet gateway as a Target Create 3 EC2 instance 1 for web server on public subnet 1 for db server on private subnet 1 for admin on mgmt subnet Create NAT Gateway, inform the subnet, on the example the public and create new Elastic IP To enable the dbserver that is in private subnet access the internet we need to update the route and add 0.0.0.0/0 and select nat gateway instance Creating NAT INSTANCE , to create a nat instance we need to launch EC2 instance and select AMI of nat on community AMI’s, select VPC and public subnet. On Private route table select (Destination) 0.0.0.0/0 (Target) NAT INSTANCE On NAT instance select Action &gt; Network &gt; Change Source Destination check and disable Create network ACL, inform name and VPC Select Edit inbound rules : Add rules number 100 Type custom , port 80 source 0.0.0.0/0, ALLOW number 200 Type HTTPS,, port 443 source 0.0.0.0/0, ALLOW number 300 Type custom , port 22 source 0.0.0.0/0, ALLOW On Edit outbound rules add the same rules number 100 Type custom , port 80 source 0.0.0.0/0, ALLOW number 200 Type HTTPS, port 443 source 0.0.0.0/0, ALLOW number 300 Type custom , port 22 source 0.0.0.0/0, ALLOW We will also need to add the Ephemeral ports Edit subnet associations and add the subnet that would like to associate VPC Amazon VPC quotas Calculadora Subnet Internet Gateway Route Tables 7.2.9 Part 10: VPN What is uma VPN ? AWS Virtual Private Network (AWS VPN) establishes a secure and private tunnel from your network or device to the AWS Cloud. You can extend your existing on-premises network into a VPC, or connect to other AWS resources from a client. AWS VPN offers two types of private connectivity that feature the high availability and robust security necessary for your data. Types : Site-to-Site Client VPN How to create / configure To set up a Site-to-Site VPN connection using a virtual private gateway: Step 1: Create a customer gateway Step 2: Create a target gateway Step 3: Configure routing Step 4: Update your security group Step 5: Create a Site-to-Site VPN connection Step 6: Download the configuration file Step 7: Configure the customer gateway device To set up Client VPN Step 1: Create a CA raiz on Certificate Manager Step 2: Request a private certificate associated with CA Step 3: Create Client VPN Endpoint, use the certificated that you have just created Step 4: Associate the VPN to VPC and subnet Step 5: Create anew security group or associate ao security group of subnet, remmeber to enable the port `1194` UDB on security group Step 6: Add authorization rule to specific network or all (`0.0.0.0/0`) Step 7: Create route, add subnet and route destionation for all (`0.0.0.0/0`) Step 8: Download client configuration file Step 9: Update the file adding the sections: * &lt;cert&gt; &lt;/cert&gt; : Export the certificate PEM and add here the body * &lt;key&gt; &lt;/key&gt; : Private key from CA add here Step 10: Import the file on client VPN 7.2.10 Part 11: ELB ELB Elastic Load Balancing Have a name or ip to re-direct to my target(server) Application Load Balancer (HTTP, HTTPS) Network Load Balancer (TCP, TLS, UDP) 7.3 GCP 7.3.1 Digital Leader Cloud Digital Leader INTRODUCTION TO DIGITAL TRANSFORMATION WITH GOOGLE CLOUD What is Cloud ? : Meetaphor for the network of data centers that store and compute information available through the internet Technology and processes need to store, manage, and access data that is transferred over the cloud Change on how organization operates and optimezes internal resources and how it delivers value to customers Cloud enables and redefines our ability to collaborate, perceive, categorize, predict and recommend in every industry for every activity 7.3.2 Cloud Engineer 7.3.3 Machine Learning Engineer 7.3.4 Data Engineer "],["devops.html", "# 8 DevOps 8.1 Docker 8.2 Jenkins 8.3 Git", " # 8 DevOps 8.1 Docker Livro Descomplicando Docker Github 8.1.1 Part 1 8.1.1.1 O que é container ? Container não é virtualização e sim isolamento Isolamento lógico Responsável Namesapace parte usuaŕios e processos como se tivessemos isolado um pedaço da máquina para o container dentro do container tenho isolamento de network, cada container tem sua interface Isolamento de físico _\"Responsável Cgroup\" recursos : CPU, RAM, IO rede, IO de bloco, etc 8.1.1.2 O que é o Docker ? Uma imagem de container é divida em camadas e so se escreve na última camanda , as abaixo são somente leitura Instalar curl -fsSL https://get.docker.com | bash Versão instalada root@turing:~# docker version Client: Docker Engine - Community Version: 20.10.8 API version: 1.41 Go version: go1.16.6 Git commit: 3967b7d Built: Fri Jul 30 19:54:22 2021 OS/Arch: linux/amd64 Context: default Experimental: true Server: Docker Engine - Community Engine: Version: 20.10.8 API version: 1.41 (minimum version 1.12) Go version: go1.16.6 Git commit: 75249d8 Built: Fri Jul 30 19:52:31 2021 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.4.9 GitCommit: e25210fe30a0a703442421b0f60afac609f950a3 runc: Version: 1.0.1 GitCommit: v1.0.1-0-g4144b63 docker-init: Version: 0.19.0 GitCommit: de40ad0 root@turing:~# Adicionar usuário ao grupo do docker usermod -aG docker &lt;user&gt; Listar os containers docker container ls Hello World docker container run -ti hello-world Steps that docker perform on docker container run ….: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. 8.1.1.3 Commandos básicos Listar todas as imagens : docker container ls -a Lista todos os containers rodando : docker container ls Exemplo criar um container -ti : Terminal e interatividade Ctrl D : mata o bash o principal processo do container e o container é finalizado Ctrl q p: Para sair do container sem encerrar o bash e container -d : Para colocar o container como daemon docker run -it ubuntu bash Reconectar ao container : docker container attach &lt;Container ID ou nome&gt; Remover o container : docker container rm &lt;ID ou nome&gt; Stop / Start / Restart / Pause container : Stop : docker container stop &lt;container ID ou nome&gt; Start: docker container start &lt;container ID ou nome&gt; Restart : docker container restart &lt;container ID ou nome&gt; Pause : docker container pause &lt;container ID ou nome&gt; Unpause : docker container unpause &lt;container ID ou nome&gt; Informações do container : docker container inspect &lt;container ID ou nome&gt; Logs : docker container logs -f &lt;ID ou nome&gt; Update para fazer atualização em um container em execução docker container update Listar as imagens docker image ls 8.1.1.4 CPU and RAM - containers Para verificar o quanto o docker esta utilizando de recursos docker container stats &lt;container ID&gt; Verificar os processos docker container top &lt;container ID&gt; Liminar o máximo de memória que o container nginx pode utilizar com parametro m # Limitando a memória em 128M docker container run -d -m 128M nginx Limitar a CPU # Limitando a CPU em 50% ou meio CPU docker container run -d -m 128M --cpus 0.5 nginx Para fazer teste de stress utilizando pacote stress do linux # Para instalar o stress apt-get update &amp;&amp; apt-get install -y stress stress -cpu 1 -vm 1 --vm-bytes 64M 8.1.1.5 Docker file Basic sample of simple docker file FROM debian LABEL app=&quot;Giro&quot; ENV VAR_1=&quot;sample variable&quot; RUN apt-get update &amp;&amp; apt-get install -y stress &amp;&amp; apt-get clean CMD = stress --cpu 1 --vm-bytes 64M --vm 1 To build docker image build`-t &lt;nome&gt;:&lt;versao&gt; . To run docker container run -d &lt;nome&gt;:&lt;versao&gt; 8.1.2 Part 2 8.2 Jenkins GitHub Repository from instructor 8.2.1 CI/CD Continuous Integration / Continuous Delivery - Practice to delivery the ready to deploy code CI : multiple integration per day CD : build allowed to deploying for customer anytime 8.2.2 Install Recommendation 1GB RAM 50GB Disk Java 8 Linux / Win / Mac To install install Jenkins Long Term Support on RedHat system Guide sudo wget -O /etc/yum.repos.d/jenkins.repo \\ https://pkg.jenkins.io/redhat-stable/jenkins.repo sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key sudo yum upgrade sudo yum install epel-release java-11-openjdk-devel sudo yum install jenkins sudo systemctl daemon-reload To start Jenkins sudo systemctl start jenkins sudo systemctl status jenkins We will need the password for initial configuration at /var/lib/jenkins/secrets/initialAdminPassword 8.2.3 Arquitecture Jenkins follow the master/slave architecture Master Node : controller, that controls the Load Distribution on Slave Node Used to schedule the build Job Dispatch Build Execution to Slave Node Monitor Slave and Record the Build Results Worker Node : Execution Node, we can have several slave nodes Execute Build dispatched by Master Not require Jenkins, only JAVA 8.2.4 Job Job is any runnable task that is controlled by Jenkins FreeStyle Project : Shell or Bash commands Pipeline : Workflow, usually written in DSL Multi Configuration Project : Will be tested on Multiple environments 8.2.5 Git &amp; GitHub 8.3 Git 8.3.1 Intro Some Concepts Commit : Saved changes to git repository Impacts history SHA1 hash for unique identifier Branches: Timeline with commits HEAD Pointer to last commit on branch Remote Related repository - but not local GitHub, GitLab, Bitbucket, etc Git Workflow Starting Local : Init the working directory Add files , etc Staging the changes using git add command Commit the changes using commit command Push the changes on remote repository Everyone can now pull the changes back Starting remotely: Create a new repository on Git remote, like Github Clone the repository to create it locally Do the work,add files Send the changes to staging area using git add command Commit the changes Send the data to remote repository using git push 8.3.2 Setup and Config Configure the name and email git config --global user.name &quot;&lt;Name&gt;&quot; git config --global user.email &quot;&lt;email&gt;&quot; To confirm the set : git config --global --list We also can check the config on ~/.gitconfig file 8.3.3 Working Locally Starting a fresh repository git init &lt;repo_name&gt; # Or inside a folder just git init git init see that inside the folder will be create a .git file After create some files can check the status of track files using status command git status To add a file in the staging area git add &lt;file&gt; # or add all files git add . Commit git commit -m &quot;&lt;message&gt;&quot; Add and commit at the same time, just one step git commit -am &quot;&lt;message&gt;&quot; To remove a file from the staging area, this will “unstage” the specified file from Git Staging area git reset HEAD &lt;file&gt; To back out a change in a file , this command will replace the file with the version last committed in Git git checkout -- &lt;file&gt; List all the commits done in the repo git log # or compat view git log --oneline --graph --decorate --color Back out changes already committed git rm &lt;file&gt; git commit -m &quot;&lt;message&gt;&quot; If remove the file using rm OS command need to do the below steps git add -u git status git commit -m &quot;&lt;msg&gt;&quot; git status Moving a file mkdir subdir git mv &lt;file&gt; &lt;subdir&gt; git status git commit -m &quot;&lt;msg&gt;&quot; if move the file without git need to use -u command on git add step Ignoring files using .gitignore file add on .gitignore # ignoring all .log files *.log 8.3.4 Working Remote Configure your .ssh key mkdir ~/.ssh cd ~/.ssh ssh-keygen -t rsa -C &quot;email&quot; &lt;enter&gt; This process will create two files , id_rsa and id_rsa.pub the content of .pub we will add into github account on SSH Keys setting. To test ssh -T git@github.com If we create a new repo from internet application we can use the below command to associate our local repo to remote repository git remote add origin git@github.com:&lt;user&gt;/&lt;repo_name&gt;.git To push the files from local to remote repository git push -u origin master This will send all files from local to remote, second time we will not be need to use -u paramter Now before perform a commit it is important to check if there are any modification by others git pull origin master Push the changes git push origin master The push sends all local changes on branch to the remote. The parameter -u is needed the fist time you push a branch to the remote. The pull receives all your remote changes fro the remote to local "],["data-science.html", "# 9 Data Science 9.1 DataCamp Python Skills for Data Science 9.2 DataCamp R Skills for Data Science 9.3 Azure Data Science Certification 9.4 AWS Data Science Certification 9.5 GCP - Professional Machine Learning Engineer 9.6 Kyndryl Data Science Roudmap", " # 9 Data Science 9.1 DataCamp Python Skills for Data Science 9.1.1 Introduction to Python 9.1.1.1 Python Basic Version 3.x - https://www.python.org/downloads We can save the script with .py and use python as calculator or usa ipython or python shell # Addition and subtraction print(5 + 5) print(5 - 5) # Multiplication and division print(3 * 5) print(10 / 2) # Exponentiation print(4 ** 2) # invest print(100 * 7.1) # Modulo print(18 % 7) # How much is your $100 worth after 7 years? print(100*1.1**7) Variable Specific, case-sensitive type(&lt;variable&gt;) to check the type of variable Types int - integer numbers float - real numbers bool - True , False str - string, text Lists [a,b,c, 1, True, 1.2 [a,b]] Collection of values, contain any type Slicing First element index 0 -1 last element Range [3:5] , last element not included [start : end(excluded)] Subsetting list of list [][] 9.2 DataCamp R Skills for Data Science 9.3 Azure Data Science Certification 9.4 AWS Data Science Certification 9.4.1 Demystifying AI / ML / DL What is AI ? Ability to scan and interpret the physical devices, for that we need to provide info of real world Knowledge (data) + Software programs = decisions Transfer human expertise to solve a specific problem (model) Machine learning and Deep learning are subset of AI ML : Data -&gt; processing -&gt; Predictions Machine learning can do : Make predictions Optimize utility functions Extract hidden data structures Classify data DL Enable the machine define the features itself, for instance, you show the machine several samples of rectangle and machine will be able to extract the features and recognize a probably rectangle. How to Establish an Effective AI Strategy Fast computing environments Data gathering from several sources, ubiquitous data Advanced learning algorithms The Flywheel of Data AI on AWS What is Machine Learning Subset of AI Process that takes data and use that to make predictinos and support decisions Types of Machine Learning Suggestion Intro to ML video 1: Complete sequence of videos here What is Deep Learning Deep Learning is a subset of Machine Learning Use many layers of non-linear processing units, for feature extraction and transformation Algorithms can be supervised and unsupervised Types of Neural Networks Feedforward Recurrent AWS Deep Learning Based Managed Services Amazon Lex : conversational engine Amazon Polly : lifelike speech Amazon Rekognition : Image analysis AWS Deep Learning AMI (custom models) AMI is pre-configured with : MXNet, TensorFlow, Microsoft Cognitive Engine, Caffe, Theano, Torch and Keras Support auto-scaling cluster of GPU for large training Suggestion Intro to DL video 1: Complete sequence of videos here 9.4.2 Machine Learning Essentials for Business and Technical Decision Makers What is Machine Learning(ML) ? : Process of training computers, using math and statistical processes, to find and recognize patterns in data. Iterative process How Amazon uses ML in products ? Browsing and purchasing data to provide recommendations Use voice interactions with Alexa using NLP Use ML to ship 1.6M packages per day How is machine learning helping AWS customers? Amazon Forecast Amazon Fraud Detector Amazon Personalize (product recommendation,direct marketing) Amazon Polly (TTS - text-to-speech) uses advanced deep learning technologies to synthesize natural-sounding human speech Amazon Transcribe (STT - speech-to-text) Amazon SageMaker Machine Learning on AWS How does machine learning work? What is AI ? : any system that is able to ingest human-level knowledge to automate and accelerate tasks performable by humans through natural intelligence. Narrow AI : where an AI imitates human intelligence in a single context (Today’s AI) General AI : where an AI learns and behaves with intelligence across multiple contexts (Future AI) What kind of solutions can ML provide? Regression : Prediction a numerical value , Zillow case Classification : Predicting label, duolingo case Ranking : Ordering items to find most relevant , Domino’s case Recommendation : Finding relevant items based on past behavior Hyatt Hotels Clustering : Finding patterns in examples NASA Anomaly detection : Finding outliers from examples, Fraud.net case’s What are some potential problems with machine learning ? Ingestion of poor quality data Explain complex models 9.4.3 Machine Learning for Business Leaders When is ML an option ? If the problem is persistent If the problem challenges progress or growth If the solution needs to scale If the problem requires personalization ir order to be solved What Does a successfull ML solution require ? People (Data Scientist, Data Engineer, ML Scientist, Software Engineers, etc) Time Cost Ask the right questions to team What are the made assumptions ? What is your learning target (hipotesis)? What type of ML problem is it ? Why did you choose this algorithm ? How will you evaluate the model performance ? How confident are you that you can generalize the results ? How to define and scope a ML Problem What is the specific business problem ? What is the current state solution ? What are the current pain points ? What is causing the pain points ? What is the problems impact ? How would the solution be used ? What is out of scope ? How do you define success (success criteria)? Input Gathering Do we have sufficient data ? Is there labeled examples ? If not , how difficult would it be to create/obtain ? What are our features ? What are going to be the most useful inputs ? Where is the data ? What is the data quality ? Output Definitions What business metric is defining success ? What are the trade-offs ? Are there existing baselines ? If not, what is the simplest solutions ? Is there any data validation need to green light the project ? How important is runtime and performance ? With those inputs and outputs we can formulate the problem as a Learning Task, is this a classification or regression problem ? What are the risks ? etc … When should you consider using machine learning to solve a problem ? Use ML when software logic is too difficult to code Use ML when the manual process is not cost effective Use ML when there is ample training data Use ML when the problems is formalizable as an ML Problem (reduce to well known ML problem regression, classification, cluster) When is Machine Learning NOT a Good Solution? No data No Labels Need to launch quickly No tolerance for mistakes When is Machine Learning is a Good Solution ? Difficult to directly code a solution Difficult to scale a code-based solution Personalized output Functions change over time 9.4.4 Process Model : CRISP-DM on the AWS Stack Into CRISP-DM “Cross Industry Standard Process - Data Mining,” excelent framework to build data science project There are 6 phases and the first (Business Understanding) one is the most important one, in that phase you going to understand the problem and know if this suitable for ML or not. Phase 1: BUSINESS UNDERSTANTING This phase there are 4 tasks : Understating business requiriment : Important to totally understand the customer needs and think on the questions from a business perspective that need to be answered (areas and business that need to improve) and convert that a problem that need to be solved or a problem that need to be answered, also high the critical features of projects (people, resources, etc) Analyzing support information : Collect information necessary based on the business question from task 1, make sure to list all the required resources and assumptions, analyze the risks, make a plan for contingencies and compare the costs and benefits for the project Converting to a Data Mining problem: Get the business question from task 2 and convert in machine learning objective (classification ? ; regression ?; clustering ? ) problem and define a criteria for successful Preparing a preliminary plan: That plan should describe the project and steps to achieve the goal: Timeline with number of stages and duration Dependencies Risks Business and Data Mining Goals Evaluation methods Tools and techniques necessary for each stage Phase 2: DATA UNDERSTANTING This phase there are basically 3 tasks : Data Collection : Need to analyze which data should be be used for the project, detail the sources and steps to extract data, having the data analyze for additional requirements (checking missing values, if data need to be encode or decode, if need to be normalized, check if are the specific fields that are more important to solve the problem ?) and consider other data sources (customer is an important resource because they know the domain knowledge). Data properties : Describe the data (Structured / Unstructured), amount of data used and metadata properties, including the complexity of data relationships and key features, also include the basics statistics (mean, median, etc), check the correlation of the main attributes, we can use python, sql, R and reporting tools using graphs to update the assumptions is necessary Quality : How many attributes contain errors ? , There are missing data ? Check the meaning of the attributes and complete the missing data, also check the inconsistencies and report all problems on this task and list the steps to solve this problem On AWS we can perform this task using Amazon Athena, Amazon QuickSight and AWS Glue Glue Manage ETL service Step 1 : Build data catalog Step 2 : Dev env to test and Generate and edit transformations Step 3 : Schedule and run your jobs Athena interact query service to run SQL queries on Amazon S3 Serveless where only pay for the queries Integrated with quicksight Support ANSI SQL operations and functions QuickSight Fast cloud powered BI service We can scale 1/10th of the cost of traditional BI solutions Secure and collaboration Phase 3 &amp; 4: DATA PREPARATION TASK &amp; MODELING Phase 3 consist in two tasks Final dataset selection : Here we should analyze the size, record selection and data types, also include and exclude columns based on data understand phase Preparing the data: Clean for quality Working on missing data : Dropping rows with missing values or adding a default value (mean, median) or work with imputation to add the missing data, we can also use statistical methods to calculate the value. It is also important to clean the corrupt data or variable noise Transforming for the best performance of model Derive additional attributes from the original (Datatime to hour, month, day …), use one-hot encoding to convert the strings , also recommend to normalize the data Merging all datasets in one final dataset Create the final dataset using joins and concatenations , recommend to revisit the Data Understanding phase to review the attributes Formatting to properly work on model Reformatting the data types and attributes (covert variables), randomly shuffle the data and remove unicode if necessary Phase 4 Modeling This phase work together with Data Preparation phase Modeling have 3 steps: Model selection and creation : Here we will select a model to address the ML problem (Regression for numeric problems and Random forest for Classification) Model testing plan : Before create the model we need to define how to test the model accuracy, split the data in Test and Training dataset (30/70), also there are other techniques, such as k-fold, for the model evaluation criterion we can use MSE, ROC, Confusion matrix, etc Model parameter tuning/testing : build the model , train the model and tweak the best performance (document the hiperparameters and reason), build multiple models with different parameters and report the findings Tools for Data Preparations and Modeling : Amazon EMR + Spark We can use EMR and the package Spark MLlib to create DataFrame based APIs for ML, using ipython notebooks, zepplin or R studio Support Scala, Python, R, Java and SQL Cost savings : Leverage spot instance for the task nodes Amazon EC2 + Deep Learning AMI The two main EC2 base ML environments are R studio and AWS Deep Learning AMI, this one preinstalled with GPU and frameworks ( MXNet, TensorFlow, Caffe2, Tourch, Keras, etc ) , also include Anaconda Data Science platform with popular libraries like numpy, scikit-learn, etc To install R studio in EC2 Phase 5: EVALUATION In this phase we have two main tasks : Evaluate how the model is performing related to business goals Dependens on : Accuracy of model or evaluation criteria on planning phase Converte the assessments to business need (monetary cost) Make a summary of results, ranking the models based on successfully criteria Make final decision to deploy or not Review the project and the assess the steps taken in each phase and perform quality assurance checks (is the data available for future training, model performance is using the determinated data) If the process fail to deploy due the successfully criteria, analise the business goals and try different approache or update the business goals and try again Phase 6: DEPLOYMENT Tasks : Planning deployment Runtime : Identity where it going to run (EC2, EC2 Container Service, AWS Lambda) Application deployment : AWS Code deploy (EC2), AWS OpsWorks (use chef), AWS Elastic Beanstalk (run the models on virtual servers) Maintenance and monitoring Infrastructure deployment : AWS CloudFormation, AWS OpsWorks, AWS Elastic Beanstalk Code Management : AWS CodeCommit, AWS CodePipeline (CI/CD) and AWS Elastic Beanstalk Monitoring: Amazon CloudWatch, AWS Cloud Trail and AWS Elastic Beanstalk Final report Document all steps and highlight processes used Goals met the project goals ? Detail the findings Identify and explain the model used and reason behind using the model Identify the customer groups to target using this model Project review Outcomes of the project : Summarize results and write thorough documentation and generalize the whole process to make it useful for the next iteration Task : create EC2 install packages and access from browser ssh &lt;connection&gt; -L localhost:8888:localhost:8888 Setup EC2 to run notebook Create EC2 instance Connect to instance via ssh Install python sudo yum update sudo yum install python Create a virtual environment and activate python3 -m venv basic source ~/basic/bin/activate Install basic database science packages pip install pandas numpy matplotlib seaborn scikit-learn statsmodels jupyter jupyterlab Configure the jupyter password jupyter notebook --generate-config jupyter notebook password Open a tunnel and Start jupyter notebook ssh -i &quot;&lt;key&gt;.pem&quot; ec2-user@&lt;ec2 machine&gt;m -f -N -L 8888:localhost:8888 jupyter notebook --no browser Access the notebook from browser http://localhost:8888/ 9.4.5 Machine Learning Terminology and Process End to End Machine Learning Process and common ML Terminoly ML Terminology Training : How ML use historical dataset to build prediction algorithm(model) Model : Core of ML process, enable the machine to determine an output variable(prediction) from an input variable Prediction (inference): Best estimate of a given input would be Process The Business Problem The Machine Learning framing (Transform the business problem into ML problem), define the type of ML Data Collection and Integration (Collect data from multiple sources) Data Preparation (steps before ML algorithm use the data) Data Cleaning Impute missing values (new variable indication the missing value, remove rows, imputation(mean, media, other)) Shuffle training data (stract a fraction of data for training) train_data = train_data.sample(frac = 1) Test-validation-train split (20% test , 10% validation, 70% train) Cross validation (Validation(30/70 or 20/10/70), Leave-one-out, k-fold) Data Visualization and Analysis (better understand of data) Statistics Scatter-plots Histograms Feature Engineering Binning : To introduce non-linearity into linear models Combine features together to create complex feature Take the log of feature or polinomial power of target Text-Features : Stop-words removal / Steamming Lowercasing, punctuation removal Cutting off very high/low percentiles TF-IDF normalization Web-page features multiple fields of text : URL, title, frames, body relative style and position Model training Loss Function (How far predictions are from objective) Square : regression, classification Hinge : classification only (robust to outliers) Logistic : Classification only (better for skewed class distribution) Regularization Prevent overfitting by constraining weights to be small Learning Parameters (decay rate) How fast the algorithm learn Decaying too aggressively - algorithm never reaches optimum Decaying too slowly - algorithm bounces around, never converge to optimum Model Evaluation Overfitting &amp; Underfitting Don’t fit data to obtain maximum accuracy Bias-Variance Tradeoff Bias : Difference between average model predictions and true target values Variance : Variation in predictions across different training data samples Evaluation Metrics Regression : RMSE - Root Meam Squared Error MAPE - Mean Absolute Percent Error R^2 - How much better is the model compared to just pick the best constrant (R^2 = 1 - (model MSE / variance)) Classification : Confusion Matrix ROC Curve Precision-Recall Precision : How correct we are when we what to predict be positive Recall (Sensitivity) : Fraction of negative that was wrongly predicted Business Goal Evaluation Evaluate how the model is performing related to business goals Make the final decision to deploy or not Evaluation depends on: Accuracy Model generalization on unseen/unknown data Business success criteria If we need more data or have more data we can add data (Data Augmentation) or feature (Feature Augmentation) Prediction : The production data MUST have the same distribution as the training data 9.4.6 Exploring Machine Learning Toolset INTRO TO AMAZON SAGEMAKER Amazon SageMaker is a fully managed service that enables data scientists and developers to quickly and easily build, train, and deploy machine learning models Components Notebooks : No setup required and we can install ML and DL frameworks, Spark and so on. Training Service : High on-demand trainnig enviroment can select the EC2 to run the experiment Hosting Service : Easy to deploy with auto-scaling API, A/B Testing and more SAGEMAKER NEO This is a new sageMaker capabilities helps developers take models train on any framework and run on any plataform Neo Components Compiler Container : read models in several formats (Tensorflow, pytorch, mxnet, xgboost) and convert to perform optimization Runtime library SAGEMAKER GROUND TRUTH Tool on SageMaker to label the dataset, auto label part of dataset and send the rest to human perform the task. Can setup end to end label job using Ground truth Ground Truth use active learn that identify the data that is well understood and can be labeled automatically and which data is not well understood and need to be revised by humans REKOGNITION Image and facial recognition service, deep learning based, no experience required. Key features Object and scene detection Facial analysis Face comparison Face recognition Confidence Score and Processed Images DEEPLENS DeepLens is wireless-enabled camera and development plataform integrated with AWS Cloud DeepLens Workflow Step 1 : When turned on, the AWS DeepLens captures a video stream. Step 2 : Your AWS DeepLens produces two output streams: * Device stream – The video stream passed through without processing. * Project stream – The results of the model’s processing video frames Step 3 : The Inference Lambda function receives unprocessed video frames. Step 4 : The Inference Lambda function passes the unprocessed frames to the project’s deep learning model, where they are processed. Step 5 : The Inference Lambda function receives the processed frames from the model and passes the processed frames on in the project stream Frameworks (MXNet, Tensorflow and Caffe) POLLY Text to Speech service help you application to talk and increase accessibility, with independent solution and high quality voices, supporting 24 languages Polly is compliant with SSML (Speech Synthesis Markup Language), XML based starts with &lt;speech&gt; ... &lt;/speech&gt; LEX Service to build conversation interfaces between application using voice and text, same tecnology of Alexa Lex works with Amazon CloudWatch to monitoring the number of requests, latency and errors Chatbots do Amazon Lex User calls customer service line to reschedule an appointment Amazon connect calls Lex and AWS Lambda calls a database Once customer ask to reschedule, Lambda calls schedule software Once reschedule is confirmed Lambada send a text message to customer TRANSCRIBE Convert audio to text (Speech-To-Text) Amazon Transcribe is ASR (Automatic Speech Recognition) service designed to Speech-To-Text applications TRANSLATE Neural machine translation service (batch, real-time, and on-demand translations) Amazon Translate Use Cases : Translating Web-authored content in real time and on demand Batch translating pre-existing content for analysis and insights COMPREHEND 5 Main capabilities Sentiment : understand what user say (pos, neg, neutral) Entities : extract and categorize entities from unstructured text Languages : detect the language Key phrases : know phrases Topic modeling : help organize the text in topics Social Abalytics : COMPREHEND MEDICAL ML APIs specific to healthcare domain, an extend to Comprehend APIs : NERe: Json with all extract entities and relationship PHId: Protect health information on the text FORECAST Science of predicting future points in a time series based on historical data Accuracy is the most important factor in forecast Amazon Forecast is a fully managed accuracy forecast solution that uses deep learning models from over 10 years of ML experience ELASTIC INFERENCE (EI) Amazon EI Accelerator sizes : MARKETPLACE INTRODUCES ML CATEGORY 9.4.7 The Elements of Data Science INTRO TO DATA SCIENCE What is Data Science ? processes and systems to extract knowledge or insights from data (structured or unstructured) What is Machine Learning ? set of algorithms used to improve predictions by learning from large amounts of input data Learning : estimating function f by mapping data attribtes to some target value Training set : labeled examples (x, f(x)) Goal : find the best approximation f_hat that best generalizes Types: Supervised Learning : Models learn from training data that has been labeled. Unsupervised learning : Models learn from test data that has not been labeled. Semi-supervises learning (mix of label and un-label data) Reinforcement learning : Models learn by taking actions that can earn rewards. Key Issues in ML Data Quality Consistency of the data Accuracy of the data Noisy data Missing data Outliers Bias Variance Model Quality Overfitting : failure to generalize, model memorize the noise, high variance (small change in the training data lead to big changes in the results) Underfitting : Failure to capture important patterns, too simple, high bias (the results show systematic lack of fit in certain regions) Linear methods Optimized by learning weights by applying (stochastic) gradient descent to minimize loss function Methods (Linear Regression and Logistic Regression) 9.4.8 Data Engineering S3 Buckets must have a global unique name Objects (file) have a key. The key is the FULL path : &lt;my_bucket&gt;/my_folder/my_file.txt Max 5TB Backbone for ML services Perfect use case for Data Lake, with infinite size , 99.999999999% durability across multiple AZ and 99.99% availability (not available 53 min a year) Obejct storage supports any file format (CSV, JSON, Parquet, ORC, Avro, Protobuf) We can partition the data by date, by product or any strategy we would like, some tools perform this task forus (Glue and Kinises) Amazon S3 Storage Classes: Classes Details Use Case S3 Standard - General purpose * 99.99% availability (53min a year not available)* Used for frequently accessed data* Low latency and high throughput* Sustain 2 concurrent failures Big data analytics, mobile and gaming applications S3 Standard-Infrequent Access IA * For data that is less frequently accessed, but requires rapid access when needed* Low cost than S3 standard, cost on retrieval* 99.9% availability Used for Disaster recovery S3 One Zone-Infrequent access * High durability 99.999999999% in a single AZ, data lost when AZ distroied* 99.5% availability Storing secondary backup copies of on-prem data, or data you can recriate S3 Glacier Instant RetrievalLow cost for archive/backup Instant retrieval : ms retrieval , min storage duration 90 daysFlexible Retrieval : Expedite 1 to 5min , Standard 3 to 5 hours, min duration 90 daysDeep Archive : Standard 12hrs, bulk 48hrs, min duration 180 days, for long archive S3 Intelligent Tiering * Small monthly monitoring and auto-tiering fee* Move objects automatically between Tiers based on usage* No retrieval charge* Frequent Access : default* Infrequent Access &gt; 30 days* Archive Instant Access &gt; 90 days* Archive Access 90 to 700+ days* Deep Archive Access 180 to 700+ days We can move files between storage classes manually or via configuration using Lifecycle Rules Security : Encryption for objects SSE-S3 : encrypt using keys managed by AWS SSE-KMS : use Key Management Service (Customer Master Key) SSE-C : when we want to manage the keys Client Side Encryption On ML , SSE-S3 and SSE-KMS will be most likely be used SS3 means Service-side-encryption S3 Bucket policies : We can use the policies to grant access (including Cross Account) to bucket or force objects to be encrypted on upload Today we can use the default encryption option on S3 and every document sent to bucket will be encrypted by default AWS Kinesis Kinesis is a managed alternative to Apache Kafka, it is used to real-time streaming process of big data, used for application logs, metrics, IoT, clickstreams and data replicated on 3 AZs Services : Kinesis Data Streams : low latency streaming ingest at scale Stream are divided into Shards/Partitions and by default data retention is 24hrs, multiple appls can use the same stream and once data is inserted it cannot be deleted (immutability) It is for real-time Kinesis Data Analytics: real-time analytics on streams using SQL Data Analytics will take data from Firehose or Data Streams, perform modifications using SQL and send it to analytic tools Used to streaming ETL, continues metric and reponsive analytics (filtering) Machine Learning on Kinesis Data Analytics (two algorithms) RANDOM_CUT_FOREST (Used for anomaly detection on numeric columns, use recent history to compute model) HOTSPOTS (locate and return information about dense regions) Kinesis Firehose: load stream into S3, Redshift, ElasticSearch and splunk To store data in two target destination, it reads data up to 1MB, can be transformed by lambada function and write in batches into S3, RedShift, ElasticSearch, custom destionation or 3rd party (splunk, mongo, etc) It is near real-time to ingest massive data, auto-scale, supporting many formats (csv, json, orc) Kinesis video Stream: stream video in real-time real-time video stream to create ML applications GLUE DATA CATALOG GLUE Documentation Metadata repository for all tables Automated schema inference Schema visioned Integration with Athena or RedShift (schema &amp; data discovery) Glue Crawlers can help build the Data Catalog GLUE DATA CRAWLERS Go through the data to infer schema and partitions, works in JSON,CSV and PARQUET Will extract partition based on how S3 is organized GLUE ETL Transform data, clean, modify (Join, filter, dropfields, map), generate code in python or spark and the target can be S3, JDBC, RDS, RedShift or Glue Catalog ML Transformation : FindMatches ML identify duplicated or matching records in database Jobs run on Spark Platform Formats (csv, json, avro, parquet, orc and xml) Also can use any apache spark transformatino (like k-means) DATA STORE IN MACHINE LEARNING RedShift : Data warehouse, OLAP processing RDS, Aurora : Relation store OLTP DynamoDB : NoSQL data store S3: Object store, serveless OpenSearch (previously Elastic Search) : Indexing data ElastiCache : Caching mechanism AWS DATA PIPELINE Service to move data from one place to another (S3, RDS, DynamoDB,Redshift, EMR), ETL service where we can manage the task dependencies, retry and notifies on failure What is the difference between GLUE and DATA PIPELINE ? Glue is Apache Spark focus , run Scala or Python jobs Data Pipeline is an orchestration service where we have more control over the environment, compute resources and code and allow us access EC2 or EMR AWS BATCH AWS Batch run jobs as Docker images, no need to manage cluster, fully serveless and we can schedule batch jobs using Cloud Watch Events or Orchestrate batch jobs using AWS Step Functions DMS DATABASE MIGRATION SERVICE Quickly and securely way to migrate databases to AWS, it supports Oracle to Oracle or MSSQL to Aurora, we can use continuous Data Replication using CDC and it the replication must be performed EC2 instance AWS Step Functions Step Functions is used to Orchestrate and design workflows Train a Machine Learning Model 9.4.9 Exploratory Data Analysis PANDAS Data Frames : Similar table structure Series : 1D structure Numpy : arrays and math 100-numpy-exercises 100-numpy-exercises - solutions MATPLOTLIB Data Visualization Boxplot Histogram Seaborn : Python data visualization library based on matplotlib Visualizing statistical relationships Visualizing distributions of data Plotting with categorical data Visualizing regression models Scikit_learn Jupyter notebooks Type of Data Numerical (discrete 5 , 20 or continuous 2.56, 545.67) Categorical (qualitative Gender) Ordinal (Categorical with math meaning Ranking) Data Distribution Normal Probability Mass Function Working with Discrete data, visualize the probability of discrete data occur Poisson Distribution Example of probability mass function, series of events (success or failure) Binomial Distribution Work with discrete data Time Series Trends Seasonality Seasonality + Trends + Noise = Time series Amazon Athena Serveless interactive queries of S2 data lake Presto under the hood Serverless Supports (CSV, JSON, ORC, PARQUET, AVRO) Pay-as-you-go Save money using columnar formats (ORC, Parquet) Amazon QuickSight Business analytics and visualizations in the cloud Build visualizations Perform ad-hoc analysis Serveless Data Sources : RedShift, Aurora / RDS, EC2, Athena, S3 SPICE : In-memory calculation makes QuickSight fast ML Insights : Anomaly detection, Forecasting, Auto-narratives Amazon EMR Elastic MapReduce Managed Hadoop framework on EC2 Includes Spark , HBase, Presto, Flink, hive and more EMR Notebooks Spark Components that runs on top of spark core: Spark Streaming Spark SQL Graph X MLLib Classification : Logistic regression, naive bayes Regression Decision trees Recommendation ALS Cluster K-means LDA (Topic modeling) ML Workflow utilities (pipeline, feature transformation, etc) PCA, SVD, statistics, others Feature Engineering “Applied machine learning is basically feature engineering” - Andrew Ng Which features should I use ? Do I need to transform these feature ? How do I handle missing data ? Should I create new feature ? Transform ? Normalize ? Imputing Missing Data Replace by mean ? median ? Works on column level Cannot use on categorical features If not many rows and drop does not bias the data, maybe reasonable Use Machine Learning KNN , average of group of features Deep Learning, build ML to impute the data, works well for categorical data Regression (MICE) Get more data Unbalanced Data Large discrepancy between positive and negative cases Oversampling : Duplicate samples from the minority class Undersampling : Instead of creating more positive samples, remove negative ones, remove data is not the right answer SMOTE : Synthetic Minority Over-sampling TEchnique generate new samples using nearest neighbors Outliers We can use Stardard deviation to identify outliers AWS Random Cut Forest : outlier detection Binning Bucket observations together based on ranges of values Transform numeric data to ordinal data Encoding Transform data into some new representation One-Hot encoding Scalling / Normalization Some models prefer feature data to be normally distributed Scikit learn MinMaxScaler Amazon SageMaker Ground Truth and Label Generation Ground Truth creates its own model as images are labeled by people 9.4.10 Modeling Part 1 DEEP LEARNING Frameworks Tensorflow / Keras MXNet model = Sequential() model.add(Dense(64, activation=&#39;relu&#39;, input_dim=20)) model.add(Dropout(0.5)) model.add(Dense(64, activation=&#39;relu&#39;)) model.add(Dropout(0.5)) model.add(Dense(10, activation=&#39;softmax&#39;)) sgd = SGD(lr=0.01,decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss=&#39;Categorical_crossentropy&#39;, optimizer=sgd, metrics=[&#39;accuracy&#39;]) Types os Neural Network Feedforward Convolutional (CNN) : Image Recurrent (RNN) : deal with sequence in time (stop price, words sequence, translation, etc) - LSTM, GRU Activation Functions It is a function inside the node Types : Linear : do anything Binary step function : on or off Sigmoid / Logistic / TanH : Computational expensive and Vanishing Gradient problem ReLU (Rectified Linear Unit) : Easy and fast, zero or negative problem with Dying ReLU Leaky ReLU : Solve dying ReLU Parametric ReLU (PReLU) : complicated Exponential Linear Unit (ELU) Swish : for really deep neural network , developed by google Maxout : not practical double the params Softmax : used on final layer of multiple classification problem Choosing an activation function For multi clas : Softmax RNNs : TanH Others: Starts with ReLU, if need do better, Leaky ReLU, PReLU, Maxout, Swish CNNs Data that does not neatly align into columns (images, translation, sentence classification, sentiment analysis) CNN with Keras / TF Source must be : width x length x color Conv2D, Conv1D and Conv3D MaxPooling2D used to reduce the 2D layer Flatten convert 2D layer to 1D layer Typical architecture : Conv2D -&gt; MaxPooling2D -&gt; Dropout -&gt; Flatten -&gt; Dense -&gt; Dropout -&gt; Softmax Specialized CNN architectures LeNet-5 : Good for handwriting recognition AlexNet : Image classification GoogLeNet : Deepr introduce the inception modules (groups of convolution layers) ResNet (Residual Network) : Even deepr RNNs Time-series data Machine translation, image captions, machine-generated music RNN Topologies Sequence to sequence : predict stock price Sequence to vector : wordsin a sentence to sentiment Vector to sequence : create captions from an image Encoder -&gt; Decoder ( sequence -&gt; vector -&gt; sequence ) : machine translation Architectures : RNN vs GRU vs LSTM LSTM (Long Short-Term memory Cell) GRU (Gated Recurrent Unit) Deep Learning on EC2 / EMR EMR supports Apache MXNet and GPU types Types : P3 : 8 Tesla V100 GPUs P2 : 16 K80 GPUs G3 : 4 M60 GPU Deep Learning AMI’s Tuning Neural Networks (IMPORTANT TOPIC ON EXAM) Learning Rate : How far apart these samples are ? Large learning rates can overshoot the correct solution Small learning rates increase the training time Batch Size : How many trainig samples are used within each batch of each epoch Smaller batch tend get stuck in local minima Large batch size can end up getting stuck and onverge on wrong solution Regularization Techniques to prevent overfitting, ie, high accuracy on training data, but lower on test or evaluation data Dropout : Removing neurons at random , prevent specific neuron overfitting Early Stopping : Stop the training after some epochs L1 and L2 L1 : Sum of weights, “feature selection” reduce dimensionality L2 : Sum of square of weights Gradients Vanishing Gradient Problem : when the slope of the learning curve approaches zero Fix : Use LSTM Residual Networks Better choice of activation function ReLU Confusion Matrix, Precision , Recall, F1, AUC Recall (Sensitivity) : Percent of positives rightly predicted, good for fraud detection Precision : Percent of relevant results F1 Score : Harmonic mean of precision and sensitivity RMSE : Root mean squared error, accuracy measurement ROC : Recall vs false positive rate AUC : Are under the curve, used to comparing classifiers 9.5 GCP - Professional Machine Learning Engineer 9.5.1 Big Data and ML Fundamentals Compute power : We can easy create a server, execute the job, pause or delete the server Storage : To create a storage bucket from UI is very simple by command line we can gsutil mb -p [PROJECT_NAME] -c [STORAGE_CLASS] -l [BUCKET_LOCATTION] gs://[BUCKET_NAME]/ Types of Storage Networking Google data centers are interconnected with network speed 1 Petabit/sec Any machine communicate with any other in the data center at over 10 gigabytes for sec Security : Base that covers all google applications Communication to GCP are encrypted in transit Stored data are encrypted BigQuery data are encrypted On top of security , network, storage and computer power google have a top layer Big Data and ML Products GCP resource hierarchy GCP Offers Most popular : Compute Engine : GCP IaaS lets you run VM on-demand on cloud Kubernetes engine (GKE): Clusters of machines running containers, this tools orchestrate the containers to enable the appls running on containers work properly App Engine : GCP fully managed PaaS , you create the code and google deal with all resources and infra Cloud Functions : Serveless execution environment, execute your code in response events Complete list of GCP offers : Key roles in a data-driven organization 9.5.2 Recommending Products using Cloud SQL and Spark Recommending Products : Model learns what you like, and dont like, what you buy and dont buy, and then starts suggest similar products Recommendation systems require data, a model and training/serving infrastructure How recommendations works on GCP (sample of housing rentals recommendation) Step 1 : Ingest the ratings fo all the houses Step 2 : Traing a ML model to predict a user rating of every house on database Step 3 : Pick the top five rated houses and present to user How often and where will you compute the predicted rating ? Week? Day ? (Batch) Where store the ratings ? Cloud SQL is an option Where to store the data in GCP ? Cloud SQL : Google managed RDBMS, supports MySQL and PostgreSQL Advantages Familiar Flexible price Managed backups Connect from anywhere Automatic replication Fast connection from GCE &amp; GAE Google security Cloud Dataproc Autoscaling provides flexible capability and you can store the data on Cloud Store (HDFS), Bigtable (HBase) or Big Query Can use Preemptible VMs : suitable for fault tolerant Hadoop without cluster management Lift-and-shift existing hadoop workload Connect with Cloud Storage to separate compute and storage Re-size clusters effortlessly. Preemptible VMs for cost saving 9.5.3 Predict Visitor Purchases Using BigQuery ML Intro to BigQuery is a petabyte-scale fully-managed data warehouse Serverless Flexible pricing model (pay as you go) Data encryption and security Geospatial data types &amp; functions Foundation for BI and AI 9.6 Kyndryl Data Science Roudmap 9.6.1 Data Science- Project Management Methodology - CRISP-DM 9.6.1.1 KDD Select Interpret the data Select data relevant to analysis Preprocessing Outliers Missing Values Transform Useful features Smoothing (- binning - cluster) Aggregation (- Weekly - month) Normalization Data Mining Explore Graph Predict Models Evaluating Check Evaluate the results Analysis 9.6.1.2 SEMMA Sample : Subset of data (train, test validation) Explore: Understand the data M:odify: Clean, feature engineering Model: data mining, modeling Assess: Model performance 9.6.1.3 CRISP-DM Business Understand Data Understand Data Preparation Modeling Evaluation Deploy 9.6.1.3.1 1. Business Understand initial plan Steps: Define Business Problem : Define the objective, the analitical problem, the expectations, success criteria, pain points Assess and Analyze Scenarios Define Data Mining Problem Project plan : Deliverable (timeline, costs, success criteria, assumptions, constraints, etc) 9.6.1.3.2 2. Data Understand Data Collection : Primary data source (survery, experiments) or secondary data source (ERP, CRM, database) Data Preparation / Description Quantitative (count, continuous ) vs Qualitative (categorical) Balance vs Imbalance (one class less than 30% = Imbalance) Structure (tabular) vs Unstructured(video, img, audio, text) vs Semi-structure Exploration - Data Analysis Inferencial stats Sampling - Balacing vs Imbalancing Balancing : random sampling, sampling Imbalancing: stratified sampling, K-fold, smote, msmote, leve-one-out Descriptive stats Meam , media, mode variance, std, range skewness kurtoses Graphical Univariant Boxplot - Outliers, shape of distribution Histogram - Shape, outliers QQ Plot check train and test dataset if they are in the same distribution Bivariant Scatter : correlation, coeficient (+1, -1) , strong (r &gt; 0.85 ) weak (r &lt; 0.4), cluster, linear Data Quality Analysis Idenfity outliers, missing values Levels of granularity Inconsistence Wrong data errors Meta info 9.6.1.3.3 3. Data Preparation In this step we clean, curate, wrangle and prepare the data Outliers : 3R Techniques (Rectify, Remove, Retain) Missing Data: Imputation (mean, median, mode, regression, knn, etc) Data Transform : Log, exp, boxcox, etc, done when data are non-normal Data Normalization / Standartization Normalization (mean = 0 , std =1 ) Standardization (min = 0 , max = 1) - MinMaxScaller Discretization, Binning, Grouping Dummy variable - OneHotEncoding Apply domain knowledge to generate more features 9.6.1.3.4 4. Modeling Select model techniques Model building Model evaluation and tuning Model Assessment Supervised Learning Predict Y based on X Categorical (2 class or multiclass) numerical - Prediction User preference - Recommendation Relevance - Retrival Regression Analysis y = continuous : Linear Regression y = discrete (2 categories) : Logistic Regression y = discrete (&gt; 2 categories) : Multinominal / Ordinal Regression y = Count : Poisson / Negative Binominal REgression (var &gt; mean) Excessive Zero : ZIP (Zero Inflated Position) ZINB (Zero Inflated Negative Binomial) Hurdle KNN Naive Bayes Black Box Neural Network Support Vector Machine Ensemble Stacking : Multi Techniques (Linear + DT + KNN) mean or majority Bagging : Randon Forest - good for discrete Boosting: Decistion tree, Gradient boosting, XGB, AdaBoost Unsupervised Learning Cluster / Segmentation - reduce Row Kmeans - non hierarchical - elbow curve Hierarchical - agglomerative - deprogram DBSCAN - application with noise OPTICS - ordering points to identify cluster structure CLARA - cluster large application - for large datasets K-medians / K-medoids (for lot of outlines) / K-modes (lot of categorical variables) Dimension Reduction - reduce columns PCA SVD Association Rules / Market Basket Analysis / Affinity Analysis Support Confidence EFT Ration &gt; 1 Recommended system Network Analysis Degree Page rank others Test Mining / NLP Bow TDW / DTW TF / TDIDF Forecasting / Time Series Model Based Approaches Trend: Linear, Exponential , Quadratic Seasonality : additive or multiplicative Data Base Approaches AR - Auto regressive MA - Movie average ES - Exponential smoothing SES HOHS / Double Exponential Smoothing Winters, others Overtiffing (variance) vs Underfitting (Bias) Reinforcement Learning (learning from rewards) Semi-supervised learning Active learning, transfer learning, structure prediction 9.6.1.3.5 5. Evaluation There are no better type of evaluate need to analyze the problem and data / results to select the best metric Mean Error Mean Absolute deviation Mean Squared Error Root Mean Squared Error Mean Percentage Error Mean Absolute percentage error For Categorical we also have the Confustion Matrix TP : Correct Predictive Positive TN : Correct Predictive Negative FP : Incorrect Predictive Positive FN : Incorrect Predict Negative Precision : Prob of correctly identify a random patient with disease have a disease. (Positive Correct predicted) Sensitive (Recall or Hit Rate): Proportion of people with disease who are correctly identified as having disease Specificity (True Negative Rate) : Proportion of people with NO disease being characterized as not have disease FP Rate (Type 1 error) : 1 - Specificity FN Rate (Type 2 error) : 1 - Sensitivity F1 : 1 to 0 Measure that balance precision and recall ROC AUC : Are under the curve 0.9 - 1.0 : outstanding 0.8 - 0.9 : good 0.7 - 0.8 : acceptable 0.6 - 0.7 : poor 0.5 - 0.5 : no discrimination Model Assessment Model performance and success criteria agreed upon early are in sync Model should be repeatable and reproducible Model is in line with Non-functional requirements, such as scale, robust, maintainable, easy to deploy Model evaluation gives satisfactory results Model is meeting business requirements Rank final models based on the quality of results and relevance Any assumptions or constants that were invalidated by the model ? Cost of deploy the entire pipeline Any pain points Data Sufficiency report Final suggestions, feedback Monitoring : PEST or SWOT 9.6.1.3.6 6. Deploy DEV to PROD Proper resources - Hardware, server, software , human model saved and then deployed Maintenance and monitoring (PEST) 9.6.2 Statistics for Data Analysis Using Python 9.6.2.1 Descriptive Statistics Central Tendency Mean : Average Mode : Most occuring number Median : Moddle value when arranged in asc or desc order Dispersion Range : highest - lowest value Standard Deviation : squared root of variance Variance Inter Quartile Range IQR : If divide the data into four parts (Q1, Q2 and Q3) Quantiles, if we divide the data into n parts, we get (n-1) points of split called quantiles 9.6.2.2 Distributions BINOMIAL The experiment consist of n repeated trials Each trial can result in just two possible outcomes(success and failure) The probability of success, denoted by p, is the same on every trial The trials are independent, that is, the outcome on one trial does not affect the outcome of other trials In Python from scypy.stats import binom binom.cdf(k , n , p) # cumulative distibution function - for less than or equal to 2 binom.pmf(k , n , p) # Probability mass function - for specific number of, defects binom.sf(k , n , p) # for more than 2 (similar 1 - cdf) binom.mean(n, p) # for mean of the dist binom.std(n, p) # for standard deviation of the dist binom.var(n, p) # for the variance of the dist POISSON The possibilities of success are infinite (Number of people in a queue, Number of accident in a city) are sample of this distribution Measure the number of success similar to binomial As binomial are for discrete distribution Properties : The experiment results in a success or failure The mean of success occurs in a specific region is known Outcomes are random The outcomes of interest are rare relative to the possible outcomes The variance is equal to mean In Python from scypy.stats import binom poisson.cdf(k , mu) # cumulative distribution function - for less than or equal to poisson.pmf(k , mu) # probability mass function - for exact value poisson.sf(k , mu) # for more than (similar 1 - cdf) poisson.mean(mu) # for mean of the distr poisson.var(mu) # for variance of the distr poisson.std(mu) # for standard deviation of the distr NORMAL Most common distribution for continuous data Properties : Normal distribution is symmetrically Long Tails / Bell shaped Mean, mode and median are the same 68% of area under the curve falls with 1 std of the mean 95% of area under the curve falls with 2 std of the mean 99.7% of area under the curve fall with 3 std of the mean The total area under the normal curve is equal to 1 The probability of any particular value is 0 The probability that X is greater than or less than a value = area norm.cdf(x,mu,sigma) # Cumulative distribution function - for less than or equal to norm.pdf(x,mu,sigma) # Probability density function (not Probability mass function) - for exact value norm.sf(x,mu,sigma) # For more than (similar to 1-cdf) norm.mean(mu) # For mean of the distribution norm.var(mu) # For variance of the distribution norm.std(mu) # For standard deviation of the distribution 9.6.2.3 Inferencial and Hypothesis Testing Inferencial Stats We infer about the population based on sample data Central Limit Theorem For almost all porpulations, the sampling distribution of the mean can be approximated closely by a normal distribution, provided the sample size sufficiently large If a variable has a mens of µ and the variance \\(σ^{2}\\), as the sample size increase, the sample mean approaches a normal distribution with mean µ\\(\\overline{x}\\) and variance σ\\(\\frac{2}{x}\\) Hypothesis Testing Hypothesis testing is a method of statistical inference Commonly used tests include Comapre sample statistics with the population parameter Compare two datasets Steps for Hypothesis Testing Taking a sample and based on that sample we are predictin about the population State the alternative hypothesis State the null hypothesis Select a probability of error level (alpha). generally 0.05 Calculate the test statistics(e.g t or z score) z = (x-μ)/σ (Basic one sample) z = (x – μ) / (σ / √n) (multiple samples) Critical test statistic Use the \\(\\alpha\\) and check on Test Table Interpret the results Null Hypothesis : Basic assumption, for example : The person is innocent Alternate Hypothesis : You need to provide proof of this, for example : The person is guilty In Statistical terms you: Reject the Null Hypothesis, or Fail to reject the Null Hypothesis (not accept the Null Hypothesis) Type I Error : False Alarm Type II Error : Something change and we fail to detect the change Confidence level : C = 0.90, 0.95, 0.99 (90%, 95%, 99%) Level of Significance or Type I Error : \\(\\alpha\\) = 1 - C(0.10, 0.05, 0.01) Power Power : 1 - \\(\\beta\\) (or 1 - type II error) Type II Error : Fail to reject null hypothesis when null hypothesis is false Likelihood of rejecting null hypothesis when null hypothesis is false Or : Power is the ability of a test to correctly reject the null hypothesis P-value p-value is the lowest value of alpha for which the null hypothesis can be rejected. (Probability that the null hypothesis is correct) For example, if p = 0.045 you can reject the null hypothesis at \\(\\alpha\\) = 0.05 p is low the null must go (null get rejected), if p is high the null fly (null stay) Proportions &amp; Variances Conditions for z Test Random samples Each observation should be independent of other Sample with replacement, or If sample without replacement, the sample size should not be more than 10% of population Sampling distribution approximates Normal Distribution Population is Normally distributed and the population standard deviation is known , or Sample size &gt;= 30 One Sample One Sample z Test : Used when we have one sample from one machine Conditions for z test: Random Samples Each observation should be independent of each other (sample with replacement) or (if sample without replacement sample size should not be more than 10% or population) Sample distribution approximates Normal Distribution (Population is Normally distributed and the population std dev is known or size &gt;= 30) One Sample t Test : When we have less than 30 numbers of sample and we do not know the population standard deviation Conditions for t test: Random samples Each observation should be independent of each other (sample with replacement) or (if sample without replacement sample size should not be more than 10% or population) Sample distribution approximates Normal Distribution (Population is Normally distributed and the population std dev is unknown or size &lt; 30) One Proportion Test : Compare proportions Conditions for One Proportion test Random samples Each observation should be independent of each other (sample with replacement) or (if sample without replacement sample size should not be more than 10% or population) The data contains only two categories, such as pass / fail or yes / no For Normal Approximation (both np &gt;= 10 and n(n-p) &gt;= 10 - data should have at least 10 “successes” and at least 10 “failures”) One Variance Test : Check if variance has changed Conditions for One Variance test Random samples Each observation should be independent of each other (sample with replacement) or (if sample without replacement sample size should not be more than 10% or population) The data follows a Normal Distribution Variance Tests Chi-square Test For testing the population variance against a specified value Testing goodness of fit of some probability distribution Testing for independence of two attributes (Contingency Tables) F-test for testing equality of two variances from different population for testing equality of several means with technique of ANOVA Two Samples Two Sample z Test : Compare the sample (mean) from two machines Conditions for z test: Random Samples Each observation should be independent of each other (sample with replacement) or (if sample without replacement sample size should not be more than 10% or population) Sample distribution approximates Normal Distribution (Population is Normally distributed and the population std dev is known or size &gt;= 30) Sample of Z test hypothesis for two sample: Null Hypothesis : μ1 = μ2 Alternative hypothesis : μ1 != μ2 R sample Python sample Two Sample t test Conditions for t test: Random Samples Each observation should be independent of each other (sample with replacement) or (if sample without replacement sample size should not be more than 10% or population) Sample distribution approximates Normal Distribution (Population is Normally distributed and the population std dev is unknown or size &lt; 30) How to calculate ? Variance equal Since we have a small size of sample we going to use t test independent stats.ttest_ind() function import scipy.stats as stats machine1 = [150,152,154,152,151] machine2 = [156,155,158,155,154] stats.ttest_ind(machine1, machine2, equal_var=True) #Output # Statistics = -4.0055 # pvalue = 0.0039 Result Since the value of pvalue is less than 0.05 we will reject the Null Hypotheses H0 since there is no significant difference in the variance of two machines Variance unequal Since we have a small size of sample we going to use t test independent stats.ttest_ind() function import scipy.stats as stats machine1 = [150,152,154,152,151] machine3 = [144,162,177,150,140] stats.ttest_ind(machine1, machine3, equal_var=False) #Output # Statistics = 0.4146 # pvalue = 0.6992 Result Since pvalue is high than 0.05 we will fail reject the Null Hypotheses H0 since there is significant difference in the variance of two machines Paired t test : Compare when you have before and after results If the value in one sample affect the value in the other sample, then the samples are dependent : (Ex: Blood pressure before and after specific medicine) How to calculate ? Find the difference between two set of readings as d1, d2..dn Find the mean and std dev of these differences Using Python we can use the package scipy.stats and ttest_rel function import scipy.stats as stats before = [120,122,143,100,109] after = [122,120,141,109,109] stats.ttest_rel(before, after) # output # statistics = -0.068 # pvalue = 0.530 Results: Since pvalue is high to 0.05 we fail to reject the H0 (null hypothesis), which means there are no significant difference between the values before and after Two Proportions Test : Compare the proportions from two samples Conditions for Proportions test Random Samples Each observation should be independent of each other (sample with replacement) or (if sample without replacement sample size should not be more than 10% or population) The data contains only two categories, such as pass/fail or yes/no For Normal approximation : both np &gt;= 10 and np(1-p) &gt;= 10 : Data should have at least 10 successes and at least 10 failures for each sample (some books it is 5) Methods to calculate Pooled : H0 : p1 = p2 and Ha p1 != p2 Un-pooled : H0 p1 - p2 = d(difference) and Ha p1 - p2 != d(difference) How to calculate ? # H0 = p = p0 # Ha = p != p0 # From vendor A we test 200 pieces and find 30 defects # From vendor B we test 100 pieces and find 10 defects # Is there a significant difference in quality of those 2 vendors? (95% confidence level) from statsmodels.stats.proportion import proportion proportion.test_proportions_2indep(30,200, 10, 100, method=&#39;score&#39;) #output # Statistics = 1.198 # pvalue = 0.230 Results: Since the pvalue is higher than 0.05 we fail to reject the null hypotheses , we cannot say there is any significant difference in the proportion of this two samples Two Variances : Compare the variances from two samples Conditions and test used for two variance test: F-test for testing equality of two variances from different population for testing equality of several means with technique of ANOVA How to calculate ? * 8 samples from machine A : STDEV 1.1 * 5 samples from machine B : STDEV 11 * Is there a difference in variance at (90% confidence level) ? from scipy.stats import f # find f calculated F_cal = 11/ (1.1**2) # output 9.09 # find critical values on right dfn = n - 1 f.isf(0.05, dfn = 4, dfd = 7) # output : 4.12 # find critical value on left f.isf(0.95,4,7) # output 0.16 Results: Since the F_calc(9.09) is in the reject zone higher than right value (4.12), we reject the null hypotheses, there is a significant difference between the machines We also can use stats.bartlett(machine1, machine2) or stats.levene(machine1 , machine2) Levene test is a robust test compared with Bartlett More Than 2 Samples ANOVA is Analysis of Variance ANOVA : If we have 3 or more machines to compare To analyze the variance we have chi-square test for 1 variance test and F-test for two variance test For testing equality of several means with technique of ANOVA H0 : μ1 = μ2 = μ3 = μ4 … = μn (means are equal) Ha : At least one of the means is different from others (means are NOT equal) How to calculate ? from scipy.stats as stats m1 = [150,151,152,152,151,150] m2 = [153,152,148,151,149,152] m3 = [156,154,155,156,157,155] stats.f_oneway(m1,m2,m3) #output: #statistics : 22.264 #pvalue : 3.23e-05 Results: As the pvalue is very small we conclude that at least one machine is different from others We can also use the package statsmodels.stats with method oneway.anova_oneway() ANOVA Concept Variation within : Variation of the values in the same machine (inside or ERROR) Variation between: Variation of the values between machines (treatment) To check we take the ration of these variations using F test to conclude if there are variation of not Post Hoc Tests Post Hoc Tests attempt to control the experimentwise error rate (usually alpha = 0.05) just like one-way ANOVA is used instead of multiple t-test Tukey’s Test from statsmodels.stats.multicomp method pairwise_tukeyhsd import statsmodels.stats.oenway as oneway from statsmodels.stats.multicomp import pairwise_tukeyhsd df = mpg[mpg[&#39;cylinders&#39;] == 4][[&#39;mpg&#39;, &#39;origin&#39;]] result = pairwise_tukeyhsd(endog = df[&#39;mpg&#39;] , groups = df[&#39;origin&#39;] , alpha = 0.05 ) print(result) #output # p-adj (pvalue) = 0.7995 # Based on result we going to see the there are no significant different between europe and usa Goodmess of Fit Test Use Chi Square as test statistics To test if the sample is coming from a population with specific distribution Other goodness-of-fit tests are: Anderson-Darling Kolmogorov-Smirnov H0 : The data follow a specified distribution Ha : The data do not follow the specified distribution Sample A coin is flipped 100 times. Number of heads (40) and tails(60) . Is this coin biased ? (95% confidence level) H0 : Coin is not biased Ha : Coin is biased alpha = 0.05 # Using python import scipy.stats as stats exp = [50,50] obs = [40,60] stats.chisquare(f_obs = obs, f_exp = exp) #output pvalue = 0.0455 Result : We reject the null hypotheses which means the coin are biased Contingency Tables Help to find relationship between two discrete variables H0 : Is that there is no relationship between the row and column variables Ha : is that there is a relationship (Ha does not tell what type of relationship exists) Using python we can use scipy.stats import scipy.stats as stats sh_op = np.array([[22,26,23], [28,62,26], [72,22,66]]) stats.chip2_contingency(sh_op) # output : pvalue = 3.45e-10 Results : Reject the null hypothesis which means there is a relationship between rows and columns "]]
