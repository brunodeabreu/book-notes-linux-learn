[["pós-graduação-processamento-linguagem-natural-e-ia-generativa.html", "# 10 Pós-Graduação Processamento Linguagem Natural e IA Generativa 10.1 Sobre o documento : 10.2 Dicas : 10.3 Part 1 : DL para AI", " # 10 Pós-Graduação Processamento Linguagem Natural e IA Generativa 10.1 Sobre o documento : Estas são anotações de aula, livros ou artigos para estudo futuro. 10.2 Dicas : Devemos evitar: Projetos sem objetivos de negócio claro e muito focado na tecnologia Falta de metricas (KPI) claros para medir sucesso Iniciar com modelos ou ténicas de alta complexidade Falta de monitoramento 10.3 Part 1 : DL para AI 10.3.1 Fundamentos 10.3.1.1 Intro Arquiteturas : DNN (Redes neurais desamente conectadas) CNN (Redes neurais convolucionais) RNN (Redes Neurais Recorrentes) Autoencoders GANs (Redes Adversariais Generativas) Redes Siamesas Modelos de Capsule Modelos de Atenção e Transformes Activation function : Introduzem a não linearidade o que permite que a rede modelo funções complexas e regularização, alguns tipos de funções de ativação Sigmoid, Tanh, ReLU ( Leaky ReLU, Parametric ReLU, GenLU) Overfitting : Quando o modelo aprende demais sobre os dados e não consegue generalizar Underfitting : Quando o modelo é muito simples. não se ajusta aos dados de treino, ou seja , o modelo não aprendeuos padrões dos dados Regularizaçáo: L1 (penaliza soma absoluta) e L2 (penaliza a soma dos quadrados) são tecnicas de regressão linear e logística Dropout : certos neuronios são desligados aleatoriamente em cada interação Early Stopping : interrompe o treinamento assim que o desempenho começam a degradar. Loss function : quantifica o quão bem as previsões de um modelo se alinham com os valores reais observados. A escolha da função depende do tipo de problema a ser resolvido: Regressão : MSE, MAE Classificação: Emtropia cruzada ou log loss, Hinge Loss Modelos Generativos GANs: Gradiente descendente Tools and Frameworks Frameworks Python : PyTorch, TensorFlow, MxNet, JAX, ONNX Frameworks C++ : Armadillo, MLPack Backpropagation O modelo irá fazer a primeira passada de calculo Forward Pass e calcular o erro. Depois disso o algoritmo de Backpropagation irá através de cálculos de derivadas reduzir o erro do modelo (Loss), isso é feito alterando os pesos novamente com objetivo de reduzir o erro final. Novo peso = Peso anterior - Derivada * Learing Rate Algumas redes e arquiteuras CNN : Utilizada para detecção de objetos e lidar com imagens RNN : Utilizadas para linguagem natural ou series temporais, capaz de manter um estado de memória. Temos algumas variações LSTM (Long Short-term memory) e GRU (Gated Recurrent Units) Redes Neurais Generativas : Redes que permitem a geração de novos dados semelhantes aos dados que foram treinados, uma arquitetura de Redes generativas é a GANs (Redes Adversariais Generativas) que são duas redes treinadas simultaneamente (O gerador e o discriminador) Gerador : Produz dados novos a partir de ruído aleatório Discriminador : Tenta distinguir entre amostras geradas (fake) e dados reais O treinamento contiua até que o gerador se torne suficientemente bom para produzir dados que o discriminador não consiga diferenciar entre reais ou fake. Outro tipo de rede neural generativa é o Modelo Autorregressivo como PixelRNN, utilizado para gerar imagens ou música, Temos também Redes Geradoras de Momento Variacional (Variacional Autoencoders VAEs) : A ideia é aprender a distribuição latente dos dados de entrada e em sequida gerar novos dados Mecanismos de Atenção e Transformadores: focam e partes específicas da entrada Transfer Learning e Modelos Pré-treinados Transfer Learning é uma técnica onde um modelo desenvolvida para uma tarefa é reutilizado como ponto de partida para outra tarefa relacionada, pode ser utilizado como estratégia de inicialização de pesos Modelos pré-treinados são modelos ja treinados em grandes bases de dados: Visão Computacional : VGGNet, ResNet, Inspectino PLN : BERT, GPT, Llama, T5 Otimização : processo de ajustar os parâmetros (pesos) do modelo com o objetivo de minimizar a função de perda e com isso encontrar o conjunto ótimo de parametros que resulte na melhor performance do modelo, algoritmos utilizados : Gradiente descendente Gradiente descendente Estocastico (SGD) Momentum Adam Batch Normalization Regularização conjunto de técnicas que visam impedir o overfitting, algumas técnicas : L1 e L2 Dropout (Desativa neuronios durante o treinamento) Early Stopping (Interronpe o treinamento assim que a performance piora) LLM (Large Language Models) 10.3.1.2 Projecto 1 Attention Is All You Need: Exemplo simples para visualizar os modulos e a camada de atenção: Jupyter Notebook Project 1 Na arquitetura transformes o mecanismo de atenção do tipo (Scaled dot-product) utiliza três componentes : Q_(Query)_ : representa parte que estamos interessados, por exemplo : Em PLN poderia ser a frase que estamos tentando traduzir. Em um modelo transformer para cada posição uma query é gerada e são usadas para pontar a qualidade da entrada. K_(Key)_ : Usada para pontar a entrada e comparada com a query para determinar o grau de atenção, essa comparação resulta em um conjunto de pontuação que indica a relevancia de cada parta da entrada para representar a query K e Q determina onde o modelo deve focar V_(Value)_ : contém a info real que queremos extrair, compoe a saida do mecanismo de atenção, cada value é associado a uma key. O mecanismo de atenção calcula um conjunto de pontuações e aplica softmax para obter pesos de atenção e usa esses pesos para ponderar os values, criando uma saída . 10.3.2 Referencia : Deep Learning book Algoritmo Backpropagatio What’s the Backward-Forward FLOP Ratio for Neural Networks? Forward e Backward Pass [A Comprehensive Guide to the Backpropagation Algorithm in Neural Networks](https://neptune.ai/blog/backpropagation-algorithm-in-neural-networks-guide] What is forward and backward propagation in Deep Learning? *[A Deep Dive Into the Transformer Architecture –The Development of Transformer Models](https://www.exxactcorp.com/blog/Deep-Learning/a-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models 3 Alternativas Para Usar LLMs Transformers: is attention all we need in finance? Attention Is All You Need "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
