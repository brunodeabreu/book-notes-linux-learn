[["index.html", "Learning notes : Linux, Devops, entre outros # 1 Intro", " Learning notes : Linux, Devops, entre outros Bruno Machado 2021-12-09 # 1 Intro During some Linux courses I usually take notes for future reference, so I use this notes to organize my notes for study and future projects. "],["server-administrator-i---rh124.html", "# 2 Server Administrator I - RH124 2.1 What is linux ? 2.2 Command Line 2.3 Managing File system 2.4 Help 2.5 Text Files 2.6 Managing Local Users and Groups 2.7 Controlling access to files 2.8 Monitoring and Managing Linux process 2.9 Controlling Services and Daemons 2.10 Configuring and Securing SSH 2.11 Analysing and Storing Logs 2.12 Managing Networking 2.13 Archiving and transferring files 2.14 Intalling and updating software packages 2.15 Accessing Linux File Systems 2.16 Analysing Server and Getting Support 2.17 Extra", " # 2 Server Administrator I - RH124 2.1 What is linux ? Modular operations system where you can add components : Open Source Copyleft lic GNU : General Public License LGNU: Lesser General Public License Permissive lic: MIT Simplified BSD Apache Software License Red Hat contribute and facilitate open source projects, validating the code and support code that Red Hat validate and test. Sample of Red Hat Products: * Red Hat Enterprise Linux 8 * Red Hat Open Shift 4 * Red Hat Ansible Automation * Red Hat Ceph Storage * Red Hat OpenStack Platform * Red Hat Virtualization * Red Hat Gluster Storage Benefits of open source software: Code can survive the loss of orginal developer or distributor We can learn from real-world code and develop more effective applications Red Hat sponsor and integrate open source projects into the Fedora project and participate in upstream projects 2.2 Command Line 2.2.1 $ and # Attention on shell when you are on command line: Root user : # Normal user : $ Sample of using - or -- -all : Shell going to be interpreting all arguments individually # Shell going to be interpreting all arguments individually a, l and l ls -all . --all : Shell will interpreting the entire word # Shell will interpreting the entire word ls --all . 2.2.2 To login into another computer using ssh ssh &lt;machine or server&gt; 2.2.3 Executing commands # List the user that you login whoami # formatting the dates date +%F # Executing one command after the other date +%A; uname -r; whoami ## runner ## 2021-12-09 ## Thursday ## 20.6.0 ## runner Conditional commands execution &amp;&amp; This is the basic IF statement The date command must complete successfully than the uname -r command will be running and if execute successfully the whoami command will be executed # If with and date +%F &amp;&amp; uname -r &amp;&amp; whoami echo echo # If with or ate +%F || uname -r &amp;&amp; whoami ## 2021-12-09 ## 20.6.0 ## runner ## ## ## bash: line 7: ate: command not found ## 20.6.0 ## runner command history !! : execute the last command !12: execute the command 12 on history ctr+r check type of file The file command is used to determining the type fo file : file /etc/issue echo echo file /bin/bash ## /etc/issue: cannot open `/etc/issue&#39; (No such file or directory) ## ## ## /bin/bash: Mach-O universal binary with 2 architectures: [x86_64:Mach-O 64-bit executable x86_64] [arm64e:Mach-O 64-bit executable arm64e] ## /bin/bash (for architecture x86_64): Mach-O 64-bit executable x86_64 ## /bin/bash (for architecture arm64e): Mach-O 64-bit executable arm64e check content of file cat or tac head or tail wc -l : count lines of file 2.3 Managing File system 2.3.1 The file system hierarchy / Try command tree -d / : Top of system file system hierarchy /usr : unix system resources, installed software and libraries /usr/bin : regular commands and utilities /bin : binaries executable that are usable by normal users /sbin : system binaries executable typically used by root user /boot : component that are necessary to boot file system, like bootloader called grub 2 and linux kernel /dev : device files, represent hardware components /etc : extended text configuration configuration files /home : home dir for normal users /run : runtime data going to be recreate on a reboot /var: variable data that should survive a reboot, log files, database and website files /root : root user home dir /tmp : accommodate temporary files, deleted by 10 days /var/tmp : another temp dir , purge every 30 days 2.3.2 Absolute and Relative paths Absolute path, is the complete path, no ambiguity, start with / : #sample cat /etc/issue Relative path is the path to a file relative the current position, do not start with / cd / cat etc/issue 2.3.3 Managing Files mkdir -p : create the parents that do not exists pwd mkdir -p dirA/dirB/dirc ls -R dirA ## /Users/runner/work/book-notes-linux-learn/book-notes-linux-learn ## dirB ## ## dirA/dirB: ## dirc ## ## dirA/dirB/dirc: Commands cp -r : copy dir rm -i : show to you msg to confirm rmdir : remove dir rm -r : delete dir recursive mv : move touch : create files Links Index Node ((inode_): How files are identified, keep track of: * permissions * ownership * date &amp; time stamps * paths to data on file system ls -li LICENSE ## 12892990340 -rw-r--r-- 1 runner staff 6556 Dec 9 19:08 LICENSE 40652348 : file ID (inode) 1 : means one name using this iNode right now Hard Link ln LICENSE LIC2 ls -li LIC* ## ln: LIC2: File exists ## 12892990339 -rw-r--r-- 1 runner staff 6556 Dec 9 19:08 LIC2 ## 12892990340 -rw-r--r-- 1 runner staff 6556 Dec 9 19:08 LICENSE Both LIC2 and LICENSE have the same iNode but the number right now is 2, not able to identify which one was create first, this is a hard link Soft Link ln -s README.md README_2.md ls -li READ* ## ln: README_2.md: File exists ## 12892990341 -rw-r--r-- 1 runner staff 741 Dec 9 19:08 README.md ## 12892990342 lrwxr-xr-x 1 runner staff 9 Dec 9 19:08 README_2.md -&gt; README.md Both have different iNode number, so they are two different files, the permission there are a l meaning link, this is a soft link, if we delete file 3 we have a broken link # create tmp files mkdir -p tmp ;cd tmp ;touch 1file 2file 3file 4file able alfa baker bravo cast easy echo _src # List files that match this case echo echo &quot;List files that match this case Xfile&quot; ls ?file echo &quot;----&quot; # list files that begin with a or c or e echo echo &quot;List files that begin with a or c or e &quot; ls [ace]* echo &quot;----&quot; # List files that do not start with a c or e echo echo &quot;List files that do not start with a c or e&quot; ls [^ace]* # or ls [!ace]* echo &quot;----&quot; # List all file that begin with an alphabetical character echo echo &quot;List all file that begin with an alphabetical character&quot; ls [[:alpha:]]* # List all files that begin with digit echo echo &quot;List all files that begin with digi&quot; ls [[:digit:]]* echo &quot;----&quot; # List all files that match digit or alphabetical char echo echo &quot;List all files that match digit or alphabetical char&quot; ls [[:alnum:]]* echo &quot;----&quot; # List all files that begin with punctuation echo echo &quot; List all files that begin with punctuation&quot; ls [[:punct:]]* echo &quot;----&quot; # clean cd .. rm -rf tmp ## ## List files that match this case Xfile ## 1file ## 2file ## 3file ## 4file ## ---- ## ## List files that begin with a or c or e ## able ## alfa ## cast ## easy ## echo ## ---- ## ## List files that do not start with a c or e ## 1file ## 2file ## 3file ## 4file ## _src ## baker ## bravo ## ---- ## ## List all file that begin with an alphabetical character ## able ## alfa ## baker ## bravo ## cast ## easy ## echo ## ## List all files that begin with digi ## 1file ## 2file ## 3file ## 4file ## ---- ## ## List all files that match digit or alphabetical char ## 1file ## 2file ## 3file ## 4file ## able ## alfa ## baker ## bravo ## cast ## easy ## echo ## ---- ## ## List all files that begin with punctuation ## _src ## ---- Brace expansion mkdir -p tmp ;cd tmp echo {Sun,Mon,Tues,Wednes}day.log # create dirs mkdir -p RHEL{6,7,8}; ls RHEL* # create sequence of files using .. touch song{1..5}.mp3 ; ls *.mp3 #clean cd .. rm -rf tmp ## Sunday.log Monday.log Tuesday.log Wednesday.log ## RHEL6: ## ## RHEL7: ## ## RHEL8: ## song1.mp3 ## song2.mp3 ## song3.mp3 ## song4.mp3 ## song5.mp3 Variable SOMETHING=value echo $SOMETHING # command substitution echo &quot;Today is $(date +%A)&quot; ## value ## Today is Thursday skell The contenct of path /etc/skel is automatically copied to all users 2.4 Help All man pages are on /usr/share/man Commands man pinfo man -k cron ## cron(8) - daemon to execute scheduled commands (Vixie Cron) ## crontab(1) - maintain crontab files for individual users (V3) ## crontab(5) - tables for driving cron ## DateTime::Locale::en_FM(3pm) - Locale data examples for the English Micronesia (en-FM) locale ## cron(8) - daemon to execute scheduled commands (Vixie Cron) ## crontab(1) - maintain crontab files for individual users (V3) ## crontab(5) - tables for driving cron To go direct to a section , belo example of command to go direct to session 5 man 5 crontab 2.5 Text Files Channels 0 : stdin , read only used by keyboard 1 : stdout, write only, terminal 2 : stderr, write only, terminal 3+ : file name, read and write We can direct errors to a specific file like below I do not have the file or dir abc ls abc file1 2&gt; errors.log # output file1 # on errors.log ls: cannot access &#39;abc&#39;: No such file or directory We also can send both to the same file ls /show /boot &amp;&gt; combine.log # another option # ls /show /boot &gt; combine.log 2&gt;&amp;1 cat combine.log rm combine.log ## ls: /boot: No such file or directory ## ls: /show: No such file or directory Another usage when we have an output with lot of errors and can only the the results, all the errors message was sent to /dev/null find / -iname passwd 2&gt; /dev/null #output /usr/share/bash-completion/completions/passwd /usr/share/doc/passwd /usr/share/lintian/overrides/passwd /usr/bin/passwd /etc/passwd /etc/pam.d/passwd A good usage is send successfully output to an output.log file and the errors to errors.log file ls /shoe /boot &gt;&gt; output.log 2&gt;&gt; errors.log Send emails with content of file, the &lt; will direct the content of file to email to &lt;user&gt; mail -s &quot;Subject text&quot; &lt;user&gt; &lt; file Pipe The resolv.conf have 6 lines in total but we can see the lines without comment # and direct the output using pipe | wc -l /etc/resolv.conf echo # check the lines without comment grep ^[^#] /etc/resolv.conf echo # combine the grep with pipe grep ^[^#] /etc/resolv.conf | wc -l ## 17 /etc/resolv.conf ## ## nameserver 1.1.1.1 ## nameserver 208.67.222.222 ## ## 2 Another way is get the output and save on file using pipe | and tee find / -iname passwd 2&gt; /dev/null | tee find.out 2.5.1 vim Modes : Insert : i Command : default yy : Copy p : paste 5p : paste 5 times dd : delete ZZ : save and quite cw : change word x : delete character r : replace character a : append Extend command : : q! : exit without save wq : save and exit Visual : v crt + v : block column to manipulate column shift + v: line mode to select line x : delete u : undo 2.5.2 Changing shell Setting a editor # To set env EDITOR=nano crontab -e # To unset expor -n EDITOR The crotab will open on nano editor instead of vi or vim Also can set the variable EDITOR to nano and the same will hapens if we call crontab é # using export export EDITOR=nano Some .bash* files and adjust All the history are saved on .bash_history .bashrc user specific env for example: we can add more lines on history add the variable export HISTFILESIZE=2000 add export HISTTIMEFORMAT=\"%F %T \" 2.6 Managing Local Users and Groups whoami : Show your user id : show your user, groups Type of users* Super User root Account users, not used by people Regular users 2.6.1 Users /etc/passwd : file with user info grep bruno /etc/passwd #output bruno:x:1000:1000:bruno,,,:/home/bruno:/bin/bash User : Bruno x : long time ago where password where stored 1000 : User ID 1000 : group ID Comment User home dir : /home/bruno:/bin/bash The password is stored on /etc/shadow, only root can access this file Sample : 6$CSsXcYG1L/4ZfHr/$2W6evvJahUfzfHpc9X.45Jc6H30E...output omitted.. 6 : Hash algorithm used , SHA-512 CSsXcYG1L/4ZfHr/ Used by cript info W6evvJahUfzfHpc9X.45Jc6H30E has password Another way to list and return user info is using getent command getent passwd bruno #output bruno:x:1000:1000:bruno,,,:/home/bruno:/bin/bash useradd : create new user, sample : useradd kano userdel : remove user sample 1: userdel kano, but do not remove the data and home dir sample 2: userdel -r kano, delete user and data/home dir usermod -c : add comments to user usermod -a -G &lt;user&gt; -a : append to the secondary group membership, if do not use a overwritten the current info -G : group name usermod -g &lt;group&gt; &lt;user&gt; -g : change the primary group info usermod -L &lt;user&gt; : Lock a user account usermod -U &lt;user&gt; : Unlock a user account 2.6.2 Groups /etc/group : file with group info grep bruno /etc/group #output Show in New Window /etc/issue: ASCII text adm:x:4:syslog,bruno cdrom:x:24:bruno sudo:x:27:bruno dip:x:30:bruno plugdev:x:46:bruno lpadmin:x:113:bruno bruno:x:1000: sambashare:x:130:bruno bruno : group name x : pwd info that is not used 1000 : group id Members : of group Show groups from my users using command groups. groupadd : add new group (GID &gt; 1000) -g : specify the GID, groupadd -g 1000 group1 -r : create the system group (GID 0-999) groupdel : remove group groupdel &lt;group_name&gt; groupmod : modify the group -n : change the group name, groupmod -n &lt;new_name&gt; &lt;old_name&gt; 2.6.3 Gaining superuser access su : change to root super user, without set the profile, the path still your regular user su - : change to root user and set the profile and env of root sudo : allow run commands as another user sudo visudo : sudoers file Line : if you are member the group wheel you can be logged in from any computer and you can run all cmds as all users # Allows people in group wheel to run all commands %wheel ALL=(ALL) ALL % indicate a group sudo -i or sudo su - : assume as root user /etc/sudoers.d/&lt;user&gt; : drop config into that on dir /etc/sudores.d as if you are editing the /etc/sudoers Tip To create /etc/sudoers.d/admin file and grant all members of admin group total privileges echo &quot;%admin ALL=(ALL) ALL&quot; &gt;&gt; /etc/sudoers.d/admin To create just for one user echo &quot;user ALL=(ALL) ALL&quot; &gt;&gt; /etc/sudoers.d/user 2.6.4 Managing user passwords We no longer store password on /etc/passwd, the password are stored on /etc/shadow chage : used to change the aging info for a user password Sample : chage -m 1 -M 26 -W 4 -I 3 -E (2019-05-31) &lt;user&gt; -m : min wait time before user can change the pwd again, in this case 1 day -M : when they have to changed the pwd, in this case in 26 days need to change -W : warning of 4 days -I : once the pwd expire they have 3 more days to login and change the pwd -E : expiration date chage -l : show all the info /etc/login.defs : Define info of login such PASS_MAX_DAYS, PASSMIN_DAYS, etc Add user with nologin shell useradd -s &lt;user&gt; -s /sbin/nologin Tip Change the user and update expiration to more 180 days chage -E $(date -d + 180days +%Y-%m-%d) &lt;user&gt; 2.7 Controlling access to files drwxrwxrwx d : dir, can be l link, etc r | 4 : read w | 2 : write x | 1 : execute Owning User : first 3 rwx Owning Group : next 3 Other : last 3 Commands chmod : change the permission mod chmod 740 &lt;file or dir&gt; or chmod o+rw &lt;file or dir&gt; chown : change ownership chmod user:group &lt;file or dir&gt; chmod :group &lt;file or dir&gt; or chmod user &lt;file or dir&gt; Tip Give you read and execute permission do dir but no execute on files inside the dir, i.e, we can list the content but not execute files inside chmod -R a=rX 2.7.1 Special permissions Sticky bit : In a collaborative dir you can create files on the dir and only delete files that you have created u+s (suid) : Files executes as the user that owns the file, not the user that ran the file g+s (guid) : file execute as the group that owner the file, files newly create in directory have their group owner set to match the group owner of the dir. o+t (sticky): Users with write access to dir can only remove files that they own Sticky bit — directories o+t To set o+t or 1 chmod o+t &lt;dir&gt; or chmod 1770 &lt;dir&gt; To remove o-t or 0 chmod o-t &lt;dir&gt; or chmod 0770 &lt;dir&gt; # To set sticky bit chmod o+t dirA ls -ld dirA # To remove sticky bit #chmod o-t dirA s : Files created on the dir will have the same owning group of dir Set grid to dirs or files g+s To set g+s chmod g+s &lt;dir&gt; To unset g-s chmod g-s &lt;dir&gt; # To set chmod g+s dirA ls -ld dirA # To remove # chmod g-s dirA Set uid to files u+s To set u+s chmod u+s &lt;file&gt; To unset u-s chmod u-s &lt;file&gt; 2.7.2 Default permissions - umask umask : show umask info umask 0000 : set the umask to 0000 or 777 to new dirs To change the user umask we can update the .bashrc file 0022: * The permissions of files going to be 755 2.8 Monitoring and Managing Linux process Process state description 2.8.1 Commands to monitor top %CPU load average tasks ps aux , ps -ef PID : process ID PPID: Parent process ID Time CMD TTY : from where the process is running htop 2.8.2 Controlling jobs sample : gnome-calculator &amp; Command : * jobs : list all jobs * fg %&lt;job_number&gt; : Bring the job to foreground : * Ctrl+z : suspend/stop the job * ps j : show the info relate jobs * bg %&lt;job_number&gt; : restart job * Ctrl+c : terminate job 2.8.3 Killing process Signals kill -l ## 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL ## 5) SIGTRAP 6) SIGABRT 7) SIGEMT 8) SIGFPE ## 9) SIGKILL 10) SIGBUS 11) SIGSEGV 12) SIGSYS ## 13) SIGPIPE 14) SIGALRM 15) SIGTERM 16) SIGURG ## 17) SIGSTOP 18) SIGTSTP 19) SIGCONT 20) SIGCHLD ## 21) SIGTTIN 22) SIGTTOU 23) SIGIO 24) SIGXCPU ## 25) SIGXFSZ 26) SIGVTALRM 27) SIGPROF 28) SIGWINCH ## 29) SIGINFO 30) SIGUSR1 31) SIGUSR2 default kill -15 &lt;process&gt; die right now kill -9 &lt;process&gt; stop kill -19 &lt;process&gt; continue kill -18 &lt;process&gt; kill with -15 several with same name killall &lt;name&gt; pkill -t pts/2 : terminate the user logged on pts/2* pkill -SIGTERM tail : will kill the tail process running pstree : display tree view of process pgrep -l -u &lt;user&gt; : identify the process that going to be killed by pkill 2.8.4 Monitor process activity Load Average uptime command review the load average uptime ## 19:09 up 3 mins, 1 user, load averages: 11.38 7.97 3.47 1,24 : Last min 1,07 : Last 5 min 0,93 : Last 15min To analyze the load average need to know how many CPU’s do we have lscpu | grep -i &#39;CPU(s)&#39; #or echo echo &quot;========Number of CPUs=====&quot; : cat /proc/cpuinfo | grep &quot;model name&quot; | wc -l #output CPU(s): 8 On-line CPU(s) list: 0-7 NUMA node0 CPU(s): 0-7 ========Number of CPUs===== : 8 So 1,24 / 8 is 0,155 , what means that my CPU is busy 15% of the time on last min On last 15min my cpu is 11% (0,93/8) last 15min and 13%(1,07/8) last 5min. To know if the CPU is overload the results show be 1.13 what means that my CPU is overload by 13% on that particular time Another example #From lscpu, the system has four logical CPUs, so divide # load average: 2.92, 4.48, 5.20 #divide by number of logical CPUs: 4 4 4 ---- ---- ---- per-CPU load average: 0.73 1.12 1.30 # This system&#39;s load average appears to be decreasing. # With a load average of 2.92 on four CPUs, all CPUs were in use ~73% of the time. # During the last 5 minutes, the system was overloaded by ~12%. # During the last 15 minutes, the system was overloaded by ~30% top is another command that can be used to monitor the system k : ask by the PID to be terminated M : sort by memory h : help shift + w : write the top config on /home/&lt;user&gt;/.config/procps/toprc On top : PID : process ID PR : Priority VIRT : Virtual memory that process is using RES : Physical RAM used SHR : Shared memory %CPU %MEN Time : how long it is running Command 2.9 Controlling Services and Daemons The systemd is responsable for initializing the system and uses units that represent daemons: * service : database, web service, tc * target : collection of units * device * socket systemctl list-units | head -n 10 # or # systemctl # systemctl list-units --type=service # systemctl list-units --type=target status Status of sshd : systemctl status sshd #systemctl list-units --type=service systemctl status bluetooth.service #output ● bluetooth.service - Bluetooth service Loaded: loaded (/lib/systemd/system/bluetooth.service; enabled; vendor preset: enabled) Active: active (running) since Sun 2021-08-22 08:50:54 -03; 5 days ago Docs: man:bluetoothd(8) Main PID: 906 (bluetoothd) Status: &quot;Running&quot; Tasks: 1 (limit: 18970) Memory: 2.7M CGroup: /system.slice/bluetooth.service └─906 /usr/lib/bluetooth/bluetooth Stop : systemctl stop &lt;service&gt; Start : systemctl start &lt;service&gt; Restart: systemctl restart &lt;service&gt; Reload : systemctl reload &lt;service&gt; ask : systemctl is-active &lt;service&gt; or ...is-enable..., is-failed disable: systemctl disable &lt;service&gt;, the service will not start when the system is started list dependency : systemcl list-dependecies &lt;service&gt; mask : systemctl mask &lt;service&gt; prevent service to be started unmask: systemctl unmask &lt;service&gt; 2.10 Configuring and Securing SSH On RHEL8 we have OpenSSH which implement not only the SSH daemon but also SSH command line tool ~/.ssh/known_hosts stores the fingerprint sent by server to future communication To define the StrictHostKeyChecking we can edit the file ~/.ssh/config or /etc/ssh/ssh_config 2.10.1 Configure SSH Key-based Authentication private - decrypt When you connect to ssh server using private key the remote machine will generate a challenge (encrypted with your public key), if you are able to decrypt that encrypted challenge then yo are allowed to make a connection public - encrypt when you ssh to a server the public key is saved on the SSH server 2.10.2 Create a public and private keypair Issue to start and create the Key : ssh-keygen Enter the file name Create a paraphrase Result: We going to have a public and private key created Install the key on server ssh-copy-id -i .ssh/&lt;public key name&gt; &lt;hostname&gt; Test ssh -i .ssh/&lt;public key name&gt; &lt;hostname&gt; Add the private key to agent # start the agent eval $(ssh-agent) # add the key ssh-add .ssh/&lt;public key name&gt; 2.10.3 Customizing OpenSSH Service Config Disable the root ability to login on /etc/ssh/sshd_config to modify the daemon config and change the PermitRootLogin to no Tecniques to avoid password Create a sshusers group and configure /etc/ssh/sshd_config 4 allowGroups, or Create a private key and public key 2.11 Analysing and Storing Logs Systemd is the heart of RHEL8 system and need to analyse how it is working The journal collects messages from several sources (booting, daemons, etc) and we can query using journalctl but this is not persistent by default. There are another process rsyslog that read syslog and receive systemd-journal and save it on /var/log Facility is one of the following keywords : auth, authpriv, cron, daemon, kern, lpr, mail, mark, news, security, syslog, user, uucp, local0 to local7. Priority is one of the following keywords : debug, info, notice, warning, warn, err, error, crit, alert, emerg, panic /var/log/messages : Store most of syslog messages /var/log/secure : store syslog messages related to security and authentication operations rsyslog : service that organize syslog messages into /var/log /var/log : directory of syslog files /var/log/maillog : store syslog messages related mail server /var/log/cron : store syslog messages related to the schedule jobs /var/log/boot.log : store console message related to system startup Syslog codes 0 - emerg 1 - alert 2 - crit 3 - err 4 - warning 5 - notice 6 - info 7 - debug The message is organized as facility.priority and we can configure to direct it to particular file like authpriv.notice and send to /var/log/foo Config file : /etc/rsyslog.conf, where we can add all rules Avoid edit the main config file /etc/rsyslog.conf and use drop-in directory /etc/rsyslog.d Log rotate prune framework /etc/logrotate.conf : Config file /etc/logrotate.d : drop-in dir Manual messages to syslog sample : logger -p local7.notice &quot;log entry created on host&quot; 2.11.1 Reviewing System Hournal Entries Journal is not persistent and can find the logs on /run/log/journal systemd-journald - command journalctl journalctl -r : reverse the log message, last page of msg show first journalctl -u &lt;unit&gt; : show logs from unit such as sshd.service journalctl -u &lt;unit&gt; --since today : show today message of particular unit journalctl -u &lt;unit&gt; --since \"2019-04-15 09:00:00\" --until \"2019-04-15 11:00:00\" -p warning : Show the warning message of unit in particular time journalctl --since \"2019-04-15 09:00:00\" --until \"2019-04-15 11:00:00\" -p warning : Show ALL warning message in particular time since request all warning we also going to see the err, crit, alert and emerg messages journalctl -p err : show the errors journalctl -f : to monitor the system journalctl -n 50 : to show the last 50 messages journalctl _SYSTEM_UNIT=sshd.service _PID=xx : will show to you message about this unit and PID journalctl -b -1 : show the data related one boot if the journal is persistent 2.11.2 Preserving the system journal By default the journal is not persistent on /run/log/journal and after reboot we lose all data. To preserve the journal we need to create the dir /var/log/journal, also by default journal are not allow to get more than 10% of file system or leave less than 15% of the file system free. The configuration are on /etc/journald.conf 2.11.3 Maitaining Accurate time Have system in sync is important Checking time timedatectl #output Local time: Fri 2021-08-27 13:24:50 -03 Universal time: Fri 2021-08-27 16:24:50 UTC RTC time: Fri 2021-08-27 16:24:50 Time zone: America/Sao_Paulo (-03, -0300) System clock synchronized: yes NTP service: n/a RTC in local TZ: no To list the timezones we can use the command timedatectl list-timezones and set with command: timedatectl set-timezone &lt;timezone&gt; The system will use NTP (Network Time Protocol) to synchronize the time with a machine to perform that we can set it to True timedatectl set-ntp true RedHat 7 and 8 use the chronyd.service to synchronize the time and inside of /etc/chrony.conf we going to find the list of servers that are in sync and can check the sources with chronyc sources 2.12 Managing Networking TCP/IP layers : Application : How clients communicate across plataforms, sample Web browser talking withWeb server SSH : remote login HTPS : web NFS or CIFS : file share SMTP : emal Transport : How packets are sent and received like TCP : connection-oriented bi-direction form of guaranteed messaging UDP : connectionless unidirectional and non-guaranteed messaging Internet : Specify how packets are routed acrosss network Link layers : physical askpects of networking MAC Devices : The name dependes on where and how the device is connected EN : Ethernet devices ENO1 : Onboard Ethernet Interface with index number 1 ENS3 : Ethernet device in hotplug slot 3 WLAN : Wireless LAN devices WWAN : Wireless AN devices WLP4S0 : PCI buss number 4 connect on slot 0 2.12.1 TCPIP IPV4 IP IP Address : 32bits divided up into 4 octets, this is the interface unique identity on the network Subnet mask : divide the IP address into the host portion as well as the network portion and that is used to facilitate routing Notation : 255.255.255.0 or /24 IPV6 “New implementation of IP” : Allow every single person to have its own unique IP Address IPV6 is 128-bit number with 8 colon separated groups of 4 hexadecimal nibbles, also use subnet mask and normally make use of /64, i.e, 64 bits Common ipv6 address ::1/28 : localhost ::/0 : default route fe80::/10 : link-local (start with fe80), those link are not routable, when we have IPV6 enable the link-local will be allocated automatically and allow to communicate with other machines using IPV6 on the same local segment ip a #output 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: enp3s0f1: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc fq_codel state DOWN group default qlen 1000 link/ether 80:fa:5b:4d:16:6d brd ff:ff:ff:ff:ff:ff 3: wlp4s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether f8:94:c2:74:dc:04 brd ff:ff:ff:ff:ff:ff inet 192.168.15.8/24 brd 192.168.15.255 scope global dynamic noprefixroute wlp4s0 valid_lft 35492sec preferred_lft 35492sec inet6 fe80::19da:6be7:d851:7ea9/64 scope link noprefixroute valid_lft forever preferred_lft forever The IPV6 : inet6 fe80::19da:6be7:d851:7ea9/64 scope link Interface : enp3s0f1 MAC : link/ether 80:fa:5b:4d:16:6d brd ff:ff:ff:ff:ff:ff IPV4 : inet 192.168.15.8/24 brd 192.168.15.255 scope global dynamic Commands PING tool for IPV6 : ping6 &lt;ipv6&gt;%&lt;interface&gt; Show IPv4 table : ip route Show IPv6 table : ip -6 route Show specific interface : ip a s &lt;interface Properties of interface : ip link show Link statistics : ip -s link show Files /etc/hosts : There are number of names that resolve ip address /etc/services : List of commonly used services /etc/resolv.conf : Wejre DNS service are defined 2.12.2 Validating Network Configuration Commands to review IP config ip a Tools nmcli : command line tool to manage network nmtui : graphical tool to manager network ip : to list ip config netstat : check ports and process tracepath : work similar traceourte tracepath access.redhat.com ss : information about process that are opening up listening sample : ss -plunt sample2 : ss -lt : What is listening, t for TCP 2.12.3 Configure Networking from the Command Line Tool: nmcli an interface to the Network Manager daemon that supoprt tab complition Config on : /etc/sysconfig/network-scripts High level of how network is configured : nmcli #output Show in New Window /etc/issue: ASCII text /bin/bash: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=a6cb40078351e05121d46daa768e271846d5cc54, for GNU/Linux 3.2.0, stripped Show in New Window 40652348 -rw-rw-r-- 2 bruno bruno 6556 Aug 12 21:50 LICENSE Show in New Window wlp4s0: connected to Auto ALMAX-BRUNO &quot;Intel Wireless-AC 3168NGW&quot; wifi (iwlwifi), F8:94:C2:74:DC:04, hw, mtu 1500 ip4 default, ip6 default inet4 192.168.15.8/24 route4 0.0.0.0/0 route4 169.254.0.0/16 route4 192.168.15.0/24 inet6 fe80::19da:6be7:d851:7ea9/64 route6 fe80::/64 route6 ::/0 p2p-dev-wlp4s0: disconnected &quot;p2p-dev-wlp4s0&quot; wifi-p2p, hw enp3s0f1: unavailable &quot;Realtek RTL8111/8168/8411&quot; ethernet (r8169), 80:FA:5B:4D:16:6D, hw, mtu 1500 lo: unmanaged &quot;lo&quot; loopback (unknown), 00:00:00:00:00:00, sw, mtu 65536 DNS configuration: servers: 192.168.15.1 interface: wlp4s0 servers: fe80::aec6:62ff:fefc:7110 interface: wlp4s0 Use &quot;nmcli device show&quot; to get complete information about known devices and &quot;nmcli connection show&quot; to get an overview on active connection profiles. Consult nmcli(1) and nmcli-examples(7) manual pages for complete usage details. Connection show : nmcli connection show #output NAME UUID TYPE DEVICE Auto ALMAX-BRUNO 72e81729-a7a7-473b-8577-013339a4420b wifi wlp4s0 Wired connection 1 44945eb1-1b88-305b-84c0-c0c13d340a0a ethernet -- Details about connections : nmcli connection show \"Auto ALMAX-BRUNO\" Sample of modify profile ipv4.dns # modify the dns ncli con mod &lt;connection name&gt; ipv4.dns &lt;New DNS Name&gt; # or append, in this case we will have two values of dns ncli con mod &lt;connection name&gt; +ipv4.dns &lt;New DNS Name&gt; # grep nmcli con show &lt;name&gt; | grep dns # reinitialize the profile ncli con up &lt;connection name&gt; # check /etc/resolv.conf cat /etc/resolv.conf Status: nmcli dev status #output DEVICE TYPE STATE CONNECTION wlp4s0 wifi connected Auto ALMAX-BRUNO p2p-dev-wlp4s0 wifi-p2p disconnected -- enp3s0f1 ethernet unavailable -- lo loopback unmanaged -- Add conn sample nmcli con add con-name &lt;name of connection&gt; type ethernet ifname &lt;name of connection&gt; Up and Disconnect # Activate connection interface nmcli con up &lt;connection_name&gt; # disconect interface nmcli dev dis &lt;device&gt; Delete , reload and modify # Delete nmcli con del &lt;device&gt; # reload nmcli con reload # modify nmcli con mod &lt;name&gt; List # show NetworkManager of all Net Interface nmcli dev status # show connections nmcli con show # show config nmcli con show &lt;name&gt; 2.12.4 Editing Network Configuration Files Files : We can modify the config file and perform reload nmcli con reload and nmcli con up &lt;name&gt; or use the nmcli commands Every Device have a config file into /etc/sysconfig/network-scripts/ifcfg-*, where there are the hardware address, ipaddr, gateway, domain, etc Sample after update ifcf-* file nmcli con reload mcli con down &quot;static-ens3&quot; nmcli con up &quot;static-ens3&quot; 2.12.5 Configuring Hot Names and Name Resolution Command hostname hostnamectl hostnamectl status hostnamectl status #output Static hostname: turing Icon name: computer-laptop Chassis: laptop Machine ID: bbada5cfcc9a4a80ac8dc5ce6d9f53aa Boot ID: cf3c63ed65874b238e2f825055abbf1f Operating System: Linux Mint 20.2 Kernel: Linux 5.4.0-81-generic Architecture: x86-64 To update hostname hostnamectl set-hostname &lt;new_name&gt; # check cat /etc/hostname Name resolution are om /etc/hosts , the order of resolution are on /etc/nsswitch.conf Adding additional DNS search and IP Address for IPv4 similar we can perform for IPv6 sample # add nmcli con mod &lt;connection name&gt; +ipv4.dns &lt;IP&gt; +ipv4.dns-search &lt;name&gt; # up the connection nmcli con up &lt;connection name&gt; # check cat /etc/resolv.conf 2.13 Archiving and transferring files 2.13.1 TAR Create tar file tar -cf etc.tar /etc Test tar -tf etc.tar Extract tar -xf etc.tar # Extract one file tar -xf etc.tar etc/hosts compression and tar Create tar file and compress with gzip2 tar -czf etc-backup-$(date +%F).tar.gz /etc Create a tar file and compress with bzip2 tar -cjf etc-backup-$(date +%F).tar.bz /etc Create a tar file and compress with Xzip tar -cJf etc-backup-$(date +%F).tar.xz /etc 2.13.2 SCP Securely transfer files between systems Copy dir and files from another system copy xf dir from servera to current dir ‘.’ scp -r user@servera:/xf . Copy without password # create keygen ssh-keygen -N &#39;&#39; # copy id ssh-copy-id servera # run scp command scp -r user@servera:/xf . 2.13.3 SFTP # connect sftp user@servera # list sftp&gt; ls # create dir mkdir backup cd backup # local change dir sftp&gt; lcd /etc # upload hosts file sftp&gt; put hosts 2.13.4 Synchronizing Files Between System Transfer using rsync rsync -Par servera:/xf . If we update the data into servera on xf dir next time we perform rsync it will perform incremental receiving and download only the difference 2.14 Intalling and updating software packages To have the Red Hat software update and donwload the bins from RedHat vendor we need a subscription, there are 4 basics elements : You have to register the server with Red Hat or in fact a satellite server After that need to subscribe the server to entitle it to update Enable repositories Go to lifecycle manamgent to track and manage entitlements via portal or subscription asset manager tool Commands Status : subscription-manager status Register : subscription-manager register Attach to subscription : subscription-manager attach --auto Enable repos : subscription-manager repos --disable='*'--enable='repos_name' 2.14.1 Package manager rpm : Red Hat Package Manager is a popular format for installing software rpm database : keep track of what software is installed and versions Query the rpm database to list all packages installed rpm -qa Query a particular package rpm -q &lt;package&gt; #or rpm -qi &lt;package&gt; List files associate with packages rpm -ql &lt;package&gt; Show the config files rpm -qc &lt;package&gt; Show de documentation rpm -qd &lt;package&gt; Show the script that going to be executed rpm -q -p --scripts &lt;package&gt;.rpm Install Limitations : Cannot manage dependency rpm -i &lt;package&gt; Which package provide a particular file rpm -qf &lt;file&gt; Download using yumdownloader yumdownloader &lt;package&gt; Query on rpm file, listing a list of files provided by this package rpm -qpl &lt;file&gt;.rpm Query the config files into the rpm file rpm -qpc file.rpm Extract all the files and dirs that are inside a RPM file using rpm2cio rpm2cpio &lt;file&gt;.rpm | cpio -duim 2.14.2 Inslalling and Update Software Packages with Yum Search or get info on current repos yum search &lt;pacakge&gt; yum info &lt;pacakge&gt; Query RPM files without have to download using repoquery repoquery -l &lt;package&gt; Find out what package provide a particular file yum provides &lt;file like /etc/fstab&gt; yum profides *bin/authconfig Install yum resolve all dependencies sudo yum install &lt;package&gt; #or sudo yum localinstall &lt;package&gt;.rpm Update sudo yum update &lt;package&gt; Remove Tip : do not use -y to review what going to be removed sudo yum remove &lt;package&gt; Check groups yum group list Check packages associate with a group for example Development Tools group yum group info &quot;Development Tool&quot; Install in a particular group yum group install &quot;Development Tool Log can be checked on /var/log/dnf.rpm.log and we can see the history yum history We can undo a transaction on history sudo yum history undo &lt;number_of_line&gt; Tip List if the package are installed sudo yum list httpd mod_ssl Install ^list^-y install 2.14.3 Enabling Yum Software Repositories List the repositories that we are connected and repo definition path are on /etc/yum.repos.d/ yum repolist all Enable and disable repos using yum-config-manager yum-config-manager --disable &lt;repo_name&gt; yum-config-manager --enable &lt;repo_name&gt; List subscription manager repos list subscription-manager-repos --list 2.14.4 Managing Packages Module Streams Module is a set of RPM packages List modules and get info yum module list &lt;package/module like perl&gt; yum module info &lt;package/module like perl&gt; # List installed modules yum module list --installed Install module stream yum module install &lt;package/module like perl&gt; Remove stream yum module remove &lt;package/module like perl&gt; Disable stream yum module disable perl To add we can use the structure [BASE] name=Base baseurl=httpd://...... gpgcheck=0 Also if we have config-manager yum config-manager --add-repo &lt;URL&gt; 2.15 Accessing Linux File Systems df -h : overview of various file systems blkid /dev/vda1 : show the UUID (universally-unique identifier) of file system findmnt : tree overview of file system starting at roots lsblk : show overview of the various block devices like /dev/vda or /dev/sda lsblk -fp /dev/vdb check the UUID lsblk #Output NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 931,5G 0 disk ├─sda1 8:1 0 512M 0 part /boot/efi ├─sda2 8:2 0 915,1G 0 part /run/timeshift/backup └─sda3 8:3 0 15,9G 0 part [SWAP] Check the entire disk $ls -l /dev/sda brw-rw---- 1 root disk 8, 0 Aug 22 08:50 /dev/sda [SWAP] b : Indicate that this is a block based device du -sh : check space that has being occupied by a directory s : summary h : human read format $du -sh /var/log 1,3G /var/log 2.15.1 Mount and Unmounting File systems To Mount Check the UUID of block device using blkid /dev/vb1 for instance Create directory : mkdir -p &lt;path&gt; Mount : mount UUID=\"XXX\" /&lt;path&gt; Check the return code echo $? (0 means successfully) Check with df -h or mount command To Unmounting umount &lt;/dev/vdb1&gt; 2.15.2 Locating Files on the System find Basic find syntax : find &lt;where&gt; &lt;how&gt; &lt;what is lokking for&gt; Some examples : find / -name ssh_config : looking for particular name find / -iname ssh_config: using i going to use case insensitive search find /usr -iname \"*.pdf\" : looking all files that end with pdf find / -user &lt;user_name&gt; : find files for a particular user find / -user &lt;user_name&gt; -delete : find and delete find / -type f -user rick -size 10M : find files from user rick with 10MB (+10M more than 10MB , -10M less than 10MB) find /home -size +10M -exec ls -lh {} \\; : find on home files more than 10MB and list with ls -lh find /home -size +10M iname \"*.mkv\" -exec rm -f {} \\; : find files on home with more than 10MB ending with .mkv and execute rm -f on each file, similar find /home -size +10M iname \"*.mkv\" -delete` find /home -type f -perm /111 : find files on home that have execute permission (111) find /home -min -60 : find files modified in last 60min find / -user &lt;user_name&gt; -exec -cp {} /&lt;path_copy&gt;/ \\; locate pre-req : run the command updatedb to create an index of the files on file system locate &lt;file&gt; : basic locate a file 2.16 Analysing Server and Getting Support cockpit service used to analysing and managing remote servers, this is a web user interface that use TCP 9090 Enable : systemctl enable --now cokpit.socket Reload firewall daemon and set permanent : firewall-cm --add-service cockpit --permanent and firewall-cmd --reload link : https://:9090/system 2.16.1 Deteting and Resolving issues with Red Hat Insights It is a form of AI which is available as a software as a service Install Insights Register and attach the server on subscription-manager sudo subscription-manager register --auto-attach Install client Insights and register sudo yum install -y insights-client sudo insights-client --register Go to insights dashboard cloud.readhat.com/insight 2.17 Extra 2.17.1 Create 8G file disk Use dd command to create 8G disk file dd if=/dev/zero of=&lt;/path/diskfile.img&gt; bs=1M count=8192 create loop device loseup -fP diskfile.img list the device losetup -a 4.Can use new diskfile as a device fdisk /dev/loop0 2.17.2 Find users wit Set UID find / -perm /4000 -exec ls -l {} \\ 2&gt; /dev/null 2.17.3 Show dump device xxd -l 512 /dev/sda | less 2.17.4 Configure labels of FS tune2fs -L &lt;label&gt; /dev/sda&lt;number&gt; "],["server-administrator-ii---rh134.html", "# 3 Server Administrator II - RH134 3.1 Improving Command Line Productivity 3.2 Scheduling Future Tasks 3.3 Tuning System Performance 3.4 Controling Access to Files with ACLs 3.5 Managing SELinux Security 3.6 Managing Basic Storage 3.7 Managing Logical Volumes 3.8 Implementing Advanced Storage Features 3.9 Accessing Network-Attached Storage 3.10 Controlling The Boot Process 3.11 Managing Network Security 3.12 Installing Red Hat Enterprise Linux 3.13 Running Containers", " # 3 Server Administrator II - RH134 3.1 Improving Command Line Productivity Writing Simple Bash Scripts The Bash interpreter start in the first line of script with #! /bin/bash and to run the script we need : Config the execute permission on script Have the script on $HOME/bin and call the script name or any other bin path inside your $PATH LOOPS Syntax of bash for loop for &lt;VARIABLE&gt; in &lt;LIST&gt;; do &lt;COMMAND&gt; &lt;VARIABLE&gt; done Sample: #1 for HOST in host1 host2 host3; do echo $HOST; done #2 for HOST in host{1,2,3}; do echo $HOST; done #3 for HOST in host{1..3}; do echo $HOST; done #4 for FILE in file* ; do ls $FILE; done #5 for FILE in file{a..c}; do ls $FILE; done #6 for PACKAGE in $(rpm -qa | grep kernel); \\ do echo &quot;$PACKAGE was installed on \\ $(date -d @$(rpm -q -qqf &quot;%{INSTALLTIME} \\n&quot; $PACKAGE))&quot;; done Sequence Sample : #1 seq 2 2 10 output 2 4 6 8 10 #2 for EVEN in $(seq 2 2 10); do &quot;$EVEN&quot;;done Exit Codes 0 : successfully 0 : not successfully 0 to 255 : range To display the exit code echo $? IF / THEN #1 if &lt;condition&gt;; then &lt;STATEMENT&gt; ... &lt;STATEMENT&gt; else &lt;STATEMENT&gt; ... &lt;STATEMENT&gt; fi #2 if &lt;condition&gt;; then &lt;STATEMENT&gt; ... &lt;STATEMENT&gt; elif &lt;condition&gt;; then &lt;STATEMENT&gt; ... &lt;STATEMENT&gt; else &lt;STATEMENT&gt; fi Sample systemctl is-active psacct &gt; /dev/null 2&gt;&amp;1 if [ $? -ne 0 ] ; then sudo systemctl start psacct else sudo systemctl stop psacct fi Regex to match text in command outputs Regex can be used with command as vim, grep and less There are several options below some samples : Show me lines that starts with cat using ^ grep &#39;^cat&#39; &lt;FILE&gt; Show me lines that ends with cat using $ grep &#39;cat$&#39; &lt;FILE&gt; Show me lines that start and end with cat using ^ and $ grep &#39;^cat$&#39; &lt;FILE&gt; Whildcard for any character c.t grep &#39;^c.t$&#39; &lt;FILE&gt; Output cat cit cot cpt cst Show me lines with one character of list in [xxx] grep &#39;^c[aou]t$&#39; &lt;FILE&gt; output cat cot cut Show me lines with c + &lt;characters&gt; + t grep &#39;c.*t&#39; &lt;FILE&gt; output cat zuchetto zymochemistry Show me lines with c + &lt;2 characters&gt; + t Simple way ‘c..t’ but we can use modifiers : grep &#39;c.\\{2\\}t&#39; &lt;FILE&gt; #output Yacolt Zacynthus zoocultural Show me lines that starts with c + &lt;starts with 2 up to 3 characters&gt; + t grep &#39;c.\\{2,3\\}t&#39; &lt;FILE&gt; #output zirconate zoophysicist zuchetto zygocactus zoocyst grep Options -i case insensitivity -v invert the search -E search for ln and nx in particular file grep -E 'ln|nx file' Do not show lines that starts with # ^# or blank lines ^$ grep -vE &#39;^#|^$&#39; file Do not show lines that starts with # or ; grep -v &#39;^[#;]&#39; file 3.2 Scheduling Future Tasks One time Job : at command at TIMESPEC command to schedule a new job Samples at now + 5min at teatime tomorrow (teatime is 16:00) at noon + 4 days at 5pm august 3 2021 Cool options -g specify a queue g , queue goes a to z *atq to check the jobs schedules on my queue To inspect all info about job at -c &lt;job number&gt; To remove a job atrm &lt;job number&gt; To monitor a job or queue watch atq Recurring Jobs cron command USER Basic commands : crontab -l : list the jobs for the current user crontab -r : Remove all jobs from current user crontab -e : Edit jobs crontab &lt;filename&gt; : Remove all jobs and replace with the job read from filename, if no file is specified stdin is used Once the job is schedule will have a file on /var/spool/cron/ To check the structure of contrab schedule we can check the file /etc/crontab cat /etc/crontab #output SHELL=/bin/sh PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin # Example of job definition: # .---------------- minute (0 - 59) # | .------------- hour (0 - 23) # | | .---------- day of month (1 - 31) # | | | .------- month (1 - 12) OR jan,feb,mar,apr ... # | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat # | | | | | # * * * * * user-name command to be executed Other options # every 5min */5 * * * * &lt;xxx&gt; # every 5min, between 9am and 5pm Sun and Wed in July */5 9-16 * Jul sun, wed Logs : Can check the cron jobs logs on /var/log/cron SYSTEM cron jobs There are some ways to have a system cron jobs /etc/crontab Simple add the job on file /etc/cron.d/ Add a file with job schedule on this path /etc/cron/ run the command to check ls /etc/cron\\.* we can have folders with cron job files or scripts The backup mechanism of cron files is anacron and it is configured on /etc/anacrontab and the purpose is make sure that all important jobs always run There are different files on /var/spool/anacron for each daily, weekly and monthly jobs with timestamps based on schedule if there were a job that should be perform and machine is offline the job will be triggered Systemd Timer Unit This is a new scheduling function introduced on RHEL7, sample : The sysstat package provides a systemd timer unit called sysstat-collect.timer to collect system statistics every 10 minutes. The following output shows the configuration lines of /usr/lib/systemd/system/sysstat-collect.timer. ...output omitted... [Unit] Description=Run system activity accounting tool every 10 minutes [Timer] OnCalendar=*:00/10 [Install] WantedBy=sysstat.service To check the timers on system systemctl --type timer If need to modify we can do on /etc/systemd/system and after change the timer unit config files need to reload the daemon and activate the timer unit # Reload systemctl daemon-reload # Activate systemctl enable --now &lt;unitname&gt;.timr Managing Temporary Files We can configure timers that manages temporary files. Some applications use /tmp to hold temp data Others use use specific locations such as daemon and user-specific volatile dirs under /run, when system reboot those volatile store will be gone. The tool systemd-tmpfiles provide structured and configurable method to manage temp dirs and files When systemd starts a system, one of the first service units launched is systemd-tmpfiles-setup. This service runs the command systemd-tmpfiles --create --remove This command reads configuration files from : /usr/lib/tmpfiles.d/*.conf, /run/tmpfiles.d/*.conf, and /etc/tmpfiles.d/*.conf. Any files and directories marked for deletion in those configuration files is removed, and any files and directories marked for creation (or permission fixes) will be created with the correct permissions if necessary. Cleaning Temporary Files with a Systemd Timer The systemd timer unit called systemd-tmpfiles-clean.timer triggered systemd-tmpfiles-clean.service on regular interval, which executes the command to clean systemd-tmpfiles --clean To view the contents of the systemd-tmpfiles-clean.timer config files: systemctl cat systemd-tmpfiles-clean.timer If need to check the parameter frequence of clean up we need to make sure to reload and enable the timer systemctl daemon-reload systemctl enable --now systemd-tmpfiles.clean.timer Cleaning Temporary Files Manually Command : systemd-tmpfiles --clean this command wll purge all files which have not been accessed, changed, or modified more recently than max age defined on config file The format of config file systemd-tmpfiles is detailed in the tmpfiles.d manual page Sample: create the /run/systemd/seats directory if it does not yet exist, owned by the user root and the group root, with permissions set to rwxr-xr-x. This directory will not be automatically purged. #Type, Path, Mode, UID, GID, Age, and Argument d /run/systemd/seats 0755 root root - Create the /home/student directory if it does not yet exist. If it does exist, empty it of all contents. When systemd-tmpfiles –clean is run, remove all files which have not been accessed, changed, or modified in more than one day. #Type, Path, Mode, UID, GID, Age, and Argument D /home/student 0700 student student 1d Create the symbolic link /run/fstablink pointing to /etc/fstab. #Never automatically purge this line. #Type, Path, Mode, UID, GID, Age, and Argument L /run/fstablink - root root - /etc/fstab Configuration File Precedence The config file can exists in three places: /etc/tmpfiles.d/*.conf provided by the relevant RPM packages, should not edit /run/tmpfiles.d/*.conf volatile files, used by daemons /usr/lib/tmpfiles.d/*.conf 3.3 Tuning System Performance tuned daemon allow us optmize system performance by selection a tunning profile To install and enable tuned yum install tuned systemctl enable --now tuned 3.3.1 Profiles balanced : Ideal for systems that require a compromise between power saving and performance. desktop : Derived from the balanced profile. Provides faster response of interactive applications. throughput-performance : Tunes the system for maximum throughput. latency-performance : Ideal for server systems that require low latency at the expense of power consumption. network-latency : Derived from the latency-performance profile. It enables additional network tuning parameters to provide low network latency. network-throughput : Derived from the throughput-performance profile. Additional network tuning parameters are applied for maximum network throughput. powersave : Tunes the system for maximum power saving. oracle : Optimized for Oracle database loads based on the throughput-performance profile. virtual-guest : Tunes the system for maximum performance if it runs on a virtual machine. virtual-host : Tunes the system for maximum performance if it acts as a host for virtual machines 3.3.2 Managing profiles from command line To active tuned-adm active To List all available profiles tuned-adm list To switch the active profile to a different one tuned-adm profile &lt;profile_name&gt; tuned-adm active To have a recommendation of profile tuned-adm recommened To deactivate tuned-adm off tuned-adm active 3.3.3 Influencing Process Scheduling Prioritize or de-prioritize specific process with nice and renice Nice values 19 : Nicest (lowest priority) 0 : Neutral -20: Least nice (highest priority) Display Nice Levels from the command line ps axo pid, comm, nice, cls --sort=-nice Start process with different Nice levels # default nice is 10 nice sha1sum /dev/zero &amp; # setting to 15 nice -n 15 sha1sum &amp; Change the Nice level of existing process renice -n &lt;level&gt; &lt;process number&gt; 3.4 Controling Access to Files with ACLs ACLs Access Control List are extention of permissions To check if file have ACL we going to see a + on long list ouput -rwxrw----+ 1 user operators 130 Mar 19 23:56 reports.txt The group permission on ls -l is masked, will not be the real permission of this file need to check ACL settings Changing group permissions on a file with an ACL by using chmod does not change the group owner permissions, but does change the ACL mask. Use setfacl -m g::perms file if the intent is to update the file’s group owner permissions. View File or Directory ACLs using getfacl command [user@host content]$ getfacl reports.txt # file: reports.txt # owner: user # group: operators user::rwx user:consultant3:--- user:1005:rwx #effective:rw- group::rwx #effective:rw- group:consultant1:r-- group:2210:rwx #effective:rw- mask::rw- other::--- The ACL MASK Defines the maximum permissions that you can grant. It does not restrict permissions of the file owner or other user. all files and directories that implement ACL will have an ACL mask. By default , the mask is recalculated whenever any of the affected ACLs are added, modified or deleted Changing ACL file permissions setfacl $ setfacl -m u:name:rX file -m : modify , x delete u : user, g for group, o for others name: name of user rX : permission X uppercase can be used to indicate that execute permission should only be set on dir and not regular files, unless the file already has the relevant execute permission. ACL recursive modifications $ setfacl -R -m u:name:rX directory Deleting ACL $ setfacl -x u:name,g:name file Delete all ACL entries $ setfacl -b file 3.5 Managing SELinux Security Security Enhanced Linux (SELinux) is an additional layer of system security. The primary goal of SELinux is to protect user data from system services that have been compromised Targeted policy default SELinux consists of sets of policies, defined by the application developers, that declare exactly what actions and accesses are proper and allowed for each binary executable, configuration file, and data file used by an application. Modes Enforcing : SELinux is enforcing access control rules. Computers generally run in this mode. Permissive : SELinux is active but instead of enforcing access control rules, it records warnings of rules that have been violated. This mode is used primarily for testing and troubleshooting Disabled : SELinux is turned off entirely: no SELinux violations are denied, nor even recorded. Discouraged! To show SELinux information ls -lZ Checking the current state [user@host ~]# getenforce Enforcing Check the persistently state cat /etc/selinux/config Set enforce mode [user@host ~]# setenforce usage: setenforce [ Enforcing | Permissive | 1 | 0 ] [user@host ~]# setenforce 0 [user@host ~]# getenforce Permissive Check status sestatus 3.5.1 Controlling SELinux File Contexts When copy a file or create a new file the file inherits the SELinux context of the directory If I move a file it retain the SELinux context Change context of a file Commands semanage : create the role, a create page is man semanage-fcontext , sample of rule : # create the rule semanage fcontext -a -t httpd_sys_content_t &quot;/web(/.*)?&quot; # apply a new rule restorecon -R -v /web fcontext : used to list and see changes # l : list # C : change semage fcontext -lC restorecon : use when the file already in the correct location, for example, if someone moved the file from A to B and B is the correct location, using the command restorecon &lt;file&gt; will restore the context chcon : It is not persistent , does not survive restorecon or relable, avoid 3.5.2 Adjusting SELinux Policy with Booleans SELinux booleans are switches that change the behavior of the SELinux policy. SELinux booleans are rules that can be enabled or disabled. Check booleans [user@host ~]$ getsebool -a abrt_anon_write --&gt; off abrt_handle_event --&gt; off abrt_upload_watch_anon_write --&gt; on antivirus_can_scan_system --&gt; off antivirus_use_jit --&gt; off ...output omitted... [user@host ~]$ getsebool httpd_enable_homedirs httpd_enable_homedirs --&gt; off # list booleans [user@host ~]$ sudo semanage boolean -l | grep httpd_enable_homedirs httpd_enable_homedirs (on,on) Allow httpd to enable homedirs write pending values to policy, P(persistence) setsebool -P httpd_enable_homedirs on list booleans with current state and diff from default state [user@host ~]$ sudo semanage boolean -l -C SELinux boolean State Default Description cron_can_relabel (off,on) Allow cron to can relab 3.5.3 Investigating and REsolving SELinux issues sealert display info during SELinux troubleshooting Guide to troubleshooting SELinux issues: Before thinking of making any adjustments, consider that SELinux may be doing its job correctly by prohibiting the attempted access. The most common SELinux issue is an incorrect file context, “when we move files”. Another remedy for overly restrictive access could be the adjustment of a Boolean. It is possible that the SELinux policy has a bug that prevents a legitimate access. Monitor SELinux Violations If we have the package setroubleshoot-serve installed /var/log/audit/audit.log : received log messages related SELinux violations /var/log/messages : short summary of SELinux violations messages 3.6 Managing Basic Storage 3.6.1 Adding Partition, File Systems and Persistent Mounts MBR Partitioning Schema The Master Boot Record (MBR) partitioning schema dictated how disks are partitioned on system running BIOS firmware Can have manimum of four primary partition Maximum disk and partition size of 2TiB Using extended and logical partitions we can create a maximum of 15 partitions GPT Partitioning Schema The Unified Extensible Firmware Interface (UEFI) firmware is a standard laying out Does not have limit of 2TB Maximum of 128 partitions Maximum of 8ZiB zebibytes have a backup on the end of the disk Managing Partitions with Parted Partition editors are programs which allow admin to make changes to a disk’s parted command Display info of /dev/vda with subcommand print if do not provide subcommand will open an interactive session we can change the display to KB, MB, GB, TB or S for sector using unit s argument parted makes the change immediately [root@host ~]# parted /dev/vda print Model: Virtio Block Device (virtblk) Disk /dev/vda: 53.7GB Sector size (logical/physical): 512B/512B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 1049kB 10.7GB 10.7GB primary xfs boot 2 10.7GB 53.7GB 42.9GB primary xfs Writing the partition table on a NEW DISK MBR [root@host ~]# parted /dev/vdb mklabel msdos GPT [root@host ~]# parted /dev/vdb mklabel gpt The mklabel subcommand wipes the existing partition table. Only use mklabel when the intent is to reuse the disk without regard to the existing data. If a new label changes the partition boundaries, all data in existing file systems will become inaccessible. Creating MBR Partitions Specify the disk device to create the partition on using parted # parted /dev/vdb Use the mpart subcommand to create new primary or extented partition (parted) mkpart Partition type? primary/extended? primary Indicate the file-system type, to get the list of all types use parted /dev/vdb help mkpart File system type? [ext2]? xfs Specify the sector on disk Start? 2048s Specify where the new partition will end End? 1000MB Exit using quit Run the udevadm settle for the system detect the new partition Option: if we have all the details we can use only one command to perform all actions [root@host ~]# parted /dev/vdb mkpart primary xfs 2048s 1000MB Creating GPT Partitions Specificy the disk device to create the partition on # parted /dev/vdb Use the mkpart to start creating the new partition, with GPT each partition is given a name (parted) mkpart Partition name? []? usersdata Indicate the type File system type? [ext2]? xfs Specify the sector on disk that the new partition starts on Start? 2048s Specify the end End? 1000MB Exit using quit Run the udevadm settle for the system detect the new partition Option: if we have all the details we can use only one command to perform all actions [root@host ~]# parted /dev/vdb mkpart usersdata xfs 2048s 1000MB Deleting Partitions Specify the disk [root@host ~]# parted /dev/vdb Identify the partition number of the partition to delete (parted) print Delete using rm subcomand The rm subcommand immediately deletes the partition from the partition table on the disk. (parted) rm &lt;number&gt; Exit using quit Creating File Systems Check if fdisk for MBR and gdisk for GPT are installed $ which fdisk # check the package $ rpm -qf /sbin/fdisk # install gdisk yum -y install gdisk Sample of using gdisk $ gdisk /dev/vdd ? : help p : print n : new partition Specify the partition number, First sector and Last sector or size Specify the GUID Label L : show all label codes print to check c : change the partition name w : write Confirm Check with lsblk Create the file system using mkfs.xfs $ mkfs.xfs /dev/vdb1 To check the file system creation using blkid will show the UUID $ blkid To mount temporarily mount mount /dev/vdb1 /mnt Persistently mount update /etc/fstab Reload the daemon systemctl daemon-reload check fs lsblk --fs create the dir mkdir &lt;dir&gt; mount the fs mount &lt;fs&gt; Add label to FS tune2fs -L &lt;label&gt; /dev/sda&lt;number&gt; Show dump device xxd -l 512 /dev/sda | less 3.6.2 Managing Swap Space A swap space is an area of a disk under the control of the Linux kernel memory management subsystem. The kernel uses swap space to supplement the system RAM by holding inactive pages of memory. The combined system RAM plus swap space is called virtual memory. Creating a Swap Space Create a partition with a file system type of linux-swap Run udevadm settle Formatting the device mkswap /dev/vdb2 Add on /etc/fstab #sample UUID=39e2667a-9458-42fe-9665-c5c854605881 swap swap defaults 0 0 Activate the swap swappon /dev/vdb2 # activate all the swap spaces swapon -a # check swap swapon -s Setting Swap Space Priority default is -2 Update /etc/fstab and specify pri=priority number instead of defaults 3.7 Managing Logical Volumes 3.7.1 Creating Logical Volumes Using Logical volumes is easier to manage disk space, we can allocate to logical volume free space from volume group and file system can be resized LVM Definitions Physical devices Storage devices used to save data stored in a lofical volume Could be a disk partition, whole disk, RAID arrays or SAN Disk Device must be initialized as an LVM Physical volumes (PVs) “Physical” storage used with LVM We must initilize a device as a physical volume before use as LVM PV can only be allocated to a single VG Volume group (VGs) Storage pool made up of one or more physical volumes A VG can consiste of unused space an any number of logical Volumes Logical volumes (LVs) Create from free physical extents in a volume group and provide the storage used by applications, users and OS Collection of logical extents (LEs), which map to physical extents Steps to create logical volumes Summary: 1. Create partition 2. pvcreate 3. vgcreate 4. lvcreate 5. mkfs or mkswap 6. mount Use lsblk, blkid or cat /proc/partition to identify the devices Prepare the physical device Use parted, gdisk or fdisk tp create a new partition for use with LVM Type of Linux LVM on LVM partitions Use 0x8e for MBR Use partprobe to register the new partition with the kernel Create a physical volume pvcreate to label the partition as a physical volume, it divides the physical volume into physical extents (PEs) of fixed size of 4MB block. pvcreate /dev/vdb2 /dev/vdb1 Create the volume group vgcreate used to collect one or more physical volumes into a volume group. It is equivalent of hard disk -s option specify the extend size vgcreate vg01 /dev/vdb2 /dev/vdb1 -s 4M This creates a VG called vg01 that is the combined size, in PE units, of the two PVs /dev/vdb2 and /dev/vdb1 Create a logical volume Use lvcreate to create a new logical volume from available physical extents in a volume group -n : to set the LV name -L : to set the LV size in bytes -l : to set the LV size in extents lvcreate -n lv01 -L 700M vg01 Add the file system Use mkfs to create an XFS file system on the new logical volume # create FS mkfs -t xfs /dev/vg01/lv01 # create dir mount point mkdir /mnt/data # Update /etc/fstab /dev/vg01/lv01 /mnt/data xfs defaults 1 2 # mount mount /mnt/data Remove a Logical Volume Umount the fs and remove the info from /etc/fstab umount /mnt/data Remove the logical volume lvremove /dev/vg01/lv01 Remove the volume group vgremove vg01 Remove the physical volumes pvremove /dev/vdb2 /dev/vdb1 Review LVM Status Info Physical Volumes pvdisplay /dev/vdb1 Volumes Groups vgdisplay vg01 Logical Volumes lvdisplay /dev/vg01/lv01 3.7.2 Extending and Reducing Logical Volumes Extending Volume groups : We can add more disk space to a volume group by adding additional physical volumes. Then assign the new physical extents to logical volumes. Reducing the volume group : We also can remove unused physical volume from a volume group * First use pvmove to move data from extents on one physical volume to extents on another physical extents Extending a Volume group Prepare the physical device and create the physical volume [root@host ~]# parted -s /dev/vdb mkpart primary 1027MiB 1539MiB [root@host ~]# parted -s /dev/vdb set 3 lvm on [root@host ~]# pvcreate /dev/vdb3 A PV only needs to be created if there are no PVs free to extend the VG. Use vgextend to add the new physical volume to the volume group [root@host ~]# vgextend vg01 /dev/vdb3 vgdisplay to confirm the additional physical extents are available [root@host ~]# vgdisplay vg01 Inspect the Free PE / Size Usually after that you allocated the new space on FS, the -r option will extend the file system lvextend -r -L 100G /dev/volume_name/lv_name To allocated all free space can use -l +100%FREE Reducing a Volume Group Use pvmode PV_DEVICE_NAME to relocate any physical extents [root@host ~]# pvmove /dev/vdb3 This command moves the PEs from /dev/vdb3 to other PVs with free PEs in the same VG. Always backup the data before pvmove Reduce the volume using vgreduce [root@host ~]# vgreduce vg01 /dev/vdb3 This removes the /dev/vdb3 PV from the vg01 VG and it can now be added to another VG. Alternatively, pvremove can be used to permanently stop using the device as a PV Extending a Logical Volume and XFS File System Verify that the volume group has space available. [root@host ~]# vgdisplay vg01 Extend the logical volume with lvextendLV_DEVICE_NAME [root@host ~]# lvextend -L +300M /dev/vg01/lv01 Extend the file system using xfs_growfs mountpoint [root@host ~]# xfs_growfs /mnt/data Verify the new size of file system [root@host ~]# df -h /mountpoint Extending a Logical Volume and ext4 File System Verify that the volume group has space available. [root@host ~]# vgdisplay vg01 Extend the logical volume with lvextend LV_DEVICE_NAME [root@host ~]# lvextend -L +300M /dev/vg01/lv01 Extend the file system [root@host ~]# resize2fs /dev/vg01/lv01 Extend a logical volume and swap space Verify that the volume group has space available. [root@host ~]# vgdisplay vg01 Deactivate the swap space. swapoff -v /dev/vgname/lvname Extend the logical volume with lvextend LV_DEVICE_NAME [root@host ~]# lvextend -L +300M /dev/vg01/lv01 Format the logical volume as swap space. mkswap /dev/vgname/lvname Activate the swap space swapon -va /dev/vgname/lvname 3.8 Implementing Advanced Storage Features 3.8.1 Managing Storage with Stratis STRATIS is a new storage-management solution for Linux, runs as a service that manages pools of physical storage devices and transparently creates and manages volumes for the newly created file system. Instead of immediately allocating physical storage space to the file system when it is created, Stratis dynamically allocates that space from the pool as the file system stores more data We can create multiple pools from different storage devices. File systems created by Stratis should only be reconfigured with Stratis tools and commands. To use we need to install the stratis-cli and stratisd To install Stratis [root@host ~]# yum install stratis-cli stratisd [root@host ~]# systemctl enable --now stratisd Create pools of one or more block devices using the stratis pool create command. [root@host ~]# stratis pool create pool1 /dev/vdb To view the list of available pools [root@host ~]# stratis pool list To add additional block devices to a pool [root@host ~]# stratis pool add-data pool1 /dev/vdc To view the block devices of a pool [root@host ~]# stratis blockdev list pool1 To create a file system from a pool [root@host ~]# stratis filesystem create pool1 fs1 To view the list of available file systems [root@host ~]# stratis filesystem list To create a snapshot [root@host ~]# stratis filesystem snapshot pool1 fs1 snapshot1 To mount the Stratis file system persistently # get UID [root@host ~]# lsblk --output=UUID /stratis/pool1/fs1 # add on /etc/fstab UUID=31b9363b-add8-4b46-a4bf-c199cd478c55 /dir1 xfs defaults,x-systemd.requires=stratisd.service 0 0 The x-systemd.requires=stratisd.service mount option delays mounting the file system until after systemd starts the stratisd.service during the boot process 3.8.2 Compressing and Deduplicating Storage with VDO Virtual Data Optimizer (VDO) is a Linux device mapper driver that reduces disk space usage on block devices, and minimizes the replication of data. Kernel modules kvdo : Control data compression uds : Deduplication VDO phases to reduce the footprint on storage Zero-block Elimination filter out data that contain only zeros Deduplication eliminate redudant blocks Compression the kvdo compress the data block using LZ4 Install the vdo and kmod-kvdo [root@host ~]# yum install vdo kmod-kvdo Check status of vdo service systemctl status vdo Creating VDO volume [root@host ~]# vdo create --name=vdo1 --device=/dev/vdd --vdoLogicalSize=50G Analyzing a VDO Volume [root@host ~]# vdo status --name=vdo1 Display the list of VDO vdo list Stop / Start vdo vdo stop vdo start When the logical size of a VDO volume is more than the actual physical size, you should proactively monitor the volume statistics to view the actual usage using the vdostats –verbose command. vdostats --human-readable TIPS To test we might need to create a 2G file, below a sample dd if=/dev/urandom of=&lt;path&gt;/&lt;file_name&gt; bs=1M count=2048 If made a mistake on fstab and need to access the server with root and root home dir is mounted as read only we can remount using below command mount -o remount, rw / 3.9 Accessing Network-Attached Storage 3.9.1 Mounting Network-Attached Storage with NFS NFS servers export shares (directories). NFS clients mount an exported share to a local mount point (directory), which must exist. NFS shares can be mounted a number of ways: Manually, using the mount command Automatically at boot time using /etc/fstab On demand, using autofs or systemd.automount. Mounting NFS Shares Identify NFS Shares [user@host ~]$ sudo mkdir mountpoint [user@host ~]$ sudo mount serverb:/ mountpoint [user@host ~]$ sudo ls mountpoint Mount point use mkdir to create a mount point [user@host ~]$ mkdir -p mountpoint Mount Temporarily [user@host ~]$ sudo mount -t nfs -o rw,sync serverb:/share mountpoint -t nfs : file system type for NFS Share -o sync: immediately sync write operations with server Persistently Configure /etc/fstab [user@host ~]$ sudo vim /etc/fstab serverb:/share /mountpoint nfs rw,soft 0 0 Mount the NFS Share [user@host ~]$ sudo mount /mountpoint Unmounting NFS Shares [user@host ~]$ sudo umount mountpoint 3.9.2 Automounting Network-Attached Storage The automounter is a service (autofs) that automatically mounts NFS shares “on-demand”, and will automatically unmount NFS shares when they are no longer being used. Create an automount Install autofs [user@host ~]$ sudo yum install autofs Add a master map file to /etc/auto.master.d [user@host ~]$ sudo vim /etc/auto.master.d/demo.autofs ## add /shares /etc/auto.demo Create the mapping files [user@host ~]$ sudo vim /etc/auto.demo # add work -rw,sync serverb:/shares/work Start and enable the automounter service [user@host ~]$ sudo systemctl enable --now autofs Direct Maps Direct maps are used to map an NFS share to an existing absolute path mount point. To use the master map file should be like : /- /etc/auto.direct The content for the /etc/auto.direct will be : /mnt/docs -rw,sync serverb:/shares/docs Indirect Wildcard Maps * -rw,sync serverb:/shares/&amp; 3.10 Controlling The Boot Process 3.10.1 Selecting the boot target Describing the Red Hat Enterprise Linux 8 Boot Process The machine is powered on the UEFI or BIOS runs Power On Self Test (POST) and start, F2 or some system Esc usually to access configuartion and E to edit Firmware search for a bootable device The firmware read a boot loader and passes control of system to the boot loader, in RHEL 8 GRand Unified Bootloader version 2 (GRUB2) configured using grub2-install GRUB2 load the config from /boot/grub2/grub.cfg and display the menu, where we can select the kernel. Configured using the /etc/grub.d/ directory, the /etc/default/grub file, and the grub2-mkconfig command to generate the /boot/grub2/grub.cfg file After timeout or select the kernel boot loader load the kernel and initramfs set of initialization configured on /etc/dracut.conf.d/ using dracut and lsinitrd command Kernel initializes all hardware and execute the /sbin/init from initramfs as PID 1. The systemd execute all instances for the initrd.target such as mount root file system. Rebooting and Shutting Down systemctl poweroff : Stops all running services, unmounts all file system and power down the system systemctl reboot : Stops all running services, unmount al file system and then reboot the system systemctl halt : Stop the system, but do not power off the system Selecting a System Target systemd targets is a set of systemd units that system should start to reach the desired state graphical.target : supports multiple users, graphical-and text logins multi-user.target: system supports multiple users, text logins only rescue.target : sulogin prompt, basic system initialization completed emergency.target: sulogin prompt, initramfs pivot complete and system root mounted on / read only List dependencies for graphical.target systemctl list-dependencies graphical.target | grep target # output graphical.target ● └─multi-user.target ● ├─basic.target ● │ ├─paths.target ● │ ├─slices.target ● │ ├─sockets.target ● │ ├─sysinit.target ● │ │ ├─cryptsetup.target ● │ │ ├─local-fs.target ● │ │ └─swap.target ● │ └─timers.target ● ├─getty.target ● └─remote-fs.target To list the available targets : systemctl list-units --type=target --all Switch to a different target using systemctl isolate [root@host ~]# systemctl isolate multi-user.target Isolating a target stops all services not required by that target (and its dependencies), and starts any required services not yet started Get the default target [root@host ~]# systemctl get-default Setting a Default Target [root@host ~]# systemctl set-default graphical.target To select a different target at boot time Boot the system Interrupt the boot loader Move cursor to kernel entry Press e to edit Move the cursos to the line that starts with linux append the option for instance systemd.unit=emergency.target Ctrl+x to boot with changes 3.10.2 Resetting the Root Password Option 1 Boot the system using a Live CD Mount the root file system Edit /etc/shadow Option 2 Reboot the system Interrupt the boot loader Move the cursor to the kernel Press e to edit Move the cursor to kernel command line, starting with linux Append rd.break Press Ctrl+x to boot System will presents a root shell Change the root file system to read/write switch_root:/# mount -o remount,rw /sysroot Switch into a chroot switch_root:/# chroot /sysroot Set the new password passwd root run sh-4.4# touch /.autorelabel to include /etc/shadow Exit;Exit to system continue the boot Option 3 Reboot the system Interrupt the boot loader Move the cursor to the kernel Press e to edit Move the cursor to kernel command line, starting with linux Enable the debug using adding systemd.debug-shell option Press Ctrl+x to boot On login page Crtl+Alt+F9 and we will be on root shell Change the password Remove the debug sudo systemctl stop debug-shell.service 3.10.3 Repairing File system Issues at Boot Errors in /etc/fstab and corrupt file systems can stop a system from booting. Common File system issues: Corrupt file system Nonexistent device or UUID referenced in /etc/fstab Nonexistent mount point in / etc/fstab Incorrect mount option specified in /etc/fstab 3.10.4 Create or alter grub The default path is /etc/default/grub grup2-mkconfig -o /boot/grub2/grub.cfg reboot 3.11 Managing Network Security 3.11.1 Managing Server Firewall netfilter allows other kernel modules to interface with kernel’s network stack. Any incoming, outgoing, or forwarded network packet can be inspected, modified, dropped, or routed programmatically before reaching user space components or applications. nftables : a new filter and packet classification subsystem that has enhanced portions of netfilter’s code. Nftables uses the single nft user-space utility, allowing all protocol management to occur through a single interface, eliminating historical contention caused by diverse front ends and multiple netfilter interfaces. firewalld : a dynamic firewall manager. With firewalld, firewall management is simplified by classifying all network traffic into zones. Each zone has its own list of ports and services that are either open or closed. Pre-defined Zones all zones permit any incoming traffic which is part of a communication initiated by the system, and all outgoing traffic trusted home internal work public external dmz block drop Pre-defined Services These service definitions help you identify particular network services to configure. ssh dhcpv6-client ipp-client samba-client mdns To list services: firewall-cmd --get-services Configure the Firewall from the Command Line firwewall-cmd interacts with firewalld dynamic firewall manager, most of commands will work on the runtime config, unless the –permanent option is specified, we also must activate the setting using firewall-cmd –reload Some commanda: –get-default-zone : query the current default zone –set-default-zone=ZONE –get-zones –get-active-zones –add-source=CIDR [–zone=ZONE] –remove-source=CIDR [–zone=ZONE] –add-interface=INTERFACE [–zone=ZONE] —change-interface=INTERFACE [–zone=ZONE] –list-all [–zone=ZONE] –list-all-zones –reload Sample of commands setting the default zone to dmz, assign all traffic coming from the 192.168.0.0/24 network to the internal zone, and open the network ports from the mysql service on the internal zone [root@host ~]# firewall-cmd --set-default-zone=dmz [root@host ~]# firewall-cmd --permanent --zone=internal --add-source=192.168.0.0/24 [root@host ~]# firewall-cmd --permanent --zone=internal --add-service=mysql [root@host ~]# firewall-cmd --reload 3.11.2 Controlling SELinux Port Labeling One of the methods that SELinux uses for controlling network traffic is labeling network ports Get list of all current port label assignements semanage port -l To add a port to an existing port label (type) semanage port -a -t port_label -p tcp|udp PORTNUMBER To allow a gopher service to listen on port 71/TCP semanage port -a -t gopher_port_t -p tcp 71 To remove the binding of port 71/TCP to gopher_port_t: semanage port -d -t gopher_port_t -p tcp 71 To view local changes to the default policy semanage port -l -C To modify port 71/TCP from gopher_port_t to http_port_t semanage port -m -t http_port_t -p tcp 71 Service specific SELinux man pages found in the selinux-policy-doc package include documentation on SELinux types, booleans, and port types. If these man pages are not yet installed on your system, follow this procedure: [root@host ~]# yum -y install selinux-policy-doc [root@host ~]# man -k _selinux 3.12 Installing Red Hat Enterprise Linux 3.12.1 Installing Red Hat Enterprise Linux Supported processor architectures: x86 64-bit (AMD and Intel), IBM Power Systems (Little Endian), IBM Z, and ARM 64-bit. After downloading, create bootable installation media based on the instructions 3.12.2 Automating Installation with Kickstart Text file with specification of how the machine should be configured Using Kickstart, you specify everything Anaconda needs to complete an installation, including disk partitioning, network interface configuration, package selection, and other parameters, in a Kickstart text file. The %packages section specifies the software to be installed on the target system There are two additional sections, %pre and %post, which contain shell scripting commands that further configure the system. You must specify the primary Kickstart commands before the %pre, %post, and %packages sections, but otherwise, you can place these sections in any order in the file Sample: #version=RHEL8 ignoredisk --only-use=vda # System bootloader configuration bootloader --append=&quot;console=ttyS0 console=ttyS0,115200n8 no_timer_check net.ifnames=0 crashkernel=auto&quot; --location=mbr --timeout=1 --boot-drive=vda # Clear the Master Boot Record zerombr # Partition clearing information clearpart --all --initlabel # Use text mode install text repo --name=&quot;appstream&quot; --baseurl=http://classroom.example.com/content/rhel8.2/ x86_64/dvd/AppStream/ # Use network installation url --url=&quot;http://classroom.example.com/content/rhel8.2/x86_64/dvd/&quot; # Keyboard layouts # old format: keyboard us # new format: keyboard --vckeymap=us --xlayouts=&#39;&#39; # System language lang en_US.UTF-8 # Root password rootpw --plaintext redhat # System authorization information auth --enableshadow --passalgo=sha512 # SELinux configuration selinux --enforcing firstboot --disable # Do not configure the X Window System skipx # System services services --disabled=&quot;kdump,rhsmcertd&quot; --enabled=&quot;sshd,rngd,chronyd&quot; # System timezone timezone America/New_York --isUtc # Disk partitioning information part / --fstype=&quot;xfs&quot; --ondisk=vda --size=10000 %packages @core chrony cloud-init dracut-config-generic dracut-norescue firewalld grub2 kernel rsync tar -plymouth %end %post --erroronfail # For cloud images, &#39;eth0&#39; _is_ the predictable device name, since # we don&#39;t want to be tied to specific virtual (!) hardware rm -f /etc/udev/rules.d/70* ln -s /dev/null /etc/udev/rules.d/80-net-name-slot.rules # simple eth0 config, again not hard-coded to the build hardware cat &gt; /etc/sysconfig/network-scripts/ifcfg-eth0 &lt;&lt; EOF DEVICE=&quot;eth0&quot; BOOTPROTO=&quot;dhcp&quot; ONBOOT=&quot;yes&quot; TYPE=&quot;Ethernet&quot; USERCTL=&quot;yes&quot; PEERDNS=&quot;yes&quot; IPV6INIT=&quot;no&quot; EOF %end The Kickstart Generator website at presents dialog boxes for user inputs, and creates a Kickstart directives text file with the user’s choices. Each dialog box corresponds to the configurable items in the Anaconda installer. ksvalidator is a utility that checks for syntax errors in a Kickstart file. The pykickstart package provides ksvalidator. To find the provides of ksvalidation yum provides */ksvalidator We will see the pykickstart and install yum -y install pykickstart Checking for packages that have the kickstart in the name rpm -qad &#39;*kickstart&#39; ... ... /usr/share/doc/python3-kickstart/kickstart-docs.txt In this file kickstart-docs.txt we going to have all docs about kickstart To boot anaconda and point it to the kickstart file, press TAB and add inst.ks=LOCATION 3.12.3 Installing and Configuring Virtual Machine Red Hat Enterprise Linux 8 supports KVM (Kernel-based Virtual Machine), a full virtualization solution built into the standard Linux kernel. KVM can run multiple Windows and Linux guest operating systems virsh command is used to manage KVM Red Hat Virtualization (RHV) provides a centralized web interface that allows administrators to manage an entire virtual infrastructure Red Hat OpenStack Platform (RHOSP) provides the foundation to create, deploy, and scale a public or a private cloud Install the virtualizaation tools yum module list virt yum module install virt Verify the system requirements virt-host-validate To Manage virtual machines with cockpit Install the cockpit-machines package to add the Virtual Machines menu to Cockpit. yum install cockpit-machines Start and enable cockpit systemctl enable --now cockpit.socket 3.13 Running Containers 3.13.1 Intro Containers and Virtual Machines are different in the way they interact with hardware and the underlying operating system Virtualization: Enables multiple operating systems to run simultaneously on a single hardware platform. Uses a hypervisor to divide hardware into multiple virtual hardware systems, allowing multiple operating systems to run side by side Requires a complete operating system environment to support the application Containers: Run directly on the operating system, sharing hardware and OS resources across all containers on the system. This enables applications to stay lightweight and run swiftly in parallel. Share the same operating system kernel, isolate the containerized application processes from the rest of the system, and use any software compatible with that kernel Require far fewer hardware resources than virtual machines, which also makes them quick to start and stop and reduces storage requirements Running Containers from Container Images Container images are unchangeable, or immutable, files that include all the required code and dependencies to run a container Container images are built according to specifications, such as the Open Container Initiative (OCI) image format specification Managing Containers with Podman podman : manage containers and container image skopeo : used to inspect, copy, delete and sign images buildah: used to create new container images These tools are compatible with the Open Container Initiative (OCI). They can be used to manage any Linux containers created by OCI-compatible container engines, such as Docker 3.13.2 Running a Basic Conatiner The Red Hat Container Catalog provides a web-based interface that you can use to search these registries for certified content. The Container naming conventions registry_name/user_name/image_name:tag registry_name : name of the registry storing the image user_name : represents the user or organization to which the image belongs image_name : must be unique in the user namespace tag : image version Installing Container Managemnt Tools [root@host ~]# yum module install container-tools To pull or download the a container the image podman pull registry.access.redhat.com/ubi8/ubi:latest To retrieval, podman stores images podman images To run a container from image podman run -it registry.access.redhat.com/ubi8/ubi:latest To run and delete the image after use using podman run –rm [user@host ~]$ podman run --rm registry.access.redhat.com/ubi8/ubi cat /etc/os-release 3.13.3 Finding and Managing Container Images Podman uses a registreis.conf file : grep ^[^#] /etc/containers/registreis.conf podman info command displays configuration information for Podman podman info Search command to search conatiner registreis for a specific container image –no-trunc : option to see longer image descriptions podman search registry.redhat.io/rhel8 Inspect images skopeo inspect docker://registry.redhat.io/rhel8/python-36 podman inspect registry.redhat.io/rhel8/python-36 remove image podman rmi registry.redhat.io/rhel8/python-36:latest 3.13.4 Performing Advanced Container Management Mapping Container Hosts Ports to the container using -p [user@host ~]$ podman run -d -p 8000:8080 registry.redhat.io/rhel8/httpd-24 List the ports of cntainer [user@host ~]$ podman port -a Make sure that firewall on container allow external clients [root@host ~]# firewall-cmd --add-port=8000/tcp Check the logs of container podman logs &lt;container id&gt; Stop / restart podman [stop | restart] &lt;container id&gt; Delete container podman run &lt;container id&gt; Passing parameters with -e, -d to detach, -p to specify ports podman run -d --name container_name -e MYSQL_USER=user_name -e MYSQL_PASSWORD=user_password -e MYSQL_DATABASE=database_name -e MYSQL_ROOT_PASSWORD=mysql_root_password -p 3306:3306 registry.redhat.io/rhel8/mariadb-103:1-102 # checking mysql -h127.0.0.1 -udupsy -pbongle -P3306 show databases; 3.13.5 Attaching Persistent Storage to a Container Mounting Volume --volume host_dir:container_dir:Z For example, to use the /home/user/dbfiles host directory for MariaDB database files as /var/lib/mysql inside the container, use the following command. The following podman run command is very long and should be entered as a single line. podman run -d --name mydb -v /home/user/dbfiles:/var/lib/mysql:Z -e MYSQL_USER=user -e MYSQL_PASSWORD=redhat -e MYSQL_DATABASE=inventory registry.redhat.io/rhel8/mariadb-103:1-102 The dir must have the SELinux context container_file_t mke sure to use :Z 3.13.6 Managing Containers as Service First step is open a new session with user ssh user@localhost Enable linger loginctl enable-linger &lt;user&gt; # to check loginctl show-user user Create a dir to store the unit files mkdir -pv ~/.config/systemd/user/ Generate the service podman generate systemd --name &lt;name of service&gt; --files --new use --user to control new user services [user@host ~]$ systemctl --user daemon-reload [user@host ~]$ systemctl --user enable &lt;name of service&gt; [user@host ~]$ systemctl --user start &lt;name of service&gt; "],["server-administrator-iii---rh294.html", "# 4 Server Administrator III - RH294 4.1 Intro to Ansible 4.2 Deploying Anisble and Implementing Playbooks 4.3 Managing Variables and Facts 4.4 Implementing Task Control", " # 4 Server Administrator III - RH294 4.1 Intro to Ansible Ansible is an open source automation platform. It is a simple automation language that can perfectly describe an IT application infrastructure in Ansible Playbooks. It is also an automation engine that runs Ansible Playbooks. 4.1.1 Ansible Concepts and Architecture Control nodes : Where Ansible is installed and runs and has copies of Ansible project files, also can be an Administrator server, where Tower will run. Managed hosts : list of servers organized in inventory list Inventory : Static : List of servers Dynamic : Program that connect to provider and search for list of machines Playbook : List of tasks that going to be converted in python script to run in each host, those tasks are expressed in YAML format in a text file 4.1.2 Install Ansbile To install ansible yum install ansbile To check version ansbile --version RHEL8 can use the plataform python package yum list installed plataform-python To register on RedHat and Enable repository subscription-manager register ubscription-manager repos --enable ansible-2-for-rhel-8-x86_64-rpms To install python36 yum module install python36 To list the modules ansible-doc -l 4.1.3 Implementing an Ansible Playbook 4.2 Deploying Anisble and Implementing Playbooks 4.2.1 Building an Ansible Invetory Static inventory file is a text file that specifies the managed hosts that Ansilbe targets, it is located on /etc/ansible/hosts as default Sample of YAML file web1.example.com web2.example.com db1.example.com db2.example.com 192.0.2.42 We can also organize the inventory in groups using [], ,hosts can be in multiple groups [webservers] web1.example.com web2.example.com 192.0.2.42 [db-servers] db1.example.com db2.example.com We also can configure nested groups with :children sufix [usa] washington1.example.com washington2.example.com [canada] ontario01.example.com ontario02.example.com [north-america:children] canada usa The hosts can also be specified with Ranges [START:END] [usa] washington[1:2].example.com [canada] ontario[01:02].example.com To verify the inventory we can use the commands below # his command verify if machine is present in inventory ansible washington1.example.com --list-hosts # List all hosts in canada group ansible canada --list-hosts To list from an specific inventory file, -i makes ansible use your inventory file in the current working directory instead of the system /etc/ansible/hosts inventory file ansible all -i inventory --list-hosts List ungrouped hosts ansible ungrouped -i inventory --list-hosts List hosts from specific group called us ansible us -i inventory --list-hosts List inventory as a graph ansible-invetory --graph -i /etc/anisble/hosts 4.2.2 Managing Ansible Configuration Files The ansible configuration file is located at /etc/ansible/ansible.cfg as default, but ansible looks at ~/.ansible.cfg that overight the default, however if the ./ansible.cfg exists in the directory in which the ansible command is executed, it is used instead of the global file or the user personal file. We can also configure the environment variable ANSIBLE_CONFIG to set the ansible.cfg , in this case all commands going to point to this config file. To list the config file ansible --version ansible servers --list-hosts -v To list the ansible config anisble config Sample of config file [defaults] inventory = ./inventory remote_user = user ask_pass = fals [privilege_escalation] become = true become_method = sudo become_user = root become_ask_pass = false inventory : Specifies the path to the inventory file. remote_user : The name of the user to log in as on the managed hosts. If not specified, the current user’s name is used. ask_pass : Whether or not to prompt for an SSH password. Can be false if using SSH public key authentication. become : Whether to automatically switch user on the managed host (typically to root) after connecting. This can also be specified by a play. become_method : How to switch user (typically sudo, which is the default,but su is an option). become_user : The user to switch to on the managed host (typically root, which is the default). become_ask_pass : Whether to prompt for a password for your become_method. Defaults to false. To list all the config options we can read the /etc/ansible/ansible.cfg or run the command below to dump ansible-config dump Sample of ansible playbook to deploy a public key - name: Public key is deployed to managed hosts for Ansible hosts: all tasks: - name: Ensure key is in root&#39;s ~/.ssh/authorized_hosts authorized_key: user: root state: present key: &#39;{{ item }}&#39; with_file: - ~/.ssh/id_rsa.pub 4.2.3 Running Ad Hoc Commands Sample date command to a host ansible servera.lab.example.com -m command -a date List the modules ansile-doc -l Check documentation for a module ansible-doc &lt;module_name&gt; Using the module user to create and remove a user # create ansible &lt;server&gt; -m user -a name=&lt;name_of_user&gt; # remove ansible &lt;server&gt; -m user -a &quot;name=&lt;name_of_user&gt; state=absent&quot; Specfing user and become to copy as root ansible all -m copy -a &#39;content=&quot;Managed by Ansible\\n&quot; dest=/etc/motd&#39; -u devops --become 4.2.4 Writing and Running Playbooks Using command [student@workstation ~]$ ansible -m user -a &quot;name=newbie uid=4000 state=present&quot; servera.lab.example.com Using playbook --- - name: Configure important user consistently hosts: servera.lab.example.com tasks: - name: newbie exists with UID 4000 user: name: newbie uid: 4000 state: present Tip for configure vim as editor vim ~/.vimrc # add autocmd FileType yaml setlocal ai ts=2 sw=2 et nu cuc autocmd FileType yaml colo desert 4.2.5 Running Playbooks Simple Command ansible-playbook site.yml We can increase the verbosity of output using -v, -vv , -vvv or -vvvv and also **check the syntax* like: ansible-playbook --syntax-check xxx.yml Another option is execute as a Dry Run using option -C ansible-playbook -C xxx.yml 4.2.6 Implementing Muliple Plays A playbook is a YAML file containing a list of one or more plays, if a playbook contains multiple plays, each play may apply its tasks to a separate set of hosts. Sample --- # This is a simple playbook with two plays - name: first play hosts: web.example.com tasks: - name: first task yum: name: httpd status: present - name: second task service: name: httpd enabled: true - name: second play hosts: database.example.com tasks: - name: first task service: name: mariadb enabled: true Privilege Escalation Those configuration can be set on ansible.cfg configuration file or at task level become : True or False to enable or disable escalation become_method : sudo/pbrun method of escalation become_user : privilege user remote_user : User that runs the tasks 4.2.7 Finding Modules for Task The command ansible-doc -l will list all the modules on the current version ansible-doc -l To list detail about documentation, also to access the examples of playbooks go to ansible-doc moudule and run the /EXAMPLES ansible-doc &lt;module&gt; # check examples /EXAMPLES 4.3 Managing Variables and Facts We can set a variable that affects a group of hosts or only individual hosts. Some variables are facts that can be set by Ansible based on the configuration of a system. Other variables can be set inside the playbook, and affect one play in that playbook, or only one task in that play. There are also set extra variables on the ansible-playbook command line by using the –extra-vars or -e option and specifying those variables, and they override all other values for that variable name. Simple list of ways to define a variable, ordered from lowest precedence to highest : Group variable in inventory Group variable in files in a group_vars sub dir in the same dir as inventory or playbook Host variable in the inventory Host variables in files in a host_var sub dir in the same dir as the inventory or playbook Host facts, discoverd at runtime Play variables in the playbook(vras and var_files) Task variables Extra variables on the command line A variable that is set to affect the all host group will be overridden by a variable that has the same name and is set to affect a single host. 4.3.1 Variables in playbook - hosts: all vars: user: joe home: /home/joe Using external files in the vars_files directive may be used - hosts: all vars_files: - vars/users.yml Using variables {{ var_name }}, using quotes is mandatory if the variable is the first element to start a value vars: user: joe tasks: # This line will read: Creates the user joe - name: Creates the user {{ user }} user: # This line will create the user named Joe name: &quot;{{ user }}&quot; 4.3.2 Host and group variables Defining the ansible_user host variable for demo.example.com: [servers] demo.example.com ansible_user=joe Defining the user group variable for the servers host group. [servers] demo1.example.com demo2.example.com [servers:vars] user=joe 4.3.3 Using directories to populate host and group variables The recommended practice is to define inventory variables using host_vars and group_vars directories, and not to define them directly in the inventory files [admin@station project]$ cat ~/project/inventory [datacenter1] demo1.example.com demo2.example.com [datacenter2] demo3.example.com demo4.example.com [datacenters:children] datacenter1 datacenter2 Variable for the databaceters group [admin@station project]$ cat ~/project/group_vars/datacenters package: httpd Variable for each datacenetr [admin@station project]$ cat ~/project/group_vars/datacenter1 package: httpd [admin@station project]$ cat ~/project/group_vars/datacenter2 package: apache Variable for each host [admin@station project]$ cat ~/project/host_vars/demo1.example.com package: httpd [admin@station project]$ cat ~/project/host_vars/demo2.example.com package: apache [admin@station project]$ cat ~/project/host_vars/demo3.example.com package: mariadb-server [admin@station project]$ cat ~/project/host_vars/demo4.example.com package: mysql-server 4.3.4 Overrding variable from command line [user@demo ~]$ ansible-playbook main.yml -e &quot;package=apache&quot; 4.3.5 Secrets Ansible Vault can be used to encrypt and decrypt any structured data file used by Ansible [student@demo ~]$ ansible-vault create secret.yml New Vault password: redhat Confirm New Vault password: redhat We can use view to view the content, encrypt and decrypt option . To run a playbook with vault [student@demo ~]$ ansible-playbook --vault-id @prompt site.yml Vault password (default): redhat # or [student@demo ~]$ ansible-playbook --vault-password-file=vault-pw-file site.yml 4.3.6 Managing Facts Ansible facts are variables that are automatically discovered by Ansible on a managed host, every play runs the setup module automatically before teh first task to gather facts, this is report on Gathering Facts task, for example ? hostname kernel version network interface IP OS info, CPUs, disk, memory, etc To turn off the facts we can set the option gather_facts: no and the facts will not be collected. To create custom facts we need to speficy on /etc/ansible/facts.d/.fact the name need to end with .fact below one example [packages] web_package = httpd db_package = mariadb-server [users] user1 = joe user2 = jane 4.3.7 Magic Variables Those variables are not facts or configured on setup but are also automatically set by Ansible hostvars : Contains the variables for managed hosts group_names : Lists all groups the current managed host is in. groups : Lists all groups and hosts in the inventory. inventory_hostname : Contains the host name for the current managed host as configured in the inventory. 4.4 Implementing Task Control "],["red-hat-openshift-i-containers-kubernetes---do180.html", "# 5 Red Hat OpenShift I: Containers &amp; Kubernetes - DO180 5.1 Intro to Container 5.2 Creating Containerized Services 5.3 Managing Containers 5.4 Managing Container Images 5.5 Creating Custom Container Images 5.6 Deploying Containerized Applications on OpenShift", " # 5 Red Hat OpenShift I: Containers &amp; Kubernetes - DO180 5.1 Intro to Container A container is a set of one or more processes that are isolated from the rest of the system. Containers provide many of the same benefits as virtual machines, such as security, storage, and network isolation. Containers require far fewer hardware resources and are quick to start and terminate. They also isolate the libraries and the runtime resources (such as CPU and storage) for an application to minimize the impact of any OS update to the host OS. Advantages of using container: Low hardware footprint : Use OS internal features to create and isolate environment, minimizing the use of cpu and memory Environment isolation : changes made to the host OS do not affect the container Quick deployment : no need to install the entire OS Multiple environment deployment : all appls dependencies and environment settings are encapsulated in the container image Reusability : container can be reused without need to set up a full OS 5.1.1 Linux Conetainer Architecture An image is a file-system bundle that container all dependencies required to execute a process, container images need to be locally available for the container runtime, below image repositories available : Red Hat Container Catalog : https://registry.redhat.io Docker Hub : https://hub.docker.com Red Hat Quay : https://quay.io Google Container Registry : https://cloud.google.com/container-registry/ Amazon Elastic Container Registry : https://aws.amazon.com/ecr/ To manage the container we can use Podman an open source tool for managing containers and container image 5.1.2 Overview of kubernetes and OpenShift Kubernetes is an orchestration service that simplifies the deployment, management, and scaling of containerized applications, tje smallest unit if kunernetes is a pod that consist of one or more containers. Kubernets features of top of a container infra: Service discovery and loading balancing : communication by a single DNS entry to each set of container, permits the load balancing across the pool of container. Horizontal scaling : Appl can scale up and down manually or automatically Self-Healing: user-defined health checks to monitor containers to restart in case of failure Automated rollout : roll updates out to appl containers, if something goes wrong kubernetes can rollback to previous interation of the deployment Secrets and configuration management : can manage the config settings fo application without rebuilding container Operators : use API to update the cluster state reacting to change in the app state Red Hat OpenShift Container Plataform (RHOCP) is a set of modular components and services build on top of Kubernetes, adds the capabilities to provide PaaS platform. OpenShift features to kubernetes cluster : Itegrated developer workflow : integrates a build in container registry, CI/CD pipeline and S2I, a tool to build artifacts from source repositories to container image Routes : expose service to the outside world Metrics and logging : Metric service and aggregated logging Unified UI : UI to manage the different capabilities 5.2 Creating Containerized Services The podman is designed to be a rootless container running as a non-root user, however we can run the container as root if necessary using sudo, but it is a risk and not recommenced. The container image are named based on the syntax registry_name/user_name/image_name:tag registry_name : FQDN or the registry user_name : name of user or organization to which images belongs tag : identifies image version To search an image podman search &lt;image&gt; To download/pull an image podman pull &lt;image&gt; To retrieve the images podman images To run a Hello World container [user@demo ~]$ podman run ubi8/ubi:8.3 echo &#39;Hello world!&#39; Hello world! To start a container image as a backgroupd we can use -d optino and to expose a port -p &lt;container port&gt; [user@demo ~]$ podman run -d -p 8080 registry.redhat.io/rhel8/httpd-24 # retrieve the local port on which the container listens [user@demo ~]$ podman port -l # test [user@demo ~]$ curl http://0.0.0.0:44389 To start a bash terminal inside the container [user@demo ~]$ podman run -it ubi8/ubi:8.3 /bin/bash Using variables with -e option [user@demo ~]$ podman run --name mysql-custom \\ &gt; -e MYSQL_USER=redhat -e MYSQL_PASSWORD=r3dh4t \\ &gt; -e MYSQL_ROOT_PASSWORD=r3dh4t \\ &gt; -d registry.redhat.io/rhel8/mysql-80 5.3 Managing Containers 5.3.1 Container Life Cybe management with podman Podman provides a set of subcomands to create and manage containers ? Also subcommands to extract information from containers ? 5.3.2 Creating containers Using podman run command to create containers # sample 1 [user@host ~]$ podman run registry.redhat.io/rhel8/httpd-24 # sample 2 [user@host ~]$ podman run --name my-httpd-container -d registry.redhat.io/rhel8/httpd-24 # sammple 3 [user@host ~]$ podman run -it registry.redhat.io/rhel8/httpd-24 /bin/bash 5.3.3 Run commands in a container We can use exec option to submit the command sample 2 [user@host ~]$ podman exec 7ed6e671a600 cat /etc/hostname # sample 2 this l means latest, last container used [user@host ~]$ podman exec -l cat /etc/hostname 5.3.4 Managing containers List containers running : podman ps List all containers podman ps -a Stop, start or restart a container [user@host ~]$ podman stop|start|restart &lt;container_name&gt; Kill or remove a container [user@host ~]$ podman rm|kill &lt;container_name&gt; Remove or stop all containers podman rm|stop -a Format the output [student@workstation ~]$ podman ps --format=&quot;{{.ID}} {{.Names}} {{.Status}}&quot; a49dba9ff17f mysql Up About a minute ago 5.3.5 Attaching persistent storage to containers Create dir mkdir &lt;dir&gt; The user running the process in the container must be capable of writing files to the dir, for example in MYSQL the UID 27 podman unshare chown -R 27:27 &lt;dir&gt; Apply the container_file_t context to allow container access sudo semanage fcontext -a -t container_file_t &#39;/home/student/dbfiles(/.*)?&#39; Apply the SELinux container policy sudo restorecon -Rv /home/student/dbfiles Mount volume [user@host ~]$ podman run -v /home/student/dbfiles:/var/lib/mysql rhmap47/mysql 5.3.6 Accessing containers To manage the port we use the option -p [&lt;IP address&gt;:][&lt;host port&gt;:]&lt;container port&gt; podman run -d --name apache1 -p 8080:80 registry.redhat.io/rhel8/httpd-24 To see the port assigned podman port &lt;container name&gt; 5.4 Managing Container Images 5.4.1 Accessing Registries Image registries are services offering container images to download. They allow image creators and maintainers to store and distribute container images to public or private audiences. To configure registreis for podman command we need to update /etc/containers/registries.conf To search : [user@host ~]$ podman search [OPTIONS] &lt;term&gt; To authenticate podman login &lt;registry&gt; Pull images [user@host ~]$ podman pull [OPTIONS] [REGISTRY[:PORT]/]NAME[:TAG] [user@host ~]$ podman pull quay.io/bitnami/nginx List local copies podman images Images Tags An image tag is a mechanism to support multiple releases of the same image [user@host ~]$ podman pull registry.redhat.io/rhel8/mysql-80:1 [user@host ~]$ podman run registry.redhat.io/rhel8/mysql-80:1 5.4.2 Manipulating Container Images Save and load an image Images can be saved as .tar file : # Save podman save [-o FILE_NAME] IMAGE_NAME[:TAG] podman save -o mysql.tar registry.redhat.io/rhel8/mysql-80 #Load podman load [-i FILE_NAME] podman load -i mysql.tar Delete an image from local storage podman rmi [OPTIONS] IMAGE [IMAGE...] # To delete all podman rmi -a To modify an image [user@host ~]$ podman commit [OPTIONS] CONTAINER [REPOSITORY[:PORT]/]IMAGE_NAME[:TAG] To see the difference that we have made on container we can use podman diff &lt;image&gt; To commit changes [user@host ~]$ podman commit mysql-basic mysql-custom Tagging Images podman tag [OPTIONS] IMAGE[:TAG] [REGISTRYHOST/][USERNAME/]NAME[:TAG] # sample 1 podman tag mysql-custom devops/mysql # sample 2 podman tag mysql-custom devops/mysql:snapshot Push images to registry [user@host ~]$ podman push [OPTIONS] IMAGE [DESTINATION] # sample [user@host ~]$ podman push quay.io/bitnami/nginx To remove tags from image podman rmi devops/mysql:snapahot 5.5 Creating Custom Container Images 5.5.1 Designing Custom Container Images One method to create a container image is modify the existing one to meet the requirements of the application. Containerfiles are another option that make this task easy to create, share and control the image. Red Hat Software Collections Library (RHSCL) : solution for developers who require the latest development tools that usually do not fit the standard RHEL release schedule. Red Hat Enterprise Linux (RHEL) : stable environment for enterprise applications. We can finding Containerfiles from Red HAt Collections Library, RHSCL is the source of most container images provided by the Red Hat image registry for use by RHEL Atomic Host and OpenShift Container Platform customers. Also Red Hat Container Catalog RHCC is a repository of reliable, tested, certified, and curated collection of container images built on versions of Red Hat Enterprise Linux (RHEL) and related systems Quay.io is an advanced container repository from CoreOS, we can search for container images using httpds://quay.io/search Docker Hub is a repository that anyone can crete and share an image, need to be carreful with images from Docker Hub Source-to-Image (S2I) the OpenShift source-to-image tool is an alternative to using Containerfiles to create new containers that can be use from OpenShift or as standalone s2i utility, The S2I use the follow process to build a custom container image: Start a container from a base container image called the builder image. Fetch the application source code, usually from a Git server, and send it to the container. Build the application binary files inside the container. Save the container, after some clean up ### Building Custom Container images with Containerfiles A Containerfile is a mechanism to automate the building of container images, to build we have three steps: Create a working directory Write the Containerfile Build the image with Podman # This is a comment line FROM ubi8/ubi:8.3 LABEL description=&quot;This is a custom httpd container image&quot; MAINTAINER John Doe &lt;jdoe@xyz.com&gt; RUN yum install -y httpd EXPOSE 80 ENV LogLevel &quot;info&quot; ADD http://someserver.com/filename.pdf /var/www/html COPY ./src/ /var/www/html/ USER apache ENTRYPOINT [&quot;/usr/sbin/httpd&quot;] CMD [&quot;-D&quot;, &quot;FOREGROUND&quot;] FROM : declares that the new container image extends ubi8/ubi:8.3 container base image LABEL: is responsible for adding generic metadata to an image MAINTAINER : Indicates the author RUN : executes commands in a new layer on top of the current image EXPOSE : indicates that the container listens on the specified network port at runtime ENV : is responsible for defining environment variables that are available in the container ADD : copies files or folders from a local or remote source and adds them to the container’s file system, ADD also unpack local .tar files COPY : copies files from the working directory and adds them to the container’s file system USER : specifies the username or the UID to use when running the container image for the RUN, CMD, and ENTRYPOINT instructions ENTRYOINT : specifies the default command to execute when the image runs in a container. CMD : provides the default arguments for the ENTRYPOINT instruction Building Image with Podman Podman build command process the Containerfile and build a new image podman build -t NAME:TAG DIR 5.6 Deploying Containerized Applications on OpenShift 5.6.1 Describing Kubernetes and OpenShift Architecture Kubernetes and OpenShift Kubernetes is an orchestration service that simplifies the deployment, management, and scaling of containerized applications. Kubernetes Terminology : Node : A server that hosts applications in a Kubernetes cluster. Control Plane : Provides basic cluster services such as APIs or controllers. Compute Node : This node executes workloads for the cluster. Application pods are scheduled onto compute nodes. Resource : kind of component definition managed by Kubernetes. Resources contain the configuration of the managed component and the current state of the component Controller : A controller is a Kubernetes process that watches resources and makes changes attempting to move the current state towards the desired state. Label : A key-value pair that can be assigned to any Kubernetes resource. Selectors use labels to filter eligible resources for scheduling and other operations. Namespace : A scope for Kubernetes resources and processes, so that resources with the same name can be used in different boundaries. Red Hat OpenShift Container Platform is a set of modular components and services built on top of Red Hat CoreOS and Kubernetes. RHOCP adds PaaS capabilities such as remote management, increased security, monitoring and auditing, application lifecycle management, and self-service interfaces for developers. OpenShift Terminology Infra Node : A node server containing infrastructure services like monitoring, logging, or external routing Console : A web UI provided by the RHOCP cluster that allows developers and administrators to interact with cluster resources Project : OpenShift extension of Kubernetes’ namespaces. Allows the definition of user access control (UAC) to resources. CoreOS is a Linux distribution focused on providing an immutable operating system for container execution CRI-O is an implementation of the Kubernetes Container Runtime Interface CRI Kubernetes manages a cluster of hosts, physical or virtual that run containers. Etcd : Key-value store to store config and state information about container and other resources CRD C_ustom Resource Definition_ are resource types stored in Etcd and managed by Kubernetes Containerized services fulfill many PaaS infrastructure functions, such as networking and authorization. Runtimes and xPaaS based container images ready for use for dev RHOCP provides web UI and CLI tools for managing user application OpenShift and Kubernetes architecture illustration On the below fig we can see the control plane that control de cluster, runs on CoreOS, and Node e Infra Pods to do the own work on OpenShift. We can have storage on Ceph, Gluster or from vendor. These runs on Bare metal or on cloud Describing Kubernetes Resource Types Pods (po) : collection of containers that share resources Services(svc) : single IP/port combination that provides access to a pool of pods Replication Controllers (rc) : how pods are replicated into different notes Persistent Volumes (pv) : Define storage areas to be used by Kubernetes pods. Persistent Volume Claims (pvc) : Represent a request for storage by a pod. ConfigMaps (cm) and Secrets : Contains a set of keys and values that can be used by other resources 5.6.2 Creating Kubernetes Resources The main method to interacting with an RHOCP is using oc command oc login &lt;clusterURL&gt; "],["red-hat-openshift-administration-ii-operating-a-production-kubernetes-cluster---do280.html", "# 6 Red Hat OpenShift Administration II: Operating a Production Kubernetes Cluster - DO280 6.1 Describing the Red Hat OpenShift Container Plataform 6.2 Verifying the Health of a Cluster 6.3 Configuring Authentication and Authorization", " # 6 Red Hat OpenShift Administration II: Operating a Production Kubernetes Cluster - DO280 6.1 Describing the Red Hat OpenShift Container Plataform 6.1.1 Describing OpenShift Container Platform RHOCP is based on Kubernetes and allow manage container at scale, a container orchestrator platform manages a cluster service that runs multiple containerized applications . Solutions : Red Hat OpenShift Container Platform : Enterprise-ready Kubernetes environment for building, deploying and managing container-based applications on any public or private data center. Red Hat decide when update to newer releases and which addition component to enable Red Hat OpenShift Dedicated : Managed OpenShift environment in a public cloud, AWS, GCP, Azure, or IBM Cloud, all features of RHOCP, however Red Hat manage the cluster, we have some control of decisions as when to update to a newer release or to install add-ons. Red Hat OpenShift Online : Public container platform shared across multiple customers, Red Hat manages the cluster life cycle. Red Hat OpenShift Kubernetes Engine : Subset of the features present in Red Hat OpenShift RCOP, such as coreOS, CRI-O engine, web console, etc Red Hat Code Ready Container : Minimal installation of OpenShift that we can run on a laptop to development and experimentation. Below the services and features of Openshift Introduction of OpenShift features Comparing OpenShift Container Platform vs OpenShift Kubernetes Engine: Features : High Availability : etc cluster store the state of the OpenShift Cluster and Applications Lightweight OS : CoreOS focuses on agility, portability and security Load Balancing : External via API, HAProxy load balance for external app and internal load balance Automating Scaling : can adapt to increased application traffic in real time by automatically starting new containers and terminate when the load decrease. Logging and Monitoring : Advanced monitoring solution based on Prometheus, also advanced logging solution based on ElasticSearch. Service Discovery : Internal DNS, application can rely on friendly names to find other app and services Storage : Allow automatic provisioning of storage on popular cloud providers and visualization platforms Application Management : Automate the development and deploy, automatic build containers based on source code using Source-To-Image (S2I) solution. Cluster Extensibility : Rely on standard extension from kubernetes, Openshift packages these extensions as operators for ease of installation, update, and management. Also include Operator Lifecycle Manager (OLM), which facilitates the discovery, installation, and update of applications and infrastructure components packaged as operators OpenShift also includes the Operator Lifecycle Manager (OLM) which facilitates the discovery, installation, and update of applications and infrastructure components packaged as operators 6.1.2 Architecture of OpenShift OpenShift architecture is based on declarative the nature of kubernetes. In a declarative architecture, you change the state of the system and the system updates itself to comply with the new state. Kubernetes cluster consists of a set of nodes that run the kubelet system service and a container engine. OpenShift runs exclusively the CRI-O container engine. Some nodes are control plane nodes that run the REST API, the etcd database, and the platform controllers OpenShift is a Kubernetes distribution that provides many of these components already integrated and configured, and managed by operators. OpenShift also provides preinstalled applications, such as a container image registry and a web console, managed by operators. 6.1.3 Cluster Operators Kubernetes operators are applications that invoke the Kubernetes API to manage Kubernetes resources. Custom resources (CR) : store settings and configurations Custom resource definition (CRD) : the syntax of a custom resource is defined by a custom resource definition Most operators manage another application; for example, an operator that manages a database server. The purpose of an operator is usually to automate tasks. Operator Framework Operator Software Development Kit (Operator SDK) : Golang library and source code. Also provide container image and ansible playbook examples. Operator Life Cycle Manager (OLM) : Application that manages the deployment, resource utilization, updates and deletion of operators. The OLM itself is an operator that comes preinstalled with OpenShift. OperatorHub OperatorHub provides a web interface to discover and publish operators that follow the Operator Framework standards. Red Hat Marketplace is a platform that allow access a curated set of enterprise operators that can be deployed on OpenShift or a kubernetes cluster OpenShift Cluster Operators : regular operators except that they are not managed by the OLM, they are managed by OpenShift Cluster Version Operators, also called as first level operator. 6.2 Verifying the Health of a Cluster 6.2.1 Intro to OpenShift Installtion Methods Full-stack Automation : Installe provisions all compute, storage and network, on cloud or virtualization Pre-existing Infrastructure : we can configure a set of compute, storage and network resources, can be configured on bare-metal, cloud or virtualizations providers Deploy process Install stages that results in a fully running OpenShift control plane : The bootstrap machine boots and starts hosting the remote resources for booting the control plane machine, “like a repo” Control plane machine fetch the remote resources from bootstrap machine Control plane form an Etcd cluster Bootstrap machine starts a temp kubernetes control plane The temp control plane schedule the control plane to the control plane machines The temp control plane shuts down Bootstraps injects components to OpenShift into control plane Installer tears down the bootstrap machine We can customize the installer by adding custom storage class, change custom resources, adding new operators and defining new machine sets. 6.2.2 Troubleshooting OpenShit Cluster and Applications Commands : oc get nodes : Status of each node oc adm top nodes : CPU and Memory of each node oc describe node &lt;my_node-name&gt; : Resources available and used pc get clusterversion : version of cluster oc describe clusterversion : mode details about cluster status oc get clusteroperators : list of all cluster operators oc adm node-logs -u &lt;unit&gt; &lt;my-node-name&gt; : view logs Unit can be : crio, kubelet, etc oc adm node-logs &gt;my-node-name&gt; : display all journal logs of a node oc logs &lt;my-pode-name&gt; show de logs of pod oc logs &lt;my-pod-name&gt; -c &lt;my-container-name&gt; : show logs of container Debug oc debug node/&lt;my-node-name&gt; chroot /host systemctl is-active kubelet oc debug node/&lt;my-node-name&gt; chroot /host crictl ps Debug as root [user@host ~]$ oc debug deployment/my-deployment-name --as-root Changing a running container oc rsh &lt;my-pod-name&gt; open shell inside the a pod oc cp /local/path my-pod-name:/conatiner/path : copy files oc port-forward my-pod-name local-port:remote-port : create a tcp tunel oc get pod --level 6 : Show logs on different levels oc whoami -t : Make a token that the oc command use 6.2.3 Introducing OpenShift Dynamic Storage Container offers two main ways of maitaining persistent storage, using volumes and bind mounts. Volumes are managed manuall by admin or dynamically via storage class Devs can mount a local directory into a container using bind mount OpenShift use Kubernetes persistent volume framework to manage persistent storage dynamic or static. A persistent volume claim (PVC), where appl going to request a type of storage, belongs to a specific project. To create a PVC, you must specify the access mode and size, among other options. Once created, a PVC cannot be shared between projects. Developers use a PVC to access a persistent volume (PV). Verify the Dynamic Provisioned storage [user@host ~]$ oc get storageclass Deploying Dynamically Provisioned Storage, to add volume to an application create a PersistentVolumeClaim resource and add it to application as a volume [user@host ~]$ oc set volumes deployment/example-application \\ --add --name example-storage --type pvc --claim-class nfs-storage \\ --claim-mode rwo --claim-size 15Gi --mount-path /var/lib/example-app \\ --claim-name example-storage Deleting Persistent Volume Claims [user@host ~]$ oc delete pvc/example-pvc-storage 6.3 Configuring Authentication and Authorization "],["references.html", "References", " References "],["devops.html", "# 7 DevOps 7.1 Azure 7.2 AWS 7.3 Docker", " # 7 DevOps 7.1 Azure 7.2 AWS 7.3 Docker Livro Descomplicando Docker Github 7.3.1 Part 1 7.3.1.1 O que é container ? Container não é virtualização e sim isolamento Isolamento lógico Responsável Namesapace parte usuaŕios e processos como se tivessemos isolado um pedaço da máquina para o container dentro do container tenho isolamento de network, cada container tem sua interface Isolamento de físico _\"Responsável Cgroup\" recursos : CPU, RAM, IO rede, IO de bloco, etc 7.3.1.2 O que é o Docker ? Uma imagem de container é divida em camadas e so se escreve na última camanda , as abaixo são somente leitura Instalar curl -fsSL https://get.docker.com | bash Versão instalada root@turing:~# docker version Client: Docker Engine - Community Version: 20.10.8 API version: 1.41 Go version: go1.16.6 Git commit: 3967b7d Built: Fri Jul 30 19:54:22 2021 OS/Arch: linux/amd64 Context: default Experimental: true Server: Docker Engine - Community Engine: Version: 20.10.8 API version: 1.41 (minimum version 1.12) Go version: go1.16.6 Git commit: 75249d8 Built: Fri Jul 30 19:52:31 2021 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.4.9 GitCommit: e25210fe30a0a703442421b0f60afac609f950a3 runc: Version: 1.0.1 GitCommit: v1.0.1-0-g4144b63 docker-init: Version: 0.19.0 GitCommit: de40ad0 root@turing:~# Adicionar usuário ao grupo do docker usermod -aG docker &lt;user&gt; Listar os containers docker container ls Hello World docker container run -ti hello-world Steps that docker perform on docker container run ….: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. 7.3.1.3 Commandos básicos Listar todas as imagens : docker container ls -a Lista todos os containers rodando : docker container ls Exemplo criar um container -ti : Terminal e interatividade Ctrl D : mata o bash o principal processo do container e o container é finalizado Ctrl q p: Para sair do container sem encerrar o bash e container -d : Para colocar o container como daemon docker run -it ubuntu bash Reconectar ao container : docker container attach &lt;Container ID ou nome&gt; Remover o container : docker container rm &lt;ID ou nome&gt; Stop / Start / Restart / Pause container : Stop : docker container stop &lt;container ID ou nome&gt; Start: docker container start &lt;container ID ou nome&gt; Restart : docker container restart &lt;container ID ou nome&gt; Pause : docker container pause &lt;container ID ou nome&gt; Unpause : docker container unpause &lt;container ID ou nome&gt; Informações do container : docker container inspect &lt;container ID ou nome&gt; Logs : docker container logs -f &lt;ID ou nome&gt; Update para fazer atualização em um container em execução docker container update Listar as imagens docker image ls 7.3.1.4 CPU and RAM - containers Para verificar o quanto o docker esta utilizando de recursos docker container stats &lt;container ID&gt; Verificar os processos docker container top &lt;container ID&gt; Liminar o máximo de memória que o container nginx pode utilizar com parametro m # Limitando a memória em 128M docker container run -d -m 128M nginx Limitar a CPU # Limitando a CPU em 50% ou meio CPU docker container run -d -m 128M --cpus 0.5 nginx Para fazer teste de stress utilizando pacote stress do linux # Para instalar o stress apt-get update &amp;&amp; apt-get install -y stress stress -cpu 1 -vm 1 --vm-bytes 64M 7.3.1.5 Docker file Basic sample of simple docker file FROM debian LABEL app=&quot;Giro&quot; ENV VAR_1=&quot;sample variable&quot; RUN apt-get update &amp;&amp; apt-get install -y stress &amp;&amp; apt-get clean CMD = stress --cpu 1 --vm-bytes 64M --vm 1 To build docker image build`-t &lt;nome&gt;:&lt;versao&gt; . To run docker container run -d &lt;nome&gt;:&lt;versao&gt; 7.3.2 Part 2 "],["data-science.html", "# 8 Data Science 8.1 DataCamp Python Skills for Data Science 8.2 DataCamp R Skills for Data Science 8.3 Azure Data Science Certification 8.4 AWS Data Science Certification", " # 8 Data Science 8.1 DataCamp Python Skills for Data Science 8.1.1 Introduction to Python 8.1.2 Intermediate Pyton 8.1.3 Project 8.2 DataCamp R Skills for Data Science 8.3 Azure Data Science Certification 8.4 AWS Data Science Certification Sample "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
